PROJECT FILE TREE
================================================================================
ccda-course-project/
    run_complex_queries.sh
    run_query_full.sh
    run_ml.sh
    run_complex_queries_sample.sh
    run_ml_sample.sh
    run_sample.sh
    run.sh
    requirements.txt
    notebooks/
        streaming_sample.py
        streaming.py
        eda.py
        ml.py
        ml_sample.py
        complex_queries.py
    scripts/
        train_ml.py
        prepare_sample_stream_batches.py
        train_ml_sample.py
        download_arxiv.py
    src/
        similarity.py
        __init__.py
        transformations.py
        query.py
        utils.py
        featurization.py
        ingestion.py
    tests/
        test_ingestion.py
        test_sql.py
        test_ml.py
        test_streaming.py


/home/data/akhalegh/utils/ccda-course-project/run_complex_queries.sh
================================================================================
#!/bin/bash
set -euo pipefail

RAW="data/raw/arxiv-metadata-oai-snapshot.json"
OUT="data/processed/arxiv_full"

# ---- Spark low-memory friendly settings for BOTH ingestion & complex queries ----
export SPARK_LOCAL_DIRS="${SPARK_LOCAL_DIRS:-$(pwd)/data/tmp/spark-local}"
mkdir -p "$SPARK_LOCAL_DIRS"

# Give the local driver (and its single executor) more heap; keep tasks small
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-8g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-8g}"

# Ensure these confs are applied even when launching via `python script.py`
export PYSPARK_SUBMIT_ARGS="\
 --conf spark.sql.session.timeZone=UTC \
 --conf spark.sql.adaptive.enabled=true \
 --conf spark.sql.adaptive.coalescePartitions.enabled=true \
 --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=8m \
 --conf spark.sql.files.maxPartitionBytes=8m \
 --conf spark.sql.shuffle.partitions=800 \
 --conf spark.sql.adaptive.skewJoin.enabled=true \
 --conf spark.sql.adaptive.skewedPartitionThresholdInBytes=64m \
 --conf spark.sql.adaptive.skewedPartitionMaxSplitBytes=16m \
 --conf spark.local.dir=${SPARK_LOCAL_DIRS} \
 --driver-memory ${SPARK_DRIVER_MEMORY} \
 --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
 pyspark-shell"

echo "[check] ensuring raw dataset exists..."
if [[ ! -f "$RAW" ]]; then
  echo "[download] fetching arXiv metadata via KaggleHub..."
  python scripts/download_arxiv.py --sample 0
fi

echo "[check] ensuring FULL Parquet exists..."
if [[ ! -d "$OUT" ]] || [[ -z "$(ls -A "$OUT" 2>/dev/null || true)" ]]; then
  echo "[ingest] JSON/JSONL -> Parquet (full)…"
  python -m src.ingestion \
    --input "$RAW" \
    --output "$OUT" \
    --partition-by year \
    --repartition 200 \
    --no-stats
fi

echo "[complex] running Week 9 complex queries (FULL)…"
python notebooks/complex_queries.py \
  --parquet "$OUT" \
  --outdir full_complex_queries

echo "[done] complex query artifacts in reports/full_complex_queries/ (CSVs + PNGs)."


/home/data/akhalegh/utils/ccda-course-project/run_query_full.sh
================================================================================
#!/usr/bin/env bash
set -euo pipefail
MODEL_DIR="data/models/tfidf_full"
FEATS_TRAIN="data/processed/features_trained_full/split=train"
OUT_DIR="reports/full"

python notebooks/ml.py \
  --mode query \
  --model-dir "${MODEL_DIR}" \
  --features-train "${FEATS_TRAIN}" \
  --out "${OUT_DIR}" \
  --query-title "Diffusion models for conditional generation" \
  --query-abstract "We introduce a guidance method ..." \
  --k 10

/home/data/akhalegh/utils/ccda-course-project/run_ml.sh
================================================================================
#!/usr/bin/env bash
set -euo pipefail

# ===== Full arXiv pipeline (split -> train -> evaluate) =====
# Assumes you already have full Parquet at data/processed/arxiv_full
# Creates: model (data/models/tfidf_full), features (data/processed/features_trained_full),
# and reports (reports/full)

SPLIT_PARQUET="data/processed/arxiv_split"
MODEL_DIR="data/models/tfidf_full"
FEATS_OUT="data/processed/features_trained_full"
OUT_DIR="reports/full"
SRC_PARQUET="data/processed/arxiv_full"

mkdir -p "${OUT_DIR}"

# 1) Split train/test (configurable)
python scripts/split.py \
  --parquet "${SRC_PARQUET}" \
  --out "${SPLIT_PARQUET}" \
  --test-years 2019,2020,2021 \
  --test-size 50000 \
  --seed 42

# 2) Train TF‑IDF model + write normalized features
python scripts/train_ml.py \
  --split-parquet "${SPLIT_PARQUET}" \
  --model-dir "${MODEL_DIR}" \
  --features-out "${FEATS_OUT}" \
  --vocab-size 250000 \
  --min-df 5 \
  --use-bigrams false \
  --extra-stopwords-topdf 500 \
  --seed 42

# 3) Evaluate (Top‑K via block-by-category to avoid full cartesian)
python notebooks/ml.py \
  --mode eval \
  --model-dir "${MODEL_DIR}" \
  --split-parquet "${SPLIT_PARQUET}" \
  --features "${FEATS_OUT}" \
  --out "${OUT_DIR}" \
  --k 5 \
  --strategy block_cat \
  --eval-max-test 20000

echo "Done. Artifacts in ${OUT_DIR}/"

/home/data/akhalegh/utils/ccda-course-project/run_complex_queries_sample.sh
================================================================================
#!/bin/bash
set -euo pipefail

SAMPLE="data/sample/arxiv-sample.jsonl"
OUT="data/processed/arxiv_parquet"

# ---- Spark-friendly settings (smaller than full run) ----
export SPARK_LOCAL_DIRS="${SPARK_LOCAL_DIRS:-$(pwd)/data/tmp/spark-local}"
mkdir -p "$SPARK_LOCAL_DIRS"

# Smaller heap is fine for the sample
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-4g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-4g}"

# Apply confs even when launching via `python script.py`
export PYSPARK_SUBMIT_ARGS="\
 --conf spark.sql.session.timeZone=UTC \
 --conf spark.sql.adaptive.enabled=true \
 --conf spark.sql.adaptive.coalescePartitions.enabled=true \
 --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=8m \
 --conf spark.sql.files.maxPartitionBytes=8m \
 --conf spark.sql.shuffle.partitions=256 \
 --conf spark.sql.adaptive.skewJoin.enabled=true \
 --conf spark.sql.adaptive.skewedPartitionThresholdInBytes=64m \
 --conf spark.sql.adaptive.skewedPartitionMaxSplitBytes=16m \
 --conf spark.local.dir=${SPARK_LOCAL_DIRS} \
 --driver-memory ${SPARK_DRIVER_MEMORY} \
 --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
 pyspark-shell"

echo "[check] ensuring sample JSONL exists..."
if [[ ! -f "$SAMPLE" ]]; then
  echo "[download] fetching arXiv metadata + writing sample via KaggleHub..."
  # Creates data/raw/... and data/sample/arxiv-sample.jsonl
  python scripts/download_arxiv.py --sample 50000
fi

echo "[check] ensuring SAMPLE Parquet exists..."
if [[ ! -d "$OUT" ]] || [[ -z "$(ls -A "$OUT" 2>/dev/null || true)" ]]; then
  echo "[ingest] JSONL sample -> Parquet (partitioned by year)…"
  python -m src.ingestion \
    --input "$SAMPLE" \
    --output "$OUT" \
    --partition-by year \
    --repartition 64 \
    --no-stats
fi

echo "[complex] running Week 9 complex queries (SAMPLE)…"
python notebooks/complex_queries.py \
  --parquet "$OUT" \
  --outdir sample_complex_queries

echo "[done] complex query artifacts in reports/sample_complex_queries/ (CSVs + PNGs)."


/home/data/akhalegh/utils/ccda-course-project/run_ml_sample.sh
================================================================================
#!/usr/bin/env bash
set -euo pipefail

SPLIT_PARQUET="data/processed/arxiv_sample_split"
MODEL_DIR="data/models/tfidf_sample"
FEATS_OUT="data/processed/features_trained_sample"
OUT_DIR="reports/ml_sample"

echo "==> Step 1: Split"
python scripts/split_sample.py \
  --parquet data/processed/arxiv_parquet \
  --out "${SPLIT_PARQUET}" \
  --test-years 2007,2008,2009 \
  --test-size 1000 \
  --seed 42

echo "==> Step 2: Train & Save"
python scripts/train_ml_sample.py \
  --split-parquet "${SPLIT_PARQUET}" \
  --model-dir "${MODEL_DIR}" \
  --features-out "${FEATS_OUT}" \
  --vocab-size 80000 \
  --min-df 3 \
  --use-bigrams false \
  --extra-stopwords-topdf 200 \
  --seed 42

echo "==> Step 3: Evaluate"
python notebooks/ml_sample.py \
  --mode eval \
  --model-dir "${MODEL_DIR}" \
  --split-parquet "${SPLIT_PARQUET}" \
  --features "${FEATS_OUT}" \
  --out "${OUT_DIR}" \
  --k 3 \
  --strategy exact

echo "Done. Artifacts in ${OUT_DIR}/"


/home/data/akhalegh/utils/ccda-course-project/run_sample.sh
================================================================================
#!/bin/bash
set -euo pipefail

SAMPLE="data/sample/arxiv-sample.jsonl"
OUT="data/processed/arxiv_parquet"

# ---- Spark-friendly settings (smaller than full run) ----
export SPARK_LOCAL_DIRS="${SPARK_LOCAL_DIRS:-$(pwd)/data/tmp/spark-local}"
mkdir -p "$SPARK_LOCAL_DIRS"

# Smaller heap is fine for the sample
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-4g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-4g}"

# Apply confs even when launching via `python script.py`
export PYSPARK_SUBMIT_ARGS="\
 --conf spark.sql.session.timeZone=UTC \
 --conf spark.sql.adaptive.enabled=true \
 --conf spark.sql.adaptive.coalescePartitions.enabled=true \
 --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=8m \
 --conf spark.sql.files.maxPartitionBytes=8m \
 --conf spark.sql.shuffle.partitions=256 \
 --conf spark.sql.adaptive.skewJoin.enabled=true \
 --conf spark.sql.adaptive.skewedPartitionThresholdInBytes=64m \
 --conf spark.sql.adaptive.skewedPartitionMaxSplitBytes=16m \
 --conf spark.local.dir=${SPARK_LOCAL_DIRS} \
 --driver-memory ${SPARK_DRIVER_MEMORY} \
 --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
 pyspark-shell"

echo "[check] ensuring sample dataset exists..."
if [[ ! -f "$SAMPLE" ]]; then
  echo "[download] fetching arXiv metadata + writing sample via KaggleHub..."
  # Creates data/raw/... and data/sample/arxiv-sample.jsonl
  python scripts/download_arxiv.py --sample 50000
fi

echo "[ingest] JSONL sample -> Parquet (partitioned by year)…"
python -m src.ingestion \
  --input "$SAMPLE" \
  --output "$OUT" \
  --partition-by year \
  --repartition 64 \
  --no-stats

echo "[eda] generating reports for SAMPLE data…"
python notebooks/eda.py \
  --parquet "$OUT" \
  --topk 20 \
  --abslen-sample-frac 0.05 \
  --outdir sample

echo "[done] sample artifacts in reports/sample/ (CSVs + PNGs)."


/home/data/akhalegh/utils/ccda-course-project/run.sh
================================================================================
#!/bin/bash
set -euo pipefail

RAW="data/raw/arxiv-metadata-oai-snapshot.json"
OUT="data/processed/arxiv_full"

# ---- Spark low-memory friendly settings for BOTH ingestion & EDA ----
export SPARK_LOCAL_DIRS="${SPARK_LOCAL_DIRS:-$(pwd)/data/tmp/spark-local}"
mkdir -p "$SPARK_LOCAL_DIRS"

# Give the local driver (and its single executor) more heap; keep tasks small
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-8g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-8g}"

# Ensure these confs are applied even when launching via `python script.py`
export PYSPARK_SUBMIT_ARGS="\
 --conf spark.sql.session.timeZone=UTC \
 --conf spark.sql.adaptive.enabled=true \
 --conf spark.sql.adaptive.coalescePartitions.enabled=true \
 --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=8m \
 --conf spark.sql.files.maxPartitionBytes=8m \
 --conf spark.sql.shuffle.partitions=800 \
 --conf spark.sql.adaptive.skewJoin.enabled=true \
 --conf spark.sql.adaptive.skewedPartitionThresholdInBytes=64m \
 --conf spark.sql.adaptive.skewedPartitionMaxSplitBytes=16m \
 --conf spark.local.dir=${SPARK_LOCAL_DIRS} \
 --driver-memory ${SPARK_DRIVER_MEMORY} \
 --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
 pyspark-shell"

echo "[check] ensuring raw dataset exists..."
if [[ ! -f "$RAW" ]]; then
  echo "[download] fetching arXiv metadata via KaggleHub..."
  python scripts/download_arxiv.py --sample 0
fi

echo "[ingest] JSON/JSONL -> Parquet (full)…"
python -m src.ingestion \
  --input "$RAW" \
  --output "$OUT" \
  --partition-by year \
  --repartition 200 \
  --no-stats

echo "[eda] generating reports for FULL data…"
python notebooks/eda.py \
  --parquet "$OUT" \
  --topk 30 \
  --abslen-sample-frac 0.02 \
  --outdir full

echo "[done] artifacts in reports/full/ (CSVs + PNGs)."


/home/data/akhalegh/utils/ccda-course-project/requirements.txt
================================================================================
pyspark==3.5.1
pandas
pyarrow
matplotlib
jupyter
kagglehub


/home/data/akhalegh/utils/ccda-course-project/notebooks/streaming_sample.py
================================================================================
#!/usr/bin/env python3
"""
Week 11 (Sample) Streaming Demo (10s trigger):
- Watches data/stream/incoming_sample/ for arxiv-sample-YYYYMMDDHHMM.jsonl
- Every 10 seconds (configurable), ingests new arrivals via Spark Structured Streaming
- Applies existing transforms (src/transformations.py)
- Writes per-drop reports to reports/streaming_sample/YYYYMMDD/ (CSV + PNG)

Run:
  python notebooks/streaming_sample.py
Options:
  --input DIR
  --checkpoint DIR
  --trigger-seconds N             # default: 10 (seconds)
  --max-files-per-trigger N       # default: 1
Stop:
  Ctrl+C
"""
from __future__ import annotations
import os
import re
from pathlib import Path
import argparse

import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from pyspark.sql import functions as F, types as T
from src.utils import get_spark
from src.transformations import transform_all

INCOMING = "data/stream/incoming_sample"
REPORTS_ROOT = Path("reports/streaming_sample")
CHECKPOINT = "data/stream/checkpoints/sample_week11"

# Accept 8- or 12-digit stamps; folder uses the first 8 (YYYYMMDD)
FILE_DATE_REGEX = re.compile(r".*arxiv-sample-(\d{8})(\d{4})?\.jsonl$")

# Explicit JSON schema so the stream starts immediately
JSON_SCHEMA = T.StructType([
    T.StructField("id", T.StringType()),
    T.StructField("title", T.StringType()),
    T.StructField("abstract", T.StringType()),
    T.StructField("categories", T.StringType()),
    T.StructField("doi", T.StringType()),
    T.StructField("journal-ref", T.StringType()),
    T.StructField("comments", T.StringType()),
    T.StructField("submitter", T.StringType()),
    T.StructField("update_date", T.StringType()),
    T.StructField("submitted_date", T.StringType()),
    T.StructField("authors", T.StringType()),
    T.StructField("authors_parsed", T.ArrayType(T.ArrayType(T.StringType()))),
    T.StructField("versions", T.ArrayType(T.MapType(T.StringType(), T.StringType()))),
])

def _save_df_as_csv(df_spark, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    df_spark.toPandas().to_csv(path, index=False)
    print(f"[saved] {path}")

def _plot_line(x, y, title, xlabel, ylabel, outpng: Path):
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.plot(x, y, marker="o")
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")

def _plot_bar(labels, values, title, xlabel, ylabel, outpng: Path, rotate_xticks=False):
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.bar(labels, values)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if rotate_xticks:
        plt.xticks(rotation=60, ha="right")
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")

def _per_drop_reports(batch_df, batch_id: int):
    """
    foreachBatch handler. batch_df contains only NEW files for this micro-batch.
    We detect the date stamp from input_file_name() and create one report folder per date.
    """
    if batch_df.rdd.isEmpty():
        print(f"[batch {batch_id}] empty batch")
        return

    df_with_name = batch_df.withColumn("source_file", F.input_file_name())
    df_with_date = df_with_name.withColumn(
        "source_date",
        F.regexp_extract(F.col("source_file"), r"arxiv-sample-(\d{8})(\d{4})?\.jsonl$", 1)
    )

    dates = [r["source_date"] for r in df_with_date.select("source_date").distinct().collect()]
    for d in dates:
        if not d:
            continue
        print(f"[batch {batch_id}] generating reports for date={d}")
        sub = df_with_date.filter(F.col("source_date") == F.lit(d)).drop("source_file", "source_date")

        # Apply Week-8 transforms
        transformed = transform_all(sub)

        outdir = REPORTS_ROOT / d
        outdir.mkdir(parents=True, exist_ok=True)

        # Papers per year
        by_year = transformed.groupBy("year").count().orderBy("year")
        _save_df_as_csv(by_year, outdir / "by_year.csv")
        py = by_year.toPandas().dropna()
        if not py.empty:
            _plot_line(py["year"], py["count"], "Papers per Year", "year", "count", outdir / "papers_per_year.png")

        # Top categories (Top-15)
        topcats = (
            transformed.groupBy("primary_category")
                       .count()
                       .orderBy(F.desc("count"))
                       .limit(15)
        )
        _save_df_as_csv(topcats, outdir / "top_categories.csv")
        pc = topcats.toPandas()
        if not pc.empty:
            _plot_bar(pc["primary_category"], pc["count"],
                      "Top Primary Categories", "primary_category", "count",
                      outdir / "top_categories.png", rotate_xticks=True)

        # DOI rate by year (if available)
        if "has_doi" in transformed.columns:
            doi_year = (transformed.groupBy("year")
                                  .agg(F.avg(F.col("has_doi").cast("double")).alias("doi_rate"))
                                  .orderBy("year"))
            _save_df_as_csv(doi_year, outdir / "doi_rate_by_year.csv")
            pd_doi = doi_year.toPandas().dropna()
            if not pd_doi.empty:
                _plot_line(pd_doi["year"], pd_doi["doi_rate"] * 100.0,
                           "DOI Coverage by Year (%)", "year", "doi rate (%)",
                           outdir / "doi_rate_by_year.png")

        print(f"[batch {batch_id}] reports → {outdir}/")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", default=INCOMING, help="Directory to watch for sample drops")
    ap.add_argument("--checkpoint", default=CHECKPOINT, help="Checkpoint dir")
    ap.add_argument("--trigger-seconds", type=int, default=10, help="Micro-batch frequency in seconds (default: 10)")
    ap.add_argument("--max-files-per-trigger", type=int, default=1, help="Limit files per micro-batch")
    args = ap.parse_args()

    spark = get_spark("streaming_sample_week11")
    spark.conf.set("spark.sql.streaming.schemaInference", "true")

    # Stream new JSONL files (explicit schema + pacing)
    stream_df = (
        spark.readStream
             .format("json")
             .schema(JSON_SCHEMA)
             .option("multiLine", "false")
             .option("maxFilesPerTrigger", str(max(args.max_files_per_trigger, 1)))
             .option("pathGlobFilter", "arxiv-sample-*.jsonl")
             .load(args.input)
    )

    q = (
        stream_df.writeStream
                 .foreachBatch(_per_drop_reports)
                 .option("checkpointLocation", args.checkpoint)
                 .trigger(processingTime=f"{max(args.trigger_seconds,1)} seconds")
                 .start()
    )

    print(f"[listen] Watching {args.input} every {args.trigger_seconds} second(s). Ctrl+C to stop.")
    try:
        q.awaitTermination()
    except KeyboardInterrupt:
        print("\n[stop] Stopping stream...")
        q.stop()

if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/notebooks/streaming.py
================================================================================
#!/usr/bin/env python3
"""
Week 11 (Full) Streaming-Oriented Runner (Daily Scheduler)

Behavior:
- Runs once per day (loop) OR just once with --once.
- On Sundays (local time), pulls the latest Kaggle arXiv snapshot via kagglehub,
  stages it to data/stream/incoming/arxiv-YYYYMMDD.json (YYYYMMDD = today's date),
  then processes that drop and writes CSV+PNG reports to reports/streaming_full/YYYYMMDD/.
- Maintains a small state file to avoid duplicate processing per date.

Run (daemon-ish):
  python notebooks/streaming.py

Run a single check (useful for cron/CI/manual trigger):
  python notebooks/streaming.py --once
"""
from __future__ import annotations
import os
import time
import shutil
import argparse
from pathlib import Path
from datetime import datetime, timedelta

import kagglehub
import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from pyspark.sql import functions as F
from src.utils import get_spark
from src.transformations import transform_all

# --- Locations ---
INCOMING = Path("data/stream/incoming")
REPORTS_ROOT = Path("reports/streaming_full")
STATE_DIR = Path("data/stream/state")
STATE_FILE = STATE_DIR / "last_full_run.txt"

# --- Helpers: time/date/state ---
def _today_stamp() -> str:
    return datetime.now().strftime("%Y%m%d")  # local time

def _is_sunday() -> bool:
    # Monday=0 ... Sunday=6
    return datetime.now().weekday() == 6

def _load_last_date() -> str | None:
    if STATE_FILE.exists():
        return STATE_FILE.read_text().strip() or None
    return None

def _save_last_date(stamp: str) -> None:
    STATE_DIR.mkdir(parents=True, exist_ok=True)
    STATE_FILE.write_text(stamp, encoding="utf-8")

def _sleep_until_next_day():
    now = datetime.now()
    tomorrow = (now + timedelta(days=1)).replace(hour=0, minute=0, second=5, microsecond=0)
    secs = max(10, int((tomorrow - now).total_seconds()))
    print(f"[sleep] {secs}s until next daily check…")
    time.sleep(secs)

# --- Kaggle download + staging ---
def _download_kaggle_snapshot() -> Path:
    """
    Use kagglehub to fetch the Cornell arXiv snapshot; return the path to the JSON/JSONL file.
    """
    print("[kaggle] Downloading Cornell-University/arxiv …")
    base = Path(kagglehub.dataset_download("Cornell-University/arxiv"))
    # Prefer JSONL if present, else JSON
    candidates = list(base.rglob("arxiv-metadata-oai-snapshot.jsonl")) + \
                 list(base.rglob("arxiv-metadata-oai-snapshot.json"))
    if not candidates:
        raise FileNotFoundError("Could not find arxiv-metadata-oai-snapshot.json(.jsonl) in Kaggle download.")
    src = candidates[0]
    print(f"[kaggle] Found {src}")
    return src

def _stage_incoming(src: Path, stamp: str) -> Path:
    """
    Copy the Kaggle file into our incoming/ folder as arxiv-YYYYMMDD.json
    (we use .json extension regardless, the content is JSON Lines.)
    """
    INCOMING.mkdir(parents=True, exist_ok=True)
    dst = INCOMING / f"arxiv-{stamp}.json"
    # write to temp then atomic move
    tmp = dst.with_suffix(dst.suffix + ".tmp")
    shutil.copyfile(src, tmp)
    os.replace(tmp, dst)
    print(f"[stage] {dst}")
    return dst

# --- Small plotting helpers ---
def _save_df_as_csv(df_spark, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    df_spark.toPandas().to_csv(path, index=False)
    print(f"[saved] {path}")

def _plot_line(x, y, title, xlabel, ylabel, outpng: Path):
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.plot(x, y, marker="o")
    ax.set_title(title); ax.set_xlabel(xlabel); ax.set_ylabel(ylabel)
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")

def _plot_bar(labels, values, title, xlabel, ylabel, outpng: Path, rotate_xticks=False):
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.bar(labels, values)
    ax.set_title(title); ax.set_xlabel(xlabel); ax.set_ylabel(ylabel)
    if rotate_xticks:
        plt.xticks(rotation=60, ha="right")
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")

# --- Processing per drop ---
def _process_drop(incoming_file: Path, stamp: str):
    """
    Batch process the just-downloaded file and write per-stamp reports.
    """
    print(f"[process] Generating reports for {stamp} from {incoming_file.name}")
    spark = get_spark("streaming_week11_full")

    # Kaggle file is typically JSONL (one object per line); read as non-multiline
    df_raw = (spark.read
                   .option("multiLine", "false")
                   .json(str(incoming_file)))

    df = transform_all(df_raw)
    outdir = REPORTS_ROOT / stamp
    outdir.mkdir(parents=True, exist_ok=True)

    # Papers per year
    by_year = df.groupBy("year").count().orderBy("year")
    _save_df_as_csv(by_year, outdir / "by_year.csv")
    py = by_year.toPandas().dropna()
    if not py.empty:
        _plot_line(py["year"], py["count"], "Papers per Year", "year", "count", outdir / "papers_per_year.png")

    # Top categories (Top-30)
    topcats = (
        df.groupBy("primary_category")
          .count()
          .orderBy(F.desc("count"))
          .limit(30)
    )
    _save_df_as_csv(topcats, outdir / "top_categories.csv")
    pc = topcats.toPandas()
    if not pc.empty:
        _plot_bar(pc["primary_category"], pc["count"],
                  "Top Primary Categories", "primary_category", "count",
                  outdir / "top_categories.png", rotate_xticks=True)

    # DOI rate by year
    if "has_doi" in df.columns:
        doi_year = (df.groupBy("year")
                      .agg(F.avg(F.col("has_doi").cast("double")).alias("doi_rate"))
                      .orderBy("year"))
        _save_df_as_csv(doi_year, outdir / "doi_rate_by_year.csv")
        pd_doi = doi_year.toPandas().dropna()
        if not pd_doi.empty:
            _plot_line(pd_doi["year"], pd_doi["doi_rate"] * 100.0,
                       "DOI Coverage by Year (%)", "year", "doi rate (%)",
                       outdir / "doi_rate_by_year.png")

    spark.stop()
    print(f"[done] Reports → {outdir}/")

# --- One-shot Sunday check ---
def run_once():
    today = _today_stamp()
    last = _load_last_date()

    if not _is_sunday():
        print("[skip] Today is not Sunday; no weekly pull.")
        return

    if last == today:
        print(f"[skip] Already processed {today}.")
        return

    try:
        src = _download_kaggle_snapshot()
        dst = _stage_incoming(src, today)
        _process_drop(dst, today)
        _save_last_date(today)
    except Exception as e:
        print(f"[error] {e}")

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--once", action="store_true",
                    help="Run a single check (good for cron/CI/tests) and exit.")
    args = ap.parse_args()

    INCOMING.mkdir(parents=True, exist_ok=True)
    REPORTS_ROOT.mkdir(parents=True, exist_ok=True)
    STATE_DIR.mkdir(parents=True, exist_ok=True)

    if args.once:
        run_once()
        return

    print("[loop] Daily scheduler started. Will attempt on Sundays.")
    while True:
        run_once()
        _sleep_until_next_day()

if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/notebooks/eda.py
================================================================================
# notebooks/eda.py
"""
Comprehensive EDA for the arXiv metadata after ingestion.

Usage:
  python notebooks/eda.py --parquet data/processed/arxiv_full
  # or for the small sample:
  python notebooks/eda.py --parquet data/processed/arxiv_parquet

This script will:
  - Print schema and table-level summaries to stdout
  - Write compact CSV summaries to reports/<run_type>/
  - Save several matplotlib charts to reports/<run_type>/*.png

Notes:
  - <run_type> is "full" if the parquet path contains "full", otherwise "sample"
    (override with --outdir if you want a custom folder).
  - Plots avoid loading huge data to the driver by using aggregated Spark DF results.
"""

from __future__ import annotations
import argparse
from pathlib import Path

import pandas as pd
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession, functions as F


# ---------- helpers ----------

def pick_outdir(parquet_path: str, user_outdir: str | None) -> Path:
    if user_outdir:
        out = Path("reports") / user_outdir
    else:
        run_type = "full" if "full" in parquet_path else "sample"
        out = Path("reports") / run_type
    out.mkdir(parents=True, exist_ok=True)
    return out

def save_df_as_csv(df_spark, path: Path):
    # All saved tables are small (aggregations), safe to convert to pandas
    df_spark.toPandas().to_csv(path, index=False)
    print(f"[saved] {path}")

def matplotlib_savefig(path: Path):
    plt.tight_layout()
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"[plot] {path}")


# ---------- core EDA ----------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help="Path to Parquet output from src/ingestion.py")
    ap.add_argument("--topk", type=int, default=20, help="Top-K categories/authors to show")
    ap.add_argument("--abslen-sample-frac", type=float, default=0.05,
                    help="Sampling fraction for abstract length histogram (0<frac<=1)")
    ap.add_argument("--outdir", default=None,
                    help="Optional subfolder name under reports/. If not set, auto-chooses 'full' or 'sample'.")
    args = ap.parse_args()

    outdir = pick_outdir(args.parquet, args.outdir)

    spark = SparkSession.builder.appName("arxiv_week8_eda").getOrCreate()
    df = spark.read.parquet(args.parquet)

    # Basic info
    print("\n=== SCHEMA ===")
    df.printSchema()

    n_rows = df.count()
    n_cols = len(df.columns)
    print(f"\n=== SIZE ===\nrows={n_rows:,}  cols={n_cols}")

    
    print("\n=== COMPLETENESS (non-null %) ===")

    # compute row count first (single number)
    n_rows = df.count()
    n_cols = len(df.columns)
    print(f"\n=== SIZE ===\nrows={n_rows:,}  cols={n_cols}")

    # Do small per-column jobs to avoid a single wide aggregation that can OOM
    rows = []
    for c in df.columns:
        # count non-nulls in a tiny job
        nn = df.select(F.count(F.when(F.col(c).isNotNull(), 1)).alias("nn")).collect()[0]["nn"]
        pct = float(nn) / n_rows * 100.0 if n_rows else 0.0
        rows.append((c, int(nn), pct))

    comp_pd = pd.DataFrame(rows, columns=["column", "non_null", "non_null_pct"])\
                .sort_values("non_null_pct", ascending=False)

    comp_csv = outdir / "completeness.csv"
    comp_pd.to_csv(comp_csv, index=False)
    print(comp_pd.head(20))
    print(f"[saved] {comp_csv}")

    # DISTINCT COUNTS (selected)
    print("\n=== DISTINCT COUNTS (selected) ===")
    selected = ["arxiv_id", "primary_category", "year", "doi", "submitter"]
    distinct_rows = []
    for c in selected:
        if c in df.columns:
            distinct_rows.append((c, df.select(c).distinct().count()))
    distinct_pd = pd.DataFrame(distinct_rows, columns=["column", "distinct_count"]).sort_values("distinct_count", ascending=False)
    distinct_csv = outdir / "distinct_selected.csv"
    distinct_pd.to_csv(distinct_csv, index=False)
    print(distinct_pd)
    print(f"[saved] {distinct_csv}")

    # TEXT LENGTH STATS
    print("\n=== TEXT LENGTH STATS (title_len, abstract_len) ===")
    num_stats = df.select("title_len", "abstract_len").summary("count", "min", "25%", "50%", "75%", "max", "mean")
    save_df_as_csv(num_stats, outdir / "text_length_summary.csv")
    num_stats.show(truncate=False)

    # TOP CATEGORIES (bar)
    print("\n=== TOP CATEGORIES ===")
    topcats = (
        df.groupBy("primary_category")
          .count()
          .orderBy(F.desc("count"))
          .limit(args.topk)
    )
    save_df_as_csv(topcats, outdir / "top_categories.csv")
    topcats_pd = topcats.toPandas()
    if not topcats_pd.empty:
        plt.figure(figsize=(10, 5))
        topcats_pd = topcats_pd.sort_values("count", ascending=False)
        plt.bar(topcats_pd["primary_category"], topcats_pd["count"])
        plt.xticks(rotation=60, ha="right")
        plt.title(f"Top {len(topcats_pd)} Primary Categories")
        plt.xlabel("primary_category")
        plt.ylabel("count")
        matplotlib_savefig(outdir / "top_categories.png")

    # PAPERS PER YEAR (line)
    print("\n=== PAPERS PER YEAR ===")
    by_year = df.groupBy("year").count().orderBy("year")
    save_df_as_csv(by_year, outdir / "by_year.csv")
    by_year_pd = by_year.toPandas().dropna()
    if not by_year_pd.empty:
        plt.figure(figsize=(9, 4))
        plt.plot(by_year_pd["year"], by_year_pd["count"])
        plt.title("Papers per Year")
        plt.xlabel("year")
        plt.ylabel("count")
        matplotlib_savefig(outdir / "papers_per_year.png")

    # CATEGORY x YEAR HEATMAP (for top-K categories only)
    print("\n=== CATEGORY x YEAR HEATMAP (top categories) ===")
    top_cats_list = topcats_pd["primary_category"].tolist() if not topcats_pd.empty else []
    if top_cats_list:
        cat_year = (
            df.where(F.col("primary_category").isin(top_cats_list))
              .groupBy("primary_category", "year")
              .count()
        )
        cat_year_pd = cat_year.toPandas().pivot_table(index="primary_category", columns="year", values="count", fill_value=0)
        if not cat_year_pd.empty:
            plt.figure(figsize=(12, max(4, len(top_cats_list) * 0.35)))
            plt.imshow(cat_year_pd.values, aspect="auto")
            plt.yticks(range(len(cat_year_pd.index)), cat_year_pd.index)
            plt.xticks(range(len(cat_year_pd.columns)), cat_year_pd.columns, rotation=60, ha="right")
            plt.title("Counts by Primary Category (rows) and Year (cols)")
            plt.colorbar()
            matplotlib_savefig(outdir / "heatmap_category_year.png")
            cat_year_pd.to_csv(outdir / "category_year_matrix.csv")
            print(f"[saved] {outdir / 'category_year_matrix.csv'}")

    # ABSTRACT LENGTH DISTRIBUTION (hist; sampled)
    print("\n=== ABSTRACT LENGTH DISTRIBUTION ===")
    frac = args.abslen_sample_frac
    if frac > 0 and frac <= 1:
        abs_len_pd = df.select("abstract_len").sample(False, frac, seed=42).toPandas()
        if not abs_len_pd.empty:
            plt.figure(figsize=(8, 4))
            plt.hist(abs_len_pd["abstract_len"].dropna(), bins=50)
            plt.title(f"Abstract Length Distribution (sampled frac={frac})")
            plt.xlabel("abstract_len")
            plt.ylabel("frequency")
            matplotlib_savefig(outdir / "abstract_length_hist.png")

    # DOI AVAILABILITY BY YEAR (line)
    print("\n=== DOI AVAILABILITY BY YEAR ===")
    if "has_doi" in df.columns:
        has_doi_by_year = (
            df.groupBy("year")
              .agg(F.avg(F.col("has_doi").cast("double")).alias("doi_rate"))
              .orderBy("year")
        )
        save_df_as_csv(has_doi_by_year, outdir / "doi_rate_by_year.csv")
        doi_pd = has_doi_by_year.toPandas().dropna()
        if not doi_pd.empty:
            plt.figure(figsize=(9, 4))
            plt.plot(doi_pd["year"], (doi_pd["doi_rate"] * 100.0))
            plt.title("DOI Coverage by Year (%)")
            plt.xlabel("year")
            plt.ylabel("doi rate (%)")
            matplotlib_savefig(outdir / "doi_rate_by_year.png")

    # TOP AUTHORS (bar)
    if "authors_list" in df.columns:
        print("\n=== TOP AUTHORS (by paper count) ===")
        exploded = df.select(F.explode_outer("authors_list").alias("author"))
        top_authors = exploded.groupBy("author").count().orderBy(F.desc("count")).limit(args.topk)
        save_df_as_csv(top_authors, outdir / "top_authors.csv")

        top_authors_pd = top_authors.toPandas()
        if not top_authors_pd.empty:
            plt.figure(figsize=(10, 5))
            top_authors_pd = top_authors_pd.sort_values("count", ascending=False)
            plt.bar(top_authors_pd["author"], top_authors_pd["count"])
            plt.xticks(rotation=60, ha="right")
            plt.title(f"Top {len(top_authors_pd)} Authors by Paper Count")
            plt.xlabel("author")
            plt.ylabel("count")
            matplotlib_savefig(outdir / "top_authors.png")

    # VERSION COUNT PER PAPER (bar)
    print("\n=== VERSION COUNT PER PAPER ===")
    if "versions" in df.columns:
        df_versions = df.withColumn("n_versions", F.size("versions"))
        vhist = df_versions.groupBy("n_versions").count().orderBy("n_versions")
        save_df_as_csv(vhist, outdir / "version_count_hist.csv")

        vhist_pd = vhist.toPandas()
        if not vhist_pd.empty:
            plt.figure(figsize=(8, 4))
            plt.bar(vhist_pd["n_versions"], vhist_pd["count"])
            plt.title("Number of Versions per Paper")
            plt.xlabel("n_versions")
            plt.ylabel("count")
            matplotlib_savefig(outdir / "version_count_hist.png")

    # CATEGORY PARETO (cumulative %)
    print("\n=== CATEGORY PARETO (CUMULATIVE %) ===")
    cat_counts = df.groupBy("primary_category").count()
    cat_counts_pd = cat_counts.toPandas().sort_values("count", ascending=False)
    if not cat_counts_pd.empty:
        cat_counts_pd["cum_pct"] = (cat_counts_pd["count"].cumsum() / cat_counts_pd["count"].sum()) * 100.0
        cat_counts_pd.to_csv(outdir / "category_pareto.csv", index=False)
        plt.figure(figsize=(10, 4))
        plt.plot(range(1, len(cat_counts_pd) + 1), cat_counts_pd["cum_pct"])
        plt.title("Cumulative Share of Papers by Category (Pareto)")
        plt.xlabel("rank of category")
        plt.ylabel("cumulative % of papers")
        matplotlib_savefig(outdir / "category_pareto.png")

    print(f"\n[done] EDA artifacts written to {outdir}/")
    spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/notebooks/ml.py
================================================================================
#!/usr/bin/env python3
import argparse, os
from typing import List, Tuple
from pyspark.sql import SparkSession, functions as F, Row, Window
from pyspark.ml import PipelineModel
from pyspark.ml.linalg import SparseVector
from src.similarity import _dot_udf
from src.query import query_topk

# ---------- Spark session ----------
def make_spark():
    try:
        from src.utils import get_spark
        return get_spark(app_name="ml_week12_full")
    except Exception:
        return (
            SparkSession.builder
            .appName("ml_week12_full")
            .config("spark.sql.shuffle.partitions", "1024")
            .config("spark.sql.files.maxPartitionBytes", 256 << 20)
            .getOrCreate()
        )

# ---------- CLI ----------
def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["eval", "query"], required=True)
    ap.add_argument("--model-dir", required=True)
    ap.add_argument("--split-parquet", help="Path with split=train|test partitions")
    ap.add_argument("--features", help="Root where features were saved by training")
    ap.add_argument("--features-train", help="Path to features for split=train (for --mode query)")
    ap.add_argument("--out", required=True)
    ap.add_argument("--k", type=int, default=5)
    ap.add_argument("--strategy", choices=["exact_broadcast", "block_cat"], default="block_cat")
    ap.add_argument("--query-title")
    ap.add_argument("--query-abstract")
    ap.add_argument("--eval-max-test", type=int, default=20000)
    ap.add_argument("--seed", type=int, default=42)
    return ap.parse_args()

# ---------- IO helpers ----------
def load_features(spark, features_root, split):
    path = os.path.join(features_root, f"split={split}")
    return spark.read.parquet(path)


def explode_cats(df, col="categories"):
    dtypes = dict(df.dtypes)
    coltype = dtypes.get(col, "")
    if coltype.startswith("array"):
        tmp = df.withColumn("cat", F.explode_outer(F.col(col)))
    else:
        tmp = (
            df.withColumn("_cats", F.split(F.coalesce(F.col(col).cast("string"), F.lit("")), r"\\s+"))
              .withColumn("cat", F.explode_outer(F.col("_cats")))
              .drop("_cats")
        )
    return tmp.where((F.col("cat").isNotNull()) & (F.col("cat") != ""))


# ---------- Exact Top‑K with broadcast (small train only) ----------
def _dot(a: SparseVector, b: SparseVector) -> float:
    if a is None or b is None:
        return 0.0
    ai = dict(zip(a.indices, a.values))
    s = 0.0
    for j, v in zip(b.indices, b.values):
        if j in ai:
            s += ai[j] * v
    return float(s)


def topk_broadcast_exact(spark, test_df, train_df, k: int = 5, exclude_self: bool = True):
    train_local: List[Tuple[str, List[str], SparseVector]] = []
    for r in train_df.select("id_base", "categories", "features").toLocalIterator():
        train_local.append((r["id_base"], r["categories"], r["features"]))
    bc_train = spark.sparkContext.broadcast(train_local)

    def score_partition(iter_rows):
        import heapq as _heapq
        out_rows: List[Row] = []
        train_list = bc_train.value
        for r in iter_rows:
            tid = r["id_base"]; tfeat = r["features"]
            heap: List[Tuple[float, str, List[str]]] = []
            for nid, ncats, nfeat in train_list:
                if exclude_self and nid == tid:
                    continue
                # cosine on L2‑normed vectors == dot
                s = _dot(tfeat, nfeat)
                if len(heap) < k:
                    _heapq.heappush(heap, (s, nid, ncats))
                else:
                    if s > heap[0][0]:
                        _heapq.heapreplace(heap, (s, nid, ncats))
            for rank_idx, (score, nid, ncats) in enumerate(sorted(heap, key=lambda x: (-x[0], x[1])), start=1):
                out_rows.append(Row(test_id=tid, rank=rank_idx, neighbor_id=nid, score=score, neighbor_categories=ncats))
        return iter(out_rows)

    rdd = test_df.select("id_base", "features", "categories").rdd.mapPartitions(score_partition)
    return spark.createDataFrame(rdd)


# ---------- Block-by-category Top‑K (scales) ----------

def topk_block_by_category(test_df, train_df, k: int = 5, exclude_self: bool = True):
    t = explode_cats(test_df.select("id_base", "categories", "features")).select(
        F.col("id_base").alias("tid"), "cat", F.col("features").alias("tfeat")
    )
    c = explode_cats(train_df.select("id_base", "categories", "features")).select(
        F.col("id_base").alias("nid"), "cat", F.col("features").alias("nfeat"), F.col("cat").alias("ncat")
    )
    joined = (
        t.join(F.broadcast(c), on="cat", how="inner")
         .withColumn("score", _dot_udf(F.col("tfeat"), F.col("nfeat")))
    )
    if exclude_self:
        joined = joined.where(F.col("tid") != F.col("nid"))

    w = Window.partitionBy("tid").orderBy(F.desc("score"), F.col("nid"))
    top = (
        joined.withColumn("rank", F.row_number().over(w))
              .where(F.col("rank") <= k)
              .groupBy("tid", "rank", "nid")
              .agg(F.max("score").alias("score"))
              .withColumnRenamed("tid", "test_id")
              .withColumnRenamed("nid", "neighbor_id")
    )
    # Attach neighbor categories for reporting
    neigh = train_df.select(F.col("id_base").alias("nid"), F.col("categories").alias("neighbor_categories"))
    return top.join(neigh, top["neighbor_id"] == neigh["nid"], "left").drop("nid")


# ---------- Attach neighbor arXiv IDs ----------

def _attach_neighbor_paper_id(recs, train):
    id_map = train.select(
        F.col("id_base").alias("nid"),
        F.col("paper_id").alias("neighbor_paper_id")
    )
    return recs.join(id_map, recs.neighbor_id == id_map.nid, "left").drop("nid")


# ---------- Modes ----------

def eval_mode(spark, args):
    os.makedirs(args.out, exist_ok=True)
    _ = PipelineModel.load(args.model_dir)  # parity check

    test_full = load_features(spark, args.features, "test")
    train = load_features(spark, args.features, "train").select("id_base", "categories", "features", "paper_id")

    test = (
        test_full.orderBy(F.rand(args.seed)).limit(args.eval_max_test)
        .select("id_base", "categories", "features")
    ).cache(); _ = test.count()

    if args.strategy == "exact_broadcast":
        recs = topk_broadcast_exact(spark, test, train, k=args.k, exclude_self=True)
    else:
        recs = topk_block_by_category(test, train, k=args.k, exclude_self=True)
    recs = recs.cache(); _ = recs.count()

    recs = _attach_neighbor_paper_id(recs, train).cache(); _ = recs.count()

    recs_to_save = (
        recs.withColumn(
                "neighbor_categories",
                F.array_join(F.col("neighbor_categories").cast("array<string>"), " ")
            )
            .select("test_id", "rank", "neighbor_id", "neighbor_paper_id", "score", "neighbor_categories")
            .orderBy("test_id", "rank")
    )
    recs_out = os.path.join(args.out, "recs_topk.csv")
    recs_to_save.write.mode("overwrite").option("header", True).csv(recs_out)

    # ---- Metrics (same as sample, computed on block results) ----
    test_c = (
        explode_cats(test.select("id_base", "categories").withColumnRenamed("id_base", "tid"))
        .select("tid", F.col("cat").alias("cat_test"))
    )
    train_c = (
        explode_cats(train.select("id_base", "categories").withColumnRenamed("id_base", "nid"))
        .select("nid", F.col("cat").alias("cat_train"))
    )

    overlap = (
        recs.join(train_c, recs.neighbor_id == train_c.nid, "left")
            .join(test_c, recs.test_id == test_c.tid, "left")
            .where(F.col("cat_train").isNotNull() & F.col("cat_test").isNotNull() & (F.col("cat_train") == F.col("cat_test")))
            .groupBy("test_id", "neighbor_id", "rank")
            .agg(F.countDistinct("cat_train").alias("overlap_ct"))
    )

    rel = (
        recs.join(overlap, on=["test_id", "neighbor_id", "rank"], how="left")
            .withColumn("rel", F.when(F.col("overlap_ct") > 0, 1).otherwise(0))
            .cache()
    ); _ = rel.count()

    relevant = (
        test_c.join(train_c, test_c.cat_test == train_c.cat_train, "inner")
               .where(F.col("tid") != F.col("nid"))
               .groupBy("tid").agg(F.countDistinct("nid").alias("relevant_total"))
    )

    rel = (
        rel.join(relevant, rel.test_id == relevant.tid, "left").drop("tid").fillna({"relevant_total": 0})
    ).cache(); _ = rel.count()

    w = Window.partitionBy("test_id").orderBy("rank")
    rel = rel.withColumn("cum_rel", F.sum("rel").over(w)).cache(); _ = rel.count()

    per_test = rel.groupBy("test_id").agg(
        F.sum("rel").alias("sum_rel"),
        F.max("relevant_total").alias("relevant_total"),
    )

    prec = per_test.select("test_id", (F.col("sum_rel") / F.lit(args.k)).alias("precision_at_k"))
    recall = per_test.select(
        "test_id",
        F.when(F.col("relevant_total") > 0, F.col("sum_rel") / F.col("relevant_total")).otherwise(F.lit(None)).alias("recall_at_k"),
    )

    ap = (
        rel.withColumn("prec_at_i", F.col("cum_rel") / F.col("rank"))
           .where(F.col("rel") == 1)
           .groupBy("test_id")
           .agg(F.avg("prec_at_i").alias("ap_at_k"))
    )

    first_hit = rel.where(F.col("rel") == 1).groupBy("test_id").agg(F.min("rank").alias("first_rank"))
    mrr_val = first_hit.select(F.avg(1.0 / F.col("first_rank")).alias("mrr_at_k")).first()["mrr_at_k"]

    cov = rel.groupBy("test_id").agg((F.count("*") >= args.k).cast("int").alias("has_k"))
    coverage_at_k = cov.agg(F.avg("has_k").alias("coverage_at_k")).first()["coverage_at_k"]

    metrics = (
        prec.join(recall, "test_id", "outer").join(ap, "test_id", "outer")
    )

    ild_mean = F.lit(None).alias("intra_list_diversity")  # optional to compute item‑item

    macro = (
        metrics.agg(
            F.avg("precision_at_k").alias("precision_at_k"),
            F.avg("recall_at_k").alias("recall_at_k"),
            F.avg("ap_at_k").alias("map_at_k"),
        )
        .withColumn("mrr_at_k", F.lit(mrr_val))
        .withColumn("coverage_at_k", F.lit(coverage_at_k))
        .withColumn("intra_list_diversity", ild_mean)
    )

    out_csv = os.path.join(args.out, "metrics_at_k.csv")
    macro.write.mode("overwrite").option("header", True).csv(out_csv)

    # Qualitative examples
    test_full = load_features(spark, args.features, "test")
    qual = (
        recs.join(
            test_full.select(F.col("id_base").alias("test_id"), "title", "abstract", "categories"),
            "test_id",
        ).orderBy("test_id", "rank").limit(100)
    )

    qpath = os.path.join(args.out, "qualitative_examples.md")
    rows = [
        (
            r["test_id"],
            r["rank"],
            r["neighbor_id"],
            r["score"],
            r["neighbor_categories"],
        )
        for r in qual.select("test_id", "rank", "neighbor_id", "score", "neighbor_categories").collect()
    ]
    with open(qpath, "w") as f:
        f.write("# Qualitative Examples (Top‑K)\n\n")
        current = None
        for tid, rank, nid, score, ncats in rows:
            if current != tid:
                current = tid
                f.write(f"\n## Test: {tid}\n\n")
            cats_str = " ".join(ncats) if isinstance(ncats, list) else str(ncats)
            f.write(f"- k={int(rank)} → **{nid}** (cos={float(score):.3f}) cats={cats_str}\n")


def query_mode(spark, args):
    os.makedirs(args.out, exist_ok=True)
    model = PipelineModel.load(args.model_dir)
    train = spark.read.parquet(args.features_train)

    recs = query_topk(
        spark,
        model,
        train.select("id_base", "categories", "features"),
        args.query_title or "",
        args.query_abstract or "",
        k=args.k,
    )

    recs = _attach_neighbor_paper_id(recs, train)

    recs_to_save = (
        recs.withColumn(
                "neighbor_categories",
                F.array_join(F.col("neighbor_categories").cast("array<string>"), " ")
            )
            .select("test_id", "rank", "neighbor_id", "neighbor_paper_id", "score", "neighbor_categories")
            .orderBy("rank")
    )

    out_csv = os.path.join(args.out, "query_topK.csv")
    recs_to_save.write.mode("overwrite").option("header", True).csv(out_csv)


# ---------- main ----------

def main():
    args = parse_args()
    spark = make_spark()
    if args.mode == "eval":
        if not args.features:
            raise ValueError("--features is required for eval")
        eval_mode(spark, args)
    else:
        query_mode(spark, args)
    spark.stop()


if __name__ == "__main__":
    main()

/home/data/akhalegh/utils/ccda-course-project/notebooks/ml_sample.py
================================================================================
#!/usr/bin/env python3
import argparse, os, heapq
from typing import List, Tuple
from pyspark.sql import SparkSession, functions as F, Row, Window
from pyspark.ml import PipelineModel
from pyspark.ml.linalg import SparseVector
from src.similarity import _dot_udf  # for ILD pair calc
from src.query import query_topk

# ---------- Spark session ----------
def make_spark():
    try:
        from src.utils import get_spark
        return get_spark(app_name="ml_sample_week12")
    except Exception:
        return (SparkSession.builder
                .appName("ml_sample_week12")
                .config("spark.sql.shuffle.partitions", "64")
                .getOrCreate())

# ---------- CLI ----------
def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--mode", choices=["eval", "query"], required=True)
    ap.add_argument("--model-dir", required=True)
    ap.add_argument("--split-parquet", help="Path with split=train|test partitions")
    ap.add_argument("--features", help="Root where features were saved by training")
    ap.add_argument("--features-train", help="Path to features for split=train (for --mode query)")
    ap.add_argument("--out", required=True)
    ap.add_argument("--k", type=int, default=3)
    ap.add_argument("--strategy", choices=["exact"], default="exact")
    ap.add_argument("--query-title")
    ap.add_argument("--query-abstract")
    ap.add_argument("--eval-max-test", type=int, default=200,
                    help="Max number of test items to evaluate (random subset).")
    ap.add_argument("--seed", type=int, default=42)
    return ap.parse_args()

# ---------- IO helpers ----------
def load_features(spark, features_root, split):
    path = os.path.join(features_root, f"split={split}")
    return spark.read.parquet(path)

def explode_cats(df, col="categories"):
    """
    Robustly explode categories whether it's array<string> or string like "cs.LG cs.AI".
    Produces a 'cat' column with non-empty strings.
    """
    dtypes = dict(df.dtypes)
    coltype = dtypes.get(col, "")
    if coltype.startswith("array"):
        tmp = df.withColumn("cat", F.explode_outer(F.col(col)))
    else:
        tmp = (df.withColumn("_cats",
                             F.split(F.coalesce(F.col(col).cast("string"), F.lit("")), r"\s+"))
                 .withColumn("cat", F.explode_outer(F.col("_cats")))
                 .drop("_cats"))
    return tmp.where((F.col("cat").isNotNull()) & (F.col("cat") != ""))

def _attach_neighbor_paper_id(recs, train):
    id_map = train.select(
        F.col("id_base").alias("nid"),
        F.col("paper_id").alias("neighbor_paper_id")
    )
    return recs.join(id_map, recs.neighbor_id == id_map.nid, "left").drop("nid")

# ---------- Broadcast Top-K exact (no crossJoin) ----------
def _dot(a: SparseVector, b: SparseVector) -> float:
    if a is None or b is None:
        return 0.0
    ai = dict(zip(a.indices, a.values))
    s = 0.0
    for j, v in zip(b.indices, b.values):
        if j in ai:
            s += ai[j] * v
    return float(s)

def topk_broadcast_exact(spark: SparkSession,
                         test_df,
                         train_df,
                         k: int = 3,
                         exclude_self: bool = True):
    """
    Compute Top-K exact cosine by broadcasting TRAIN to all executors
    and doing per-partition Python scoring for TEST. Assumes features are L2-normalized.
    Returns a DataFrame with schema:
      (test_id, rank, neighbor_id, score, neighbor_categories)
    """
    train_local: List[Tuple[str, List[str], SparseVector]] = []
    for r in train_df.select("id_base", "categories", "features").toLocalIterator():
        train_local.append((r["id_base"], r["categories"], r["features"]))
    bc_train = spark.sparkContext.broadcast(train_local)

    def score_partition(iter_rows):
        import heapq as _heapq
        train_list = bc_train.value
        out_rows: List[Row] = []

        for r in iter_rows:
            tid = r["id_base"]
            tfeat = r["features"]
            # Maintain a min-heap of size k: (score, neighbor_id, neighbor_cats)
            heap: List[Tuple[float, str, List[str]]] = []
            for nid, ncats, nfeat in train_list:
                if exclude_self and nid == tid:
                    continue
                s = _dot(tfeat, nfeat)
                if len(heap) < k:
                    _heapq.heappush(heap, (s, nid, ncats))
                else:
                    if s > heap[0][0]:
                        _heapq.heapreplace(heap, (s, nid, ncats))
            # Sort by descending score, tiebreak by neighbor_id for determinism
            top = sorted(heap, key=lambda x: (-x[0], x[1]))
            for rank_idx, (score, nid, ncats) in enumerate(top, start=1):
                out_rows.append(Row(test_id=tid,
                                    rank=rank_idx,
                                    neighbor_id=nid,
                                    score=score,
                                    neighbor_categories=ncats))
        return iter(out_rows)

    test_small = test_df.select("id_base", "categories", "features")
    rdd = test_small.rdd.mapPartitions(score_partition)
    recs = spark.createDataFrame(rdd)
    return recs

# ---------- Modes ----------
def eval_mode(spark, args):
    os.makedirs(args.out, exist_ok=True)
    _ = PipelineModel.load(args.model_dir)  # load parity

    test_full = load_features(spark, args.features, "test")
    train = load_features(spark, args.features, "train").select(
        "id_base", "categories", "features", "paper_id"
    )

    # Deterministic sample of TEST to keep cost predictable
    test = (test_full
            .orderBy(F.rand(args.seed))
            .limit(args.eval_max_test)
            .select("id_base", "categories", "features"))

    test = test.cache(); _ = test.count()

    # ---- Top-K via broadcast (no crossJoin) ----
    recs = topk_broadcast_exact(
        spark, test, train, k=args.k, exclude_self=True
    ).cache()
    _ = recs.count()

    # Attach canonical arXiv IDs
    recs = _attach_neighbor_paper_id(recs, train).cache(); _ = recs.count()

    # Save recs
    recs_to_save = (
        recs
        .withColumn("neighbor_categories",
                    F.array_join(F.col("neighbor_categories").cast("array<string>"), " "))
        .select("test_id", "rank", "neighbor_id", "neighbor_paper_id", "score", "neighbor_categories")
        .orderBy("test_id", "rank")
    )
    recs_out = os.path.join(args.out, "recs_topk.csv")
    (recs_to_save.write.mode("overwrite").option("header", True).csv(recs_out))

    # ---- Metrics & analysis (robust, no ambiguous cols, proper grouping) ----
    test_c = (explode_cats(test.select("id_base", "categories")
                           .withColumnRenamed("id_base","tid"))
              .select("tid", F.col("cat").alias("cat_test")))
    train_c = (explode_cats(train.select("id_base", "categories")
                            .withColumnRenamed("id_base","nid"))
               .select("nid", F.col("cat").alias("cat_train")))

    # Overlap counts where cat matches exactly
    overlap = (recs.join(train_c, recs.neighbor_id == train_c.nid, "left")
                    .join(test_c, recs.test_id == test_c.tid, "left")
                    .where(F.col("cat_train").isNotNull() &
                           F.col("cat_test").isNotNull() &
                           (F.col("cat_train") == F.col("cat_test")))
                    .groupBy("test_id","neighbor_id","rank")
                    .agg(F.countDistinct("cat_train").alias("overlap_ct")))

    rel = (recs.join(overlap, on=["test_id","neighbor_id","rank"], how="left")
               .withColumn("rel", F.when(F.col("overlap_ct") > 0, 1).otherwise(0))
               .cache()); _ = rel.count()

    # For recall denominator, count distinct relevant neighbors per test
    relevant = (test_c.join(train_c, test_c.cat_test == train_c.cat_train, "inner")
                      .where(F.col("tid") != F.col("nid"))
                      .groupBy("tid")
                      .agg(F.countDistinct("nid").alias("relevant_total")))

    rel = (rel.join(relevant, rel.test_id == relevant.tid, "left")
             .drop("tid").fillna({"relevant_total": 0})).cache()

    # cum_rel for AP
    w = Window.partitionBy("test_id").orderBy("rank")
    rel = rel.withColumn("cum_rel", F.sum("rel").over(w)).cache()

    # ---- Precision/Recall computed from per-test aggregates (no missing-aggregation) ----
    per_test = rel.groupBy("test_id").agg(
        F.sum("rel").alias("sum_rel"),
        F.max("relevant_total").alias("relevant_total")  # constant per test_id
    )

    prec = per_test.select(
        "test_id",
        (F.col("sum_rel")/F.lit(args.k)).alias("precision_at_k")
    )

    recall = per_test.select(
        "test_id",
        F.when(F.col("relevant_total") > 0,
               F.col("sum_rel")/F.col("relevant_total")).otherwise(F.lit(None)).alias("recall_at_k")
    )

    # MAP@K
    ap = (rel.withColumn("prec_at_i", F.col("cum_rel")/F.col("rank"))
              .where(F.col("rel") == 1)
              .groupBy("test_id")
              .agg(F.avg("prec_at_i").alias("ap_at_k")))

    # MRR
    first_hit = rel.where(F.col("rel") == 1).groupBy("test_id").agg(F.min("rank").alias("first_rank"))
    mrr_val = first_hit.select(F.avg(1.0/F.col("first_rank")).alias("mrr_at_k")).first()["mrr_at_k"]

    # Coverage@K: had >=K recs
    cov = rel.groupBy("test_id").agg((F.count("*") >= args.k).cast("int").alias("has_k"))
    coverage_at_k = cov.agg(F.avg("has_k").alias("coverage_at_k")).first()["coverage_at_k"]

    # Intra-list diversity
    neigh_feats = train.select(F.col("id_base").alias("nid"), F.col("features").alias("nfeat"))
    rk = (recs.join(neigh_feats, recs.neighbor_id == neigh_feats.nid, "left")
               .select("test_id","rank","neighbor_id", F.col("nfeat").alias("feat")))
    a = rk.alias("a"); b = rk.alias("b")
    pairs = (a.join(b, (a.test_id==b.test_id) & (a.rank < b.rank))
               .withColumn("pair_cosine", _dot_udf(F.col("a.feat"), F.col("b.feat"))))
    ild_mean = pairs.groupBy("test_id").agg(F.avg("pair_cosine").alias("ild_mean"))

    # Collect macro metrics
    metrics = (prec.join(recall, "test_id", "outer")
                    .join(ap, "test_id", "outer")
                    .join(ild_mean, "test_id", "left"))

    macro = (metrics.agg(
                F.avg("precision_at_k").alias("precision_at_k"),
                F.avg("recall_at_k").alias("recall_at_k"),
                F.avg("ap_at_k").alias("map_at_k"),
                F.avg("ild_mean").alias("intra_list_diversity"))
             .withColumn("mrr_at_k", F.lit(mrr_val))
             .withColumn("coverage_at_k", F.lit(coverage_at_k)))

    out_csv = os.path.join(args.out, "metrics_at_k.csv")
    (macro.write.mode("overwrite").option("header", True).csv(out_csv))

    # Qualitative examples (tiny)
    qual = (recs.join(
                test_full.select(F.col("id_base").alias("test_id"),
                                 "title","abstract","categories"), "test_id")
                 .orderBy("test_id","rank")
                 .limit(30))
    qpath = os.path.join(args.out, "qualitative_examples.md")
    rows = [(r["test_id"], r["rank"], r["neighbor_id"], r["score"], r["neighbor_categories"])
            for r in qual.select("test_id","rank","neighbor_id","score","neighbor_categories").collect()]
    with open(qpath, "w") as f:
        f.write("# Qualitative Examples (Top-K)\n\n")
        current = None
        for tid, rank, nid, score, ncats in rows:
            if current != tid:
                current = tid
                f.write(f"\n## Test: {tid}\n\n")
            cats_str = " ".join(ncats) if isinstance(ncats, list) else str(ncats)
            f.write(f"- k={int(rank)} → **{nid}** (cos={float(score):.3f}) cats={cats_str}\n")

def query_mode(spark, args):
    os.makedirs(args.out, exist_ok=True)
    model = PipelineModel.load(args.model_dir)
    train = spark.read.parquet(args.features_train)

    recs = query_topk(
        spark, model,
        train.select("id_base","categories","features"),
        args.query_title or "", args.query_abstract or "", k=args.k
    )

    # Attach original arXiv IDs for neighbors
    recs = _attach_neighbor_paper_id(recs, train)

    recs_to_save = (
        recs
        .withColumn("neighbor_categories",
                    F.array_join(F.col("neighbor_categories").cast("array<string>"), " "))
        .select("test_id", "rank", "neighbor_id", "neighbor_paper_id", "score", "neighbor_categories")
        .orderBy("rank")
    )

    out_csv = os.path.join(args.out, "query_topK.csv")
    (recs_to_save.write.mode("overwrite").option("header", True).csv(out_csv))

# ---------- main ----------
def main():
    args = parse_args()
    spark = make_spark()
    if args.mode == "eval":
        if not args.features:
            raise ValueError("--features is required for eval")
        eval_mode(spark, args)
    else:
        query_mode(spark, args)
    spark.stop()

if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/notebooks/complex_queries.py
================================================================================
# notebooks/complex_queries.py
"""
Advanced / Complex Spark SQL analyses for arXiv metadata.

Usage:
  python notebooks/complex_queries.py --parquet data/processed/arxiv_full
  # or for sample:
  python notebooks/complex_queries.py --parquet data/processed/arxiv_parquet

This script:
  - Loads the unified arXiv Parquet dataset used in Week 8
  - Registers a temp view `papers`
  - Runs a set of advanced Spark SQL analyses (10 total), each saved to reports/<run_type or --outdir>/
  - Performs simple validations after each query (schema/row-count/logic sanity checks)
  - Is robust to optional columns (e.g., submitted_date, categories_list); falls back when missing
  - NEW: Generates a PNG plot for every CSV output in the same folder.
"""

from __future__ import annotations
import argparse
from pathlib import Path

import pandas as pd
import numpy as np

# ---- plotting (headless-friendly) ----
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from pyspark.sql import SparkSession, functions as F


# ---------- helpers ----------
def pick_outdir(parquet_path: str, user_outdir: str | None) -> Path:
    if user_outdir:
        out = Path("reports") / user_outdir
    else:
        # default to a stable folder name you requested
        out = Path("reports") / "sample_complex_queries"
    out.mkdir(parents=True, exist_ok=True)
    return out

def save_df_as_csv(df_spark, path: Path):
    # All outputs are aggregated and small -> safe to collect to pandas
    pdf = df_spark.toPandas()
    pdf.to_csv(path, index=False)
    print(f"[saved] {path}")
    return pdf  # return for plotting

# ---- generic plotting utils ----
def _finalize(ax, title: str, xlabel: str = "", ylabel: str = "", tight: bool = True):
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if tight:
        plt.tight_layout()

def _sanitize_filename(name: str) -> str:
    return (
        name.replace(" ", "_")
            .replace("/", "_")
            .replace("\\", "_")
            .lower()
    )

def save_plot(fig: plt.Figure, out_png: Path):
    fig.savefig(out_png, dpi=160)
    plt.close(fig)
    print(f"[plot] {out_png}")

def barh_topn(pdf: pd.DataFrame, label_col: str, value_col: str, out_png: Path, title: str, topn: int = 30):
    if pdf.empty:
        return
    df = pdf[[label_col, value_col]].dropna()
    df = df.sort_values(value_col, ascending=False).head(topn)
    fig, ax = plt.subplots(figsize=(10, 0.35 * max(4, len(df))))
    ax.barh(df[label_col].astype(str), df[value_col])
    ax.invert_yaxis()
    _finalize(ax, title, xlabel=value_col, ylabel=label_col)
    save_plot(fig, out_png)

def line_xy(pdf: pd.DataFrame, x_col: str, y_col: str, out_png: Path, title: str, xlabel: str = "", ylabel: str = ""):
    if pdf.empty:
        return
    df = pdf[[x_col, y_col]].dropna().sort_values(x_col)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(df[x_col], df[y_col], marker="o")
    _finalize(ax, title, xlabel=xlabel or x_col, ylabel=ylabel or y_col)
    save_plot(fig, out_png)

def scatter_xy(pdf: pd.DataFrame, x_col: str, y_col: str, out_png: Path, title: str, xlabel: str = "", ylabel: str = ""):
    if pdf.empty:
        return
    df = pdf[[x_col, y_col]].dropna()
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.scatter(df[x_col], df[y_col], alpha=0.7)
    _finalize(ax, title, xlabel=xlabel or x_col, ylabel=ylabel or y_col)
    save_plot(fig, out_png)

def single_value_figure(value: float | int | str, out_png: Path, title: str, label: str = ""):
    fig, ax = plt.subplots(figsize=(6, 3))
    ax.axis("off")
    text = f"{label}: {value}" if label else f"{value}"
    ax.text(0.5, 0.5, text, ha="center", va="center", fontsize=16)
    ax.set_title(title)
    save_plot(fig, out_png)

def weekday_sort(df: pd.DataFrame, day_col: str, num_col: str | None = None) -> pd.DataFrame:
    # Map ISO 1..7 (Mon..Sun) if available; otherwise try common abbreviations
    if num_col and num_col in df.columns:
        df[num_col] = pd.to_numeric(df[num_col], errors="coerce")
        return df.sort_values(num_col)
    order = ["Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"]
    cat = pd.Categorical(df[day_col], categories=order, ordered=True)
    df = df.assign(_order=cat)
    return df.sort_values("_order").drop(columns=["_order"])

def validate_nonempty(df, required_cols: list[str]) -> None:
    cols_ok = all(c in df.columns for c in required_cols)
    if not cols_ok:
        missing = [c for c in required_cols if c not in df.columns]
        raise AssertionError(f"Missing columns in result: {missing}")
    assert df.count() >= 0  # allow 0 rows for some edge cases; most checks enforce >0 where relevant

def validate_positive_rows(df, msg="Expected non-empty result"):
    assert df.count() > 0, msg

def ensure_aux_columns(spark):
    """
    Prepare helper views/columns:
      - categories_list: array<string>
      - n_versions: int
      - doi_int: 0/1 from has_doi
    Produces view `papers_enriched`.
    """
    cols = [c.name for c in spark.table("papers").schema]

    # Build expressions to add (no leading commas here!)
    extra_exprs = []

    # categories_list
    if "categories_list" in cols:
        extra_exprs.append("categories_list")
    elif "categories" in cols:
        extra_exprs.append("SPLIT(categories, ' ') AS categories_list")
    else:
        extra_exprs.append("ARRAY(primary_category) AS categories_list")

    # n_versions
    if "versions" in cols:
        extra_exprs.append("SIZE(versions) AS n_versions")
    else:
        extra_exprs.append("CAST(1 AS INT) AS n_versions")

    # doi_int
    if "has_doi" in cols:
        extra_exprs.append("CASE WHEN has_doi THEN 1 ELSE 0 END AS doi_int")
    elif "doi" in cols:
        extra_exprs.append("CASE WHEN doi IS NOT NULL AND TRIM(doi) <> '' THEN 1 ELSE 0 END AS doi_int")
    else:
        extra_exprs.append("CAST(0 AS INT) AS doi_int")

    select_extra = ", ".join(extra_exprs)
    q = f"SELECT *, {select_extra} FROM papers"

    spark.sql("DROP VIEW IF EXISTS papers_enriched")
    spark.sql(f"CREATE OR REPLACE TEMP VIEW papers_enriched AS {q}")


# ---------- complex analyses (10) ----------
def complex_1_category_cooccurrence(spark, outdir: Path):
    """
    Interdisciplinary Category Co-occurrence (pair counts).
    Finds category pairs that frequently co-occur in the same paper.
    """
    print("\n[complex-1] Category co-occurrence pairs (interdisciplinarity)")
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW cat_pairs AS
        SELECT
            arxiv_id,
            c1 AS cat_a,
            c2 AS cat_b
        FROM (
          SELECT arxiv_id, categories_list
          FROM papers_enriched
        ) t
        LATERAL VIEW EXPLODE(categories_list) a AS c1
        LATERAL VIEW EXPLODE(categories_list) b AS c2
        WHERE c1 < c2   -- avoid duplicates (unordered pairs)
    """)
    df = spark.sql("""
        SELECT cat_a, cat_b, COUNT(*) AS pair_count
        FROM cat_pairs
        GROUP BY cat_a, cat_b
        HAVING pair_count > 1
        ORDER BY pair_count DESC, cat_a, cat_b
        LIMIT 200
    """)
    validate_nonempty(df, ["cat_a", "cat_b", "pair_count"])
    csv_path = outdir / "complex_category_cooccurrence.csv"
    pdf = save_df_as_csv(df, csv_path)
    # plot: top pairs by count
    pdf["pair"] = pdf["cat_a"].astype(str) + " × " + pdf["cat_b"].astype(str)
    barh_topn(pdf, "pair", "pair_count",
              outdir / "complex_category_cooccurrence_top.png",
              "Top category co-occurrences (by pair_count)", topn=30)

def complex_2_author_collab_over_time(spark, outdir: Path):
    """
    Author Collaboration Network Over Time (top author pairs by year).
    """
    print("\n[complex-2] Author collaboration pairs by year (top)")
    # Need authors_list and year
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "authors_list" not in cols or "year" not in cols:
        print("[skip] authors_list or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_pairs AS
        SELECT
          year,
          a1 AS author_a,
          a2 AS author_b
        FROM (
          SELECT year, authors_list FROM papers_enriched
        )
        LATERAL VIEW EXPLODE(authors_list) L1 AS a1
        LATERAL VIEW EXPLODE(authors_list) L2 AS a2
        WHERE a1 < a2
    """)
    df = spark.sql("""
        SELECT year, author_a, author_b, COUNT(*) AS n_coauthored
        FROM author_pairs
        GROUP BY year, author_a, author_b
        HAVING n_coauthored >= 2
        ORDER BY year, n_coauthored DESC
        LIMIT 500
    """)
    validate_nonempty(df, ["year", "author_a", "author_b", "n_coauthored"])
    csv_path = outdir / "complex_author_pairs_by_year.csv"
    pdf = save_df_as_csv(df, csv_path)

    # plot A: total coauthored pairs per year
    per_year = pdf.groupby("year", as_index=False)["n_coauthored"].sum()
    line_xy(per_year, "year", "n_coauthored",
            outdir / "complex_author_pairs_by_year_totals.png",
            "Total coauthored pairs with >=2 papers per year",
            xlabel="Year", ylabel="Pair count (sum n_coauthored)")

    # plot B: top 20 pairs overall (label= "A × B (year)")
    top_pairs = pdf.copy()
    top_pairs["pair"] = top_pairs["author_a"].astype(str) + " × " + top_pairs["author_b"].astype(str) + " (" + top_pairs["year"].astype(str) + ")"
    top_pairs = top_pairs.sort_values("n_coauthored", ascending=False).head(20)
    barh_topn(top_pairs, "pair", "n_coauthored",
              outdir / "complex_author_pairs_by_year_top20.png",
              "Top author pairs by year (n_coauthored ≥ 2)", topn=20)

def complex_3_rising_declining_topics(spark, outdir: Path):
    """
    Rising and Declining Topics:
      - Count by primary_category x year
      - Compute percent change from earliest year to latest year for each category
      - Rank top rising/shrinking
    """
    print("\n[complex-3] Rising and declining topics (primary_category x year growth)")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "primary_category" not in cols or "year" not in cols:
        print("[skip] primary_category or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW cat_year_counts AS
        SELECT primary_category, year, COUNT(*) AS c
        FROM papers_enriched
        GROUP BY primary_category, year
    """)
    # Get min/max year per category, then pick counts at those endpoints
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW cat_year_bounds AS
        SELECT
          primary_category,
          MIN(year) AS y_min,
          MAX(year) AS y_max
        FROM cat_year_counts
        GROUP BY primary_category
    """)
    df = spark.sql("""
        WITH endpoints AS (
          SELECT
            b.primary_category,
            b.y_min,
            b.y_max,
            MIN(CASE WHEN c.year = b.y_min THEN c.c END) AS c_start,
            MIN(CASE WHEN c.year = b.y_max THEN c.c END) AS c_end
          FROM cat_year_bounds b
          JOIN cat_year_counts c
            ON c.primary_category = b.primary_category
           AND c.year IN (b.y_min, b.y_max)
          GROUP BY b.primary_category, b.y_min, b.y_max
        )
        SELECT
          primary_category,
          y_min,
          y_max,
          c_start,
          c_end,
          CASE WHEN c_start > 0 THEN ROUND((c_end - c_start) * 100.0 / c_start, 2) ELSE NULL END AS pct_change
        FROM endpoints
        WHERE c_start IS NOT NULL AND c_end IS NOT NULL
        ORDER BY pct_change DESC NULLS LAST
    """)
    validate_nonempty(df, ["primary_category", "y_min", "y_max", "c_start", "c_end", "pct_change"])
    csv_full = outdir / "complex_rising_declining_topics_fullrank.csv"
    pdf_full = save_df_as_csv(df, csv_full)

    # Top 20 rising and top 20 declining
    rising = df.orderBy(F.col("pct_change").desc_nulls_last()).limit(20)
    declining = df.orderBy(F.col("pct_change").asc_nulls_last()).limit(20)
    pdf_rise = save_df_as_csv(rising, outdir / "complex_rising_topics_top20.csv")
    pdf_decl = save_df_as_csv(declining, outdir / "complex_declining_topics_top20.csv")

    # Plots
    barh_topn(pdf_rise, "primary_category", "pct_change",
              outdir / "complex_rising_topics_top20.png",
              "Top 20 rising topics by % change")
    # For declining, values are negative—sort makes ascending topn fine
    pdf_decl_plot = pdf_decl.sort_values("pct_change", ascending=True)
    barh_topn(pdf_decl_plot, "primary_category", "pct_change",
              outdir / "complex_declining_topics_top20.png",
              "Top 20 declining topics by % change")

def complex_4_readability_lexical_trends(spark, outdir: Path):
    """
    Abstract readability proxy via lexical richness over time.
    lexical_richness = distinct_token_count / token_count (using whitespace tokenization)
    """
    print("\n[complex-4] Readability / Lexical richness trends by year")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "abstract" not in cols or "year" not in cols:
        print("[skip] abstract or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW abstract_tokens AS
        SELECT
          year,
          SIZE(SPLIT(LOWER(abstract), '\\\\s+')) AS tok_count,
          SIZE(ARRAY_DISTINCT(SPLIT(LOWER(abstract), '\\\\s+'))) AS distinct_tok_count
        FROM papers_enriched
        WHERE abstract IS NOT NULL AND LENGTH(abstract) > 0
    """)
    df = spark.sql("""
        SELECT
          year,
          AVG(CAST(distinct_tok_count AS DOUBLE) / NULLIF(tok_count, 0)) AS avg_lexical_richness,
          AVG(tok_count) AS avg_token_count
        FROM abstract_tokens
        GROUP BY year
        ORDER BY year
    """)
    validate_nonempty(df, ["year", "avg_lexical_richness", "avg_token_count"])
    csv_path = outdir / "complex_lexical_richness_by_year.csv"
    pdf = save_df_as_csv(df, csv_path)

    # plots: two lines (two separate figures for simplicity)
    line_xy(pdf, "year", "avg_lexical_richness",
            outdir / "complex_lexical_richness_by_year.png",
            "Average lexical richness by year",
            xlabel="Year", ylabel="Avg lexical richness")
    line_xy(pdf, "year", "avg_token_count",
            outdir / "complex_avg_token_count_by_year.png",
            "Average abstract token count by year",
            xlabel="Year", ylabel="Avg tokens")

def complex_5_doi_versions_correlation(spark, outdir: Path):
    """
    DOI vs Version Correlation:
      - Average n_versions by DOI presence
      - Pearson corr between doi_int and n_versions
    """
    print("\n[complex-5] DOI vs Versions: group averages + correlation")
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW doi_versions AS
        SELECT doi_int, n_versions FROM papers_enriched
    """)
    group_df = spark.sql("""
        SELECT
          doi_int,
          AVG(n_versions) AS avg_versions
        FROM doi_versions
        GROUP BY doi_int
        ORDER BY doi_int DESC
    """)
    validate_nonempty(group_df, ["doi_int", "avg_versions"])
    csv_group = outdir / "complex_doi_vs_versions_group.csv"
    pdf_group = save_df_as_csv(group_df, csv_group)

    corr_df = spark.sql("""
        SELECT CORR(CAST(doi_int AS DOUBLE), CAST(n_versions AS DOUBLE)) AS corr_doi_versions
        FROM doi_versions
    """)
    validate_nonempty(corr_df, ["corr_doi_versions"])
    csv_corr = outdir / "complex_doi_versions_correlation.csv"
    pdf_corr = save_df_as_csv(corr_df, csv_corr)

    # plots
    # A: bar for avg_versions by doi_int
    if not pdf_group.empty:
        pdf_group["doi_label"] = np.where(pdf_group["doi_int"].astype(int) == 1, "DOI present", "No DOI")
        barh_topn(pdf_group, "doi_label", "avg_versions",
                  outdir / "complex_doi_vs_versions_group.png",
                  "Avg # versions by DOI presence", topn=2)
    # B: single-value figure for correlation
    if not pdf_corr.empty:
        val = float(pdf_corr["corr_doi_versions"].iloc[0])
        single_value_figure(round(val, 4), outdir / "complex_doi_versions_correlation.png",
                            "Pearson correlation", label="corr(doi_int, n_versions)")

def complex_6_author_productivity_lifecycle(spark, outdir: Path):
    """
    Author productivity lifespan and volume:
      - first_year, last_year, active_span_years, paper_count
    """
    print("\n[complex-6] Author productivity lifecycle (active span)")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "authors_list" not in cols or "year" not in cols:
        print("[skip] authors_list or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_years AS
        SELECT
          EXPLODE(authors_list) AS author,
          year
        FROM papers_enriched
        WHERE authors_list IS NOT NULL
    """)
    df = spark.sql("""
        SELECT
          author,
          MIN(year) AS first_year,
          MAX(year) AS last_year,
          (MAX(year) - MIN(year)) AS active_span_years,
          COUNT(*) AS paper_count
        FROM author_years
        GROUP BY author
        HAVING paper_count >= 2
        ORDER BY active_span_years DESC, paper_count DESC
        LIMIT 1000
    """)
    validate_nonempty(df, ["author", "first_year", "last_year", "active_span_years", "paper_count"])
    csv_path = outdir / "complex_author_lifecycle_top.csv"
    pdf = save_df_as_csv(df, csv_path)

    # plot: scatter paper_count vs active_span_years
    scatter_xy(pdf, "active_span_years", "paper_count",
               outdir / "complex_author_lifecycle_scatter.png",
               "Author lifecycle: span vs paper count",
               xlabel="Active span (years)", ylabel="# papers")

def complex_7_author_category_migration(spark, outdir: Path):
    """
    Author category migration:
      For each author, the dominant category in earliest year vs latest year; keep those who changed.
    """
    print("\n[complex-7] Author category migration (earliest vs latest dominant category)")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "authors_list" not in cols or "year" not in cols or "primary_category" not in cols:
        print("[skip] need authors_list, year, primary_category")
        return

    # 1) Explode authors, then aggregate by author/year/category
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_cat_year AS
        SELECT
          author,
          year,
          primary_category,
          COUNT(*) AS c
        FROM (
          SELECT
            year,
            primary_category,
            author
          FROM papers_enriched
          LATERAL VIEW OUTER EXPLODE(authors_list) a AS author
        ) t
        WHERE author IS NOT NULL AND TRIM(author) <> ''
        GROUP BY author, year, primary_category
    """)

    # 2) Earliest and latest year per author
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_bounds AS
        SELECT
          author,
          MIN(year) AS y_min,
          MAX(year) AS y_max
        FROM author_cat_year
        GROUP BY author
    """)

    # 3) Dominant category at earliest year
    spark.sql("DROP VIEW IF EXISTS author_earliest")
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_earliest AS
        SELECT author, cat_earliest FROM (
          SELECT
            acy.author,
            acy.primary_category AS cat_earliest,
            ROW_NUMBER() OVER (
              PARTITION BY acy.author
              ORDER BY acy.c DESC, acy.primary_category
            ) AS rn
          FROM author_cat_year acy
          JOIN author_bounds b
            ON b.author = acy.author AND acy.year = b.y_min
        ) t
        WHERE rn = 1
    """)

    # 4) Dominant category at latest year
    spark.sql("DROP VIEW IF EXISTS author_latest")
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_latest AS
        SELECT author, cat_latest FROM (
          SELECT
            acy.author,
            acy.primary_category AS cat_latest,
            ROW_NUMBER() OVER (
              PARTITION BY acy.author
              ORDER BY acy.c DESC, acy.primary_category
            ) AS rn
          FROM author_cat_year acy
          JOIN author_bounds b
            ON b.author = acy.author AND acy.year = b.y_max
        ) t
        WHERE rn = 1
    """)

    # 5) Keep authors who changed category
    df = spark.sql("""
        SELECT
          e.author,
          e.cat_earliest,
          l.cat_latest
        FROM author_earliest e
        JOIN author_latest l USING (author)
        WHERE e.cat_earliest <> l.cat_latest
        ORDER BY author
        LIMIT 5000
    """)

    validate_nonempty(df, ["author", "cat_earliest", "cat_latest"])
    csv_path = outdir / "complex_author_category_migration.csv"
    pdf = save_df_as_csv(df, csv_path)

    # plot: top 20 earliest->latest transitions by frequency
    if not pdf.empty:
        trans = pdf.groupby(["cat_earliest", "cat_latest"]).size().reset_index(name="n")
        trans["transition"] = trans["cat_earliest"].astype(str) + " → " + trans["cat_latest"].astype(str)
        trans = trans.sort_values("n", ascending=False).head(20)
        barh_topn(trans, "transition", "n",
                  outdir / "complex_author_category_migration_top20.png",
                  "Top 20 author category migrations", topn=20)

def complex_8_abstract_len_vs_popularity(spark, outdir: Path):
    """
    Abstract length vs popularity proxy (n_versions):
      - Correlation between abstract_len and n_versions
      - Bucketed analysis by abstract_len deciles
    """
    print("\n[complex-8] Abstract length vs popularity proxy (versions)")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "abstract_len" not in cols:
        print("[skip] abstract_len missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW abs_pop AS
        SELECT CAST(abstract_len AS DOUBLE) AS abstract_len, CAST(n_versions AS DOUBLE) AS n_versions
        FROM papers_enriched
        WHERE abstract_len IS NOT NULL
    """)
    corr_df = spark.sql("""
        SELECT CORR(abstract_len, n_versions) AS corr_abslen_versions FROM abs_pop
    """)
    validate_nonempty(corr_df, ["corr_abslen_versions"])
    csv_corr = outdir / "complex_abstractlen_versions_correlation.csv"
    pdf_corr = save_df_as_csv(corr_df, csv_corr)

    # Decile buckets via NTILE over abstract_len
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW abs_with_bucket AS
        SELECT abstract_len, n_versions,
               NTILE(10) OVER (ORDER BY abstract_len) AS abslen_decile
        FROM abs_pop
    """)
    bucket_df = spark.sql("""
        SELECT abslen_decile,
               AVG(abstract_len) AS avg_abslen,
               AVG(n_versions) AS avg_versions
        FROM abs_with_bucket
        GROUP BY abslen_decile
        ORDER BY abslen_decile
    """)
    validate_nonempty(bucket_df, ["abslen_decile", "avg_abslen", "avg_versions"])
    csv_bucket = outdir / "complex_abstractlen_versions_by_decile.csv"
    pdf_bucket = save_df_as_csv(bucket_df, csv_bucket)

    # plots
    if not pdf_corr.empty:
        single_value_figure(round(float(pdf_corr["corr_abslen_versions"].iloc[0]), 4),
                            outdir / "complex_abstractlen_versions_correlation.png",
                            "Pearson correlation", label="corr(abstract_len, n_versions)")
    line_xy(pdf_bucket, "abslen_decile", "avg_versions",
            outdir / "complex_abstractlen_versions_by_decile.png",
            "Avg # versions by abstract length decile",
            xlabel="Abstract length decile (1=shorter)", ylabel="Avg # versions")

def complex_9_weekday_submission_patterns(spark, outdir: Path):
    """
    Weekday vs Weekend submission patterns (requires submitted_date timestamp).
    """
    print("\n[complex-9] Weekday submission patterns")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "submitted_date" not in cols:
        print("[skip] submitted_date missing")
        return

    df = spark.sql("""
        SELECT
          DATE_FORMAT(submitted_date, 'E') AS weekday_short,
          DATE_FORMAT(submitted_date, 'u') AS weekday_num,   -- 1..7 (Mon..Sun)
          COUNT(*) AS submissions
        FROM papers_enriched
        GROUP BY DATE_FORMAT(submitted_date, 'E'), DATE_FORMAT(submitted_date, 'u')
        ORDER BY CAST(weekday_num AS INT)
    """)
    validate_nonempty(df, ["weekday_short", "weekday_num", "submissions"])
    csv_path = outdir / "complex_weekday_submissions.csv"
    pdf = save_df_as_csv(df, csv_path)

    # plot: line over Mon..Sun
    pdf_plot = weekday_sort(pdf.copy(), "weekday_short", "weekday_num")
    line_xy(pdf_plot, "weekday_short", "submissions",
            outdir / "complex_weekday_submissions.png",
            "Submissions by weekday", xlabel="Weekday", ylabel="Submissions")

def complex_10_category_stability_versions(spark, outdir: Path):
    """
    Category stability via versions:
      - Average n_versions per primary_category
      - Distribution ranks
    """
    print("\n[complex-10] Category stability via versions (avg versions per primary_category)")
    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "primary_category" not in cols:
        print("[skip] primary_category missing")
        return

    df = spark.sql("""
        SELECT
          primary_category,
          AVG(n_versions) AS avg_versions,
          COUNT(*) AS n_papers
        FROM papers_enriched
        GROUP BY primary_category
        HAVING n_papers >= 20   -- stability filter
        ORDER BY avg_versions DESC, n_papers DESC
    """)
    validate_nonempty(df, ["primary_category", "avg_versions", "n_papers"])
    csv_path = outdir / "complex_category_versions_avg.csv"
    pdf = save_df_as_csv(df, csv_path)

    # plot: show top 30 categories by avg_versions
    barh_topn(pdf, "primary_category", "avg_versions",
              outdir / "complex_category_versions_avg_top30.png",
              "Avg # versions by primary category (n_papers ≥ 20)", topn=30)


# ---------- main ----------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help="Path to Parquet output from src/ingestion.py")
    ap.add_argument("--outdir", default=None, help="Optional subfolder name under reports/. If not set, uses 'full' or 'sample'.")
    args = ap.parse_args()

    outdir = pick_outdir(args.parquet, args.outdir)

    spark = (
        SparkSession.builder
        .appName("arxiv_week9_complex_queries")
        # .config("spark.sql.shuffle.partitions", "200")  # tweak if needed
        .getOrCreate()
    )
    df = spark.read.parquet(args.parquet)

    # Register base view
    df.createOrReplaceTempView("papers")

    # Build enriched view with helper columns
    ensure_aux_columns(spark)

    # Run all 10 complex analyses
    complex_1_category_cooccurrence(spark, outdir)
    complex_2_author_collab_over_time(spark, outdir)
    complex_3_rising_declining_topics(spark, outdir)
    complex_4_readability_lexical_trends(spark, outdir)
    complex_5_doi_versions_correlation(spark, outdir)
    complex_6_author_productivity_lifecycle(spark, outdir)
    complex_7_author_category_migration(spark, outdir)
    complex_8_abstract_len_vs_popularity(spark, outdir)
    complex_9_weekday_submission_patterns(spark, outdir)
    complex_10_category_stability_versions(spark, outdir)

    print(f"\n[done] Complex analytics written to {outdir}/ (CSV + PNG plots)")
    spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/scripts/train_ml.py
================================================================================
#!/usr/bin/env python3
import argparse
import json
import os
import time
from pyspark.sql import SparkSession, functions as F
from src.featurization import build_text_pipeline, compute_extra_stopwords


def make_spark():
    try:
        from src.utils import get_spark  # type: ignore
        return get_spark(app_name="train_full")
    except Exception:
        return (
            SparkSession.builder
            .appName("train_full")
            .config("spark.sql.session.timeZone", "UTC")
            .config("spark.sql.shuffle.partitions", "1024")
            .config("spark.sql.files.maxPartitionBytes", 256 << 20)
            .getOrCreate()
        )


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--split-parquet", required=True)
    ap.add_argument("--model-dir", required=True)
    ap.add_argument("--features-out", required=True)
    ap.add_argument("--vocab-size", type=int, default=250000)
    ap.add_argument("--min-df", type=int, default=5)
    ap.add_argument("--use-bigrams", type=lambda x: str(x).lower() == "true", default=False)
    ap.add_argument("--extra-stopwords-topdf", type=int, default=500)
    ap.add_argument("--seed", type=int, default=42)
    return ap.parse_args()


def ensure_id_base(df):
    if "id_base" in df.columns:
        return df
    if "id" in df.columns:
        return df.withColumn("id_base", F.regexp_replace(F.col("id").cast("string"), r"v\\d+$", ""))
    if "arxiv_id" in df.columns:
        return df.withColumn("id_base", F.regexp_replace(F.col("arxiv_id").cast("string"), r"v\\d+$", ""))
    raise ValueError("Expected 'id_base' or 'id' or 'arxiv_id' in dataset.")


def normalize_schema_for_text(df):
    dtypes = dict(df.dtypes)

    if "categories" in dtypes and dtypes["categories"] == "string":
        df = df.withColumn("categories", F.split(F.col("categories"), r"\\s+"))
    elif "categories" not in dtypes:
        df = df.withColumn("categories", F.array().cast("array<string>"))

    df = (
        df.withColumn("title", F.coalesce(F.col("title").cast("string"), F.lit("")))
          .withColumn("abstract", F.coalesce(F.col("abstract").cast("string"), F.lit("")))
    )

    if "arxiv_id" in df.columns:
        df = df.withColumn("paper_id", F.col("arxiv_id").cast("string"))
    elif "id" in df.columns:
        df = df.withColumn("paper_id", F.col("id").cast("string"))
    else:
        df = df.withColumn("paper_id", F.col("id_base"))

    df = df.withColumn("text", F.concat_ws(" ", F.lower(F.col("title")), F.lower(F.col("abstract"))))
    return df


def main():
    args = parse_args()
    spark = make_spark()

    df = spark.read.parquet(args.split_parquet)
    df = ensure_id_base(df)
    df = normalize_schema_for_text(df)

    if "split" not in df.columns:
        raise ValueError("Expected a 'split' column with values 'train'/'test' in --split-parquet")

    train = df.filter(F.col("split") == "train")
    test = df.filter(F.col("split") == "test")

    extra_sw = compute_extra_stopwords(
        spark, train.select("id_base", "abstract"), top_df=args.extra_stopwords_topdf, seed=args.seed
    )

    pipeline = build_text_pipeline(
        vocab_size=args.vocab_size,
        min_df=args.min_df,
        use_bigrams=args.use_bigrams,
        extra_stopwords=extra_sw,
    )
    model = pipeline.fit(train)

    model.write().overwrite().save(args.model_dir)
    os.makedirs(args.model_dir, exist_ok=True)
    meta = {
        "created_at": int(time.time()),
        "vocab_size": args.vocab_size,
        "min_df": args.min_df,
        "use_bigrams": args.use_bigrams,
        "extra_stopwords_topdf": args.extra_stopwords_topdf,
        "seed": args.seed,
        "spark_version": spark.version,
    }
    with open(os.path.join(args.model_dir, "model.json"), "w") as f:
        json.dump(meta, f, indent=2)

    feats_train = (
        model.transform(train)
        .select(
            "id_base",
            "paper_id",
            "title",
            "abstract",
            "categories",
            "year",
            F.col("features_norm").alias("features"),
        )
    )

    feats_test = (
        model.transform(test)
        .select(
            "id_base",
            "paper_id",
            "title",
            "abstract",
            "categories",
            "year",
            F.col("features_norm").alias("features"),
        )
    )

    (
        feats_train.repartition(256)
        .write.mode("overwrite")
        .parquet(os.path.join(args.features_out, "split=train"))
    )
    (
        feats_test.repartition(128)
        .write.mode("overwrite")
        .parquet(os.path.join(args.features_out, "split=test"))
    )

    print(f"[train] model saved to: {args.model_dir}")
    print(f"[train] features saved to: {args.features_out}/split={'{train|test}'}")
    spark.stop()


if __name__ == "__main__":
    main()

/home/data/akhalegh/utils/ccda-course-project/scripts/prepare_sample_stream_batches.py
================================================================================
#!/usr/bin/env python3
"""
Prepare weekly 'streaming' drops for the sample pipeline, emitted every N seconds.

- Reads:  data/sample/arxiv-sample.jsonl  (≈50k lines)
- Writes: data/stream/incoming_sample/arxiv-sample-YYYYMMDD.json
          5 files total; sizes [10k, 20k, 30k, 40k, 50k] (prefix/superset)
- File *names* advance one week at a time (YYYYMMDD), but files are
  *emitted* every --interval-seconds (default: 60s) to simulate streaming.

Run:
  python scripts/prepare_sample_stream_batches.py
Examples:
  # start at 2025-10-24 and drop a new weekly-dated file every minute
  python scripts/prepare_sample_stream_batches.py --start-date 2025-10-24 --interval-seconds 60

Options:
  --start-date YYYY-MM-DD   First weekly date in filenames (default: today, local)
  --interval-seconds N      Delay between drops in seconds (default: 60)
  --no-sleep                Emit all files immediately (no delay)
  --overwrite               Overwrite if a target already exists
"""

from __future__ import annotations
import argparse
import os
import time
from datetime import datetime, timedelta
from pathlib import Path

SOURCE = Path("data/sample/arxiv-sample.jsonl")
OUTDIR = Path("data/stream/incoming_sample")
SIZES = [10_000, 20_000, 30_000, 40_000, 50_000]

def parse_date(s: str) -> datetime:
    return datetime.strptime(s, "%Y-%m-%d")

def write_prefix_atomic(src: Path, dst: Path, n_lines: int, overwrite: bool = False) -> int:
    """
    Write first n_lines from src (JSONL) to dst (.json) via a temp file,
    then atomically rename so Spark treats the arrival correctly.
    """
    if dst.exists() and not overwrite:
        print(f"[skip] exists: {dst}")
        return -1

    dst.parent.mkdir(parents=True, exist_ok=True)
    tmp = dst.with_suffix(dst.suffix + ".tmp")

    count = 0
    with src.open("r", encoding="utf-8") as fin, tmp.open("w", encoding="utf-8") as fout:
        for line in fin:
            if not line.strip():
                continue
            fout.write(line)
            count += 1
            if count >= n_lines:
                break

    os.replace(tmp, dst)  # atomic move
    print(f"[write] {dst}  lines={count}")
    return count

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--start-date", type=str, default=datetime.now().strftime("%Y-%m-%d"),
                    help='First weekly date (YYYY-MM-DD) used in filenames (default: today)')
    ap.add_argument("--interval-seconds", type=int, default=60,
                    help="Seconds to wait between drops (default: 60)")
    ap.add_argument("--no-sleep", action="store_true",
                    help="Emit all files immediately without waiting between drops")
    ap.add_argument("--overwrite", action="store_true",
                    help="Overwrite target files if they already exist")
    args = ap.parse_args()

    if not SOURCE.exists():
        raise FileNotFoundError(f"Missing {SOURCE}. Generate it with your dataset download script first.")

    OUTDIR.mkdir(parents=True, exist_ok=True)
    start_dt = parse_date(args.start_date)

    for i, size in enumerate(SIZES):
        drop_dt = start_dt + timedelta(weeks=i)
        stamp = drop_dt.strftime("%Y%m%d")  # e.g., 20251024
        target = OUTDIR / f"arxiv-sample-{stamp}.jsonl"  # per your requested naming

        _ = write_prefix_atomic(SOURCE, target, size, overwrite=args.overwrite)

        if i < len(SIZES) - 1 and not args.no_sleep:
            sleep_s = max(args.interval_seconds, 1)
            print(f"[wait] sleeping {sleep_s}s before next drop...")
            time.sleep(sleep_s)

    print(f"[done] Created {len(SIZES)} weekly-dated drops under {OUTDIR}/")

if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/scripts/train_ml_sample.py
================================================================================
#!/usr/bin/env python3
import argparse
import json
import os
import time
from pyspark.sql import SparkSession, functions as F

# We rely on your Week-12 featurization helpers
from src.featurization import build_text_pipeline, compute_extra_stopwords


def make_spark():
    """
    Prefer your project utils.get_spark() if available (with memory/conf),
    otherwise build a reasonable default session.
    """
    try:
        from src.utils import get_spark  # type: ignore
        return get_spark(app_name="train_ml_sample")
    except Exception:
        return (
            SparkSession.builder
            .appName("train_ml_sample")
            .config("spark.sql.session.timeZone", "UTC")
            .config("spark.sql.shuffle.partitions", "64")
            .getOrCreate()
        )


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--split-parquet", required=True,
                    help="Parquet with partitions split=train|test (from scripts/split_sample.py)")
    ap.add_argument("--model-dir", required=True,
                    help="Where to save the fitted PipelineModel (e.g., data/models/tfidf_sample)")
    ap.add_argument("--features-out", required=True,
                    help="Where to save transformed features (split=train/test subdirs)")
    ap.add_argument("--vocab-size", type=int, default=80000)
    ap.add_argument("--min-df", type=int, default=3)
    ap.add_argument("--use-bigrams", type=lambda x: str(x).lower() == "true", default=False)
    ap.add_argument("--extra-stopwords-topdf", type=int, default=200,
                    help="How many top-DF tokens to add to stopwords from TRAIN")
    ap.add_argument("--seed", type=int, default=42)
    return ap.parse_args()


def ensure_id_base(df):
    """
    Ensure we have id_base:
    - If id_base exists, keep it.
    - Else if id or arxiv_id exists, strip trailing version (v\\d+) and use as id_base.
    """
    if "id_base" in df.columns:
        return df

    if "id" in df.columns:
        return df.withColumn("id_base", F.regexp_replace(F.col("id").cast("string"), r"v\\d+$", ""))

    if "arxiv_id" in df.columns:
        return df.withColumn("id_base", F.regexp_replace(F.col("arxiv_id").cast("string"), r"v\\d+$", ""))

    raise ValueError("Expected 'id_base' or 'id' or 'arxiv_id' in dataset.")


def normalize_schema_for_text(df):
    """
    Prepare columns for featurization:
    - categories => array<string>
    - title/abstract => non-null strings
    - text = lower(title) + ' ' + lower(abstract)
    - paper_id = prefer arxiv_id, else id, else id_base (string)
    """
    dtypes = dict(df.dtypes)

    # categories to array<string>
    if "categories" in dtypes and dtypes["categories"] == "string":
        df = df.withColumn("categories", F.split(F.col("categories"), r"\\s+"))
    elif "categories" not in dtypes:
        df = df.withColumn("categories", F.array().cast("array<string>"))

    # title/abstract safe strings
    df = (
        df.withColumn("title", F.coalesce(F.col("title").cast("string"), F.lit("")))
          .withColumn("abstract", F.coalesce(F.col("abstract").cast("string"), F.lit("")))
    )

    # Prefer canonical arXiv id when available
    if "arxiv_id" in df.columns:
        df = df.withColumn("paper_id", F.col("arxiv_id").cast("string"))
    elif "id" in df.columns:
        df = df.withColumn("paper_id", F.col("id").cast("string"))
    else:
        df = df.withColumn("paper_id", F.col("id_base"))

    # text used by the PipelineModel
    df = df.withColumn(
        "text",
        F.concat_ws(" ", F.lower(F.col("title")), F.lower(F.col("abstract")))
    )
    return df


def main():
    args = parse_args()
    spark = make_spark()

    # Load split parquet and make sure required columns exist
    df = spark.read.parquet(args.split_parquet)
    df = ensure_id_base(df)
    df = normalize_schema_for_text(df)

    if "split" not in df.columns:
        raise ValueError("Expected a 'split' column with values 'train'/'test' in --split-parquet")

    train = df.filter(F.col("split") == "train")
    test = df.filter(F.col("split") == "test")

    # Compute top-DF extra stopwords on TRAIN deterministically (by id_base)
    extra_sw = compute_extra_stopwords(
        spark, train.select("id_base", "abstract"), top_df=args.extra_stopwords_topdf, seed=args.seed
    )

    # Build + fit the featurization pipeline
    pipeline = build_text_pipeline(
        vocab_size=args.vocab_size,
        min_df=args.min_df,
        use_bigrams=args.use_bigrams,
        extra_stopwords=extra_sw
    )
    model = pipeline.fit(train)  # requires 'text'

    # Save model + metadata
    model.write().overwrite().save(args.model_dir)
    os.makedirs(args.model_dir, exist_ok=True)
    meta = {
        "created_at": int(time.time()),
        "vocab_size": args.vocab_size,
        "min_df": args.min_df,
        "use_bigrams": args.use_bigrams,
        "extra_stopwords_topdf": args.extra_stopwords_topdf,
        "seed": args.seed,
        "spark_version": spark.version,
    }
    with open(os.path.join(args.model_dir, "model.json"), "w") as f:
        json.dump(meta, f, indent=2)

    # Transform to features (L2-normalized vector is 'features_norm' from the pipeline)
    feats_train = (
        model.transform(train)
        .select(
            "id_base",
            "paper_id",            # <= original arXiv id if present
            "title",
            "abstract",
            "categories",
            "year",
            F.col("features_norm").alias("features"),
        )
    )

    feats_test = (
        model.transform(test)
        .select(
            "id_base",
            "paper_id",
            "title",
            "abstract",
            "categories",
            "year",
            F.col("features_norm").alias("features"),
        )
    )

    # Write features
    (feats_train.repartition(1)
        .write.mode("overwrite")
        .parquet(os.path.join(args.features_out, "split=train")))
    (feats_test.repartition(1)
        .write.mode("overwrite")
        .parquet(os.path.join(args.features_out, "split=test")))

    print(f"[train] model saved to: {args.model_dir}")
    print(f"[train] features saved to: {args.features_out}/split={{train|test}}")

    spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/scripts/download_arxiv.py
================================================================================
# scripts/download_arxiv.py
"""
Download the Kaggle Cornell arXiv metadata and (optionally) write a tiny JSONL sample
for Week-8 PRs / quick EDA in Codespaces.

Examples:
  python scripts/download_arxiv.py
  python scripts/download_arxiv.py --sample 50000
"""
from __future__ import annotations
import argparse
import os
import shutil
from pathlib import Path
import kagglehub

def is_likely_jsonl(path: Path, probe_lines: int = 5) -> bool:
    """
    Heuristic: JSONL typically has a complete JSON object per line.
    We'll read a few lines and check that each starts with '{' (after stripping).
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            for i in range(probe_lines):
                line = f.readline()
                if not line:
                    break
                s = line.strip()
                if not s:
                    continue
                if not s.startswith("{"):
                    return False
        return True
    except Exception:
        return False

def write_head_jsonl(src: Path, dst: Path, n: int) -> int:
    dst.parent.mkdir(parents=True, exist_ok=True)
    count = 0
    with open(src, "r", encoding="utf-8") as fin, open(dst, "w", encoding="utf-8") as fout:
        for line in fin:
            if not line.strip():
                continue
            fout.write(line)
            count += 1
            if count >= n:
                break
    return count

def main():
    ap = argparse.ArgumentParser(description="Download Kaggle arXiv dataset and optionally create a JSONL sample.")
    ap.add_argument("--sample", type=int, default=30000, help="Number of lines to write into data/sample/arxiv-sample.jsonl (0 to skip).")
    args = ap.parse_args()

    print("[KaggleHub] Downloading Cornell-University/arxiv ...")
    path = kagglehub.dataset_download("Cornell-University/arxiv")
    print("Path to dataset files:", path)

    # Locate the metadata file in the downloaded folder
    base = Path(path)
    candidates = list(base.rglob("arxiv-metadata-oai-snapshot.json")) + \
                 list(base.rglob("arxiv-metadata-oai-snapshot.jsonl"))
    if not candidates:
        raise FileNotFoundError("Could not find arxiv-metadata-oai-snapshot.json(.jsonl) in Kaggle download.")

    src = candidates[0]
    print(f"[Found] {src}")

    # Copy raw file into project (but keep it out of Git; .gitignore has data/raw/)
    proj_raw = Path("data/raw")
    proj_raw.mkdir(parents=True, exist_ok=True)
    target = proj_raw / src.name
    if target.resolve() != src.resolve():
        print(f"[Copy] -> {target}")
        shutil.copyfile(src, target)

    # Create a small JSONL sample for Week-8 PRs
    if args.sample and args.sample > 0:
        sample_dir = Path("data/sample")
        sample_dir.mkdir(parents=True, exist_ok=True)
        sample_path = sample_dir / "arxiv-sample.jsonl"

        if is_likely_jsonl(target):
            n = write_head_jsonl(target, sample_path, args.sample)
            print(f"[Sample] Wrote first {n} JSONL lines to {sample_path}")
        else:
            # If the downloaded file is pretty-printed JSON (rare), we’ll advise running with --multiline later.
            # Still write a small pseudo-sample by grabbing lines (not strict JSON); mainly for dev/testing of IO.
            n = write_head_jsonl(target, sample_path, args.sample)
            print(f"[Sample] (non-JSONL heuristic) Wrote {n} lines to {sample_path}")
            print("NOTE: If this isn't valid JSONL, use --multiline when running ingestion on the full file.")

    print("[Done] Ready for Week-8 ingestion.")

if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/src/similarity.py
================================================================================
from typing import List
from pyspark.sql import DataFrame, functions as F, Window
from pyspark.ml.linalg import VectorUDT, SparseVector

# Cosine on L2-normalized vectors == dot product.
def _dot(a: SparseVector, b: SparseVector) -> float:
    if a is None or b is None:
        return 0.0
    ai = dict(zip(a.indices, a.values))
    s = 0.0
    for j, v in zip(b.indices, b.values):
        if j in ai:
            s += ai[j] * v
    return float(s)

_dot_udf = F.udf(_dot)

def topk_exact(
    test_df: DataFrame,
    train_df: DataFrame,
    k: int = 3,
    exclude_self: bool = True,
):
    """
    Brute-force exact cosine@K (safe for sample sizes). Assumes 'features' are L2-normalized.
    Expects columns:
      test_df : id_base, categories, features
      train_df: id_base, categories, features
    Returns: test_id, rank, neighbor_id, score, neighbor_categories
    """
    # Hint broadcast if train is small (sample)
    joined = (test_df.alias("q")
              .crossJoin(F.broadcast(train_df.alias("c")))
              .withColumn("score", _dot_udf(F.col("q.features"), F.col("c.features"))))

    if exclude_self:
        joined = joined.where(F.col("q.id_base") != F.col("c.id_base"))

    w = Window.partitionBy("q.id_base").orderBy(F.desc("score"), F.col("c.id_base"))
    top = (joined
           .withColumn("rank", F.row_number().over(w))
           .where(F.col("rank") <= k)
           .select(
               F.col("q.id_base").alias("test_id"),
               "rank",
               F.col("c.id_base").alias("neighbor_id"),
               "score",
               F.col("c.categories").alias("neighbor_categories")
           ))
    return top

def pairwise_cosine_mean(df: DataFrame) -> DataFrame:
    """
    Compute mean pairwise cosine within each (test_id) recommended list for intra-list diversity (ILD).
    Lower mean cosine => more diverse list.
    Input: df with columns (test_id, rank, neighbor_id, score, features?) — features optional.
    If 'features' not present, returns mean(score) over pairs formed by ranks (proxy).
    """
    if "features" not in df.columns:
        # Use provided 'score' between query and each item; for ILD we need item-item similarity.
        # Fallback proxy: average of (score) won't reflect ILD correctly; we instead return null.
        return df.groupBy("test_id").agg(F.lit(None).alias("ild_mean"))
    # To compute pairwise item-item cosine we need features for neighbors; caller should join them.
    # This function just expects a prepared table with pairs and 'pair_cosine'.
    # Left here as placeholder; real computation is done in the notebook where we have both sides.
    return df.groupBy("test_id").agg(F.avg("pair_cosine").alias("ild_mean"))


/home/data/akhalegh/utils/ccda-course-project/src/__init__.py
================================================================================


/home/data/akhalegh/utils/ccda-course-project/src/transformations.py
================================================================================
# src/transformations.py
from pyspark.sql import DataFrame, functions as F
from src.utils import clean_text, parse_year_from_datestr, extract_primary_category, split_categories, normalize_authors, lower

def select_and_standardize(df: DataFrame) -> DataFrame:
    cols = df.columns
    base = df.select(
        F.col("id").alias("arxiv_id"),
        F.col("title"),
        F.col("abstract"),
        F.col("categories"),
        F.col("doi"),
        F.col("journal-ref").alias("journal_ref"),
        F.col("comments"),
        F.col("submitter"),
        F.col("update_date"),
        F.col("versions"),
        *([F.col("authors")] if "authors" in cols else []),
        *([F.col("authors_parsed")] if "authors_parsed" in cols else []),
    )
    base = base.withColumn("title_clean", clean_text(F.col("title")))
    base = base.withColumn("abstract_clean", clean_text(F.col("abstract")))
    base = base.withColumn("title_lower", lower(F.col("title")))
    base = base.withColumn("abstract_lower", lower(F.col("abstract")))
    base = base.withColumn("primary_category", extract_primary_category(F.col("categories")))
    base = base.withColumn("category_list", split_categories(F.col("categories")))
    base = base.withColumn("year", parse_year_from_datestr(F.col("update_date")))

    if "authors_parsed" in cols:
        base = base.withColumn(
            "authors_list",
            F.expr("transform(authors_parsed, x -> array_join(reverse(x), ' '))")
        )
    elif "authors" in cols:
        base = base.withColumn("authors_list", normalize_authors(F.col("authors")))
    else:
        base = base.withColumn("authors_list", F.array())

    base = base.withColumn("abstract_len", F.length("abstract_clean"))
    base = base.withColumn("title_len", F.length("title_clean"))
    base = base.withColumn("n_authors", F.size("authors_list"))
    base = base.withColumn("n_categories", F.size("category_list"))
    return base

def filter_for_quality(df: DataFrame, min_abstract_len: int = 40) -> DataFrame:
    return (
        df
        .where(F.col("arxiv_id").isNotNull() & (F.col("arxiv_id") != ""))
        .where(F.col("title_clean").isNotNull() & (F.length("title_clean") > 0))
        .where(F.col("abstract_clean").isNotNull() & (F.col("abstract_len") >= min_abstract_len))
    )

def add_eda_helpers(df: DataFrame) -> DataFrame:
    return df.withColumn("has_doi", (F.col("doi").isNotNull()) & (F.col("doi") != ""))

def transform_all(df: DataFrame, min_abstract_len: int = 40) -> DataFrame:
    return add_eda_helpers(filter_for_quality(select_and_standardize(df), min_abstract_len))


/home/data/akhalegh/utils/ccda-course-project/src/query.py
================================================================================
from typing import Iterable
from pyspark.sql import SparkSession, functions as F
from pyspark.ml import PipelineModel
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, NGram, CountVectorizerModel, IDFModel, Normalizer
from src.similarity import topk_exact

def vectorize_query(spark: SparkSession, model: PipelineModel, title: str, abstract: str):
    text = (title or "") + " " + (abstract or "")
    df = spark.createDataFrame([(text,)], ["text"])
    # The saved PipelineModel expects a 'text' column and emits 'features_norm'
    out = model.transform(df).select(F.col("features_norm").alias("features"))
    return out.first()["features"]

def query_topk(
    spark: SparkSession,
    model: PipelineModel,
    features_train_df,   # DataFrame with (id_base, categories, features)
    query_title: str,
    query_abstract: str,
    k: int = 5
):
    from pyspark.ml.linalg import SparseVector
    qvec = vectorize_query(spark, model, query_title, query_abstract)
    qdf = spark.createDataFrame([("Q", qvec)], ["id_base", "features"])
    qdf = qdf.withColumn("categories", F.array().cast("array<string>"))
    recs = topk_exact(qdf, features_train_df, k=k, exclude_self=False)
    return recs


/home/data/akhalegh/utils/ccda-course-project/src/utils.py
================================================================================
# src/utils.py
from pathlib import Path
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.column import Column


def get_spark(app_name: str = "arxiv_week8") -> SparkSession:
    """
    Create (or get) a SparkSession tuned for low-memory environments.

    Key choices to avoid OOM:
      - Stable local temp dir inside the repo (not /tmp)
      - Many small tasks (high shuffle partitions)
      - AQE with small advisory partition sizes
      - Small input split size per task
      - Skew handling to split oversized partitions
    """
    local_tmp = Path("data/tmp/spark-local")
    local_tmp.mkdir(parents=True, exist_ok=True)

    spark = (
        SparkSession.builder.appName(app_name)
        .config("spark.sql.session.timeZone", "UTC")

        # ---- Memory / resources (tune if needed) ----
        # If your machine is tighter on RAM, drop these to 4g.
        .config("spark.driver.memory", "6g")
        .config("spark.executor.memory", "6g")
        .config("spark.driver.maxResultSize", "2g")

        # ---- Lots of small tasks; AQE keeps them efficient ----
        .config("spark.sql.shuffle.partitions", "512")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "8m")  # small target post-shuffle
        .config("spark.sql.files.maxPartitionBytes", "8m")                # small input per task

        # ---- Skew handling: split very large partitions ----
        .config("spark.sql.adaptive.skewJoin.enabled", "true")
        .config("spark.sql.adaptive.skewedPartitionThresholdInBytes", "64m")
        .config("spark.sql.adaptive.skewedPartitionMaxSplitBytes", "16m")

        # ---- Temp / stability ----
        .config("spark.local.dir", str(local_tmp))
        .config("spark.shuffle.checksum.enabled", "false")  # avoid fragile checksum files

        # ---- Parquet default compression (can be overridden per-writer) ----
        .config("spark.sql.parquet.compression.codec", "zstd")

        # ---- Misc ----
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")
        .getOrCreate()
    )
    return spark


# ---------- helper column transforms ----------

def clean_text(col: Column) -> Column:
    """Trim and collapse whitespace."""
    return F.regexp_replace(F.trim(col), r"\s+", " ")


def parse_year_from_datestr(col: Column) -> Column:
    """Extract the first 4-digit year as int."""
    return F.regexp_extract(col.cast("string"), r"(\d{4})", 1).cast("int")


def extract_primary_category(categories_col: Column) -> Column:
    """First whitespace-delimited token is the primary category."""
    return F.split(F.coalesce(categories_col, F.lit("")), r"\s+")[0]


def split_categories(categories_col: Column) -> Column:
    """Split on whitespace into array, dropping empties."""
    arr = F.split(F.coalesce(categories_col, F.lit("")), r"\s+")
    return F.filter(arr, lambda x: x != "")


def normalize_authors(authors_col: Column) -> Column:
    """
    Normalize authors string into an array:
      - Replace ' and ' with commas
      - Normalize comma/whitespace
      - Trim and drop empties
    """
    replaced = F.regexp_replace(F.coalesce(authors_col, F.lit("")), r"\s+and\s+", ",")
    replaced = F.regexp_replace(replaced, r"\s*,\s*", ",")
    arr = F.split(replaced, ",")
    arr = F.transform(arr, lambda x: F.trim(x))
    return F.filter(arr, lambda x: x != "")


def lower(col: Column) -> Column:
    """Lowercase after cleanup."""
    return F.lower(clean_text(col))


/home/data/akhalegh/utils/ccda-course-project/src/featurization.py
================================================================================
from typing import List, Iterable
from pyspark.sql import DataFrame, SparkSession, functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import (RegexTokenizer, StopWordsRemover,
                                CountVectorizer, IDF, NGram)
from pyspark.ml.feature import Normalizer

DEFAULT_DOMAIN_STOPS = [
    "arxiv", "paper", "result", "results", "method", "methods",
    "propose", "proposed", "show", "shows", "using", "based"
]

def _lower_and_strip(df: DataFrame) -> DataFrame:
    return (df
            .withColumn("title_clean", F.lower(F.col("title")))
            .withColumn("abstract_clean", F.lower(F.col("abstract")))
            .withColumn("text", F.concat_ws(" ", "title_clean", "abstract_clean")))

def compute_extra_stopwords(spark: SparkSession, train_df: DataFrame, top_df: int = 200, seed: int = 42) -> List[str]:
    """
    Return the top 'top_df' tokens by document frequency on TRAIN.
    Deterministic: counts distinct id_base per token (no monotonic ID).
    """
    # Ensure id_base exists (split step creates it)
    if "id_base" not in train_df.columns:
        # fall back to 'id' if present
        if "id" in train_df.columns:
            train_df = train_df.withColumn("id_base", F.regexp_replace(F.col("id"), r"v\d+$", ""))
        else:
            raise ValueError("compute_extra_stopwords requires 'id_base' or 'id' in train_df")

    tok = RegexTokenizer(
        inputCol="abstract", outputCol="tmp_tokens",
        pattern=r"[^\\p{L}]+", gaps=True, toLowercase=True
    )
    toks = tok.transform(train_df.select("id_base", "abstract"))

    exploded = (toks
        .select("id_base", F.explode_outer("tmp_tokens").alias("tok"))
        .where(F.length("tok") > 1))

    # One (doc, token) occurrence max
    doc_tok = exploded.dropDuplicates(["id_base", "tok"])

    dfreq = doc_tok.groupBy("tok").agg(F.countDistinct("id_base").alias("df"))
    top = dfreq.orderBy(F.desc("df")).limit(top_df)

    return [r["tok"] for r in top.collect()]


def build_text_pipeline(
    vocab_size: int = 80000,
    min_df: int = 3,
    use_bigrams: bool = False,
    extra_stopwords: Iterable[str] = ()
) -> Pipeline:
    """
    Pipeline:
      RegexTokenizer -> StopWordsRemover(default + extra + domain) -> (optional NGram(2) + concat)
      -> CountVectorizer(minDF, vocabSize) -> IDF -> Normalizer(p=2) as 'features_norm'
    """
    tokenizer = RegexTokenizer(
        inputCol="text", outputCol="tokens",
        pattern=r"[^\\p{L}]+", gaps=True, toLowercase=True)

    remover = StopWordsRemover(
        inputCol="tokens", outputCol="tokens_sw_removed",
        stopWords=sorted(set(StopWordsRemover.loadDefaultStopWords("english") +
                             list(DEFAULT_DOMAIN_STOPS) + list(extra_stopwords)))
    )

    stages = [
        # title + abstract => text
        # (use small SQL stage before to avoid a Python UDF)
    ]

    # We'll assemble 'text' upstream before calling the pipeline's .fit()
    # but keep tokenizer as the first stage for clarity.
    stages.append(tokenizer)
    stages.append(remover)

    final_tokens_col = "tokens_sw_removed"

    if use_bigrams:
        bigr = NGram(n=2, inputCol=final_tokens_col, outputCol="tokens_bi")
        stages.append(bigr)
        # concat unigrams + bigrams (keep counts — use array_concat)
        stages.append(_ConcatArrays(inputCol1=final_tokens_col, inputCol2="tokens_bi", outputCol="tokens_all"))
        final_tokens_col = "tokens_all"

    vectorizer = CountVectorizer(
        inputCol=final_tokens_col, outputCol="tf", vocabSize=vocab_size, minDF=min_df
    )
    idf = IDF(inputCol="tf", outputCol="features_tfidf")
    norm = Normalizer(inputCol="features_tfidf", outputCol="features_norm", p=2.0)

    return Pipeline(stages=[tokenizer, remover] + (stages[2:] if len(stages) > 2 else []) + [vectorizer, idf, norm])

# Helper transformer: concat two array<string> columns preserving multiplicity.
from pyspark.ml import Transformer
from pyspark.ml.param.shared import Param, Params
from pyspark.sql.types import ArrayType, StringType, StructType

class _ConcatArrays(Transformer):
    def __init__(self, inputCol1: str, inputCol2: str, outputCol: str):
        super().__init__()
        self.inputCol1 = inputCol1
        self.inputCol2 = inputCol2
        self.outputCol = outputCol

    def _transform(self, dataset: DataFrame) -> DataFrame:
        return dataset.withColumn(self.outputCol, F.array_concat(F.col(self.inputCol1), F.col(self.inputCol2)))


/home/data/akhalegh/utils/ccda-course-project/src/ingestion.py
================================================================================
# src/ingestion.py
"""
OOM-resistant ingestion for Kaggle arXiv metadata → Parquet.

Strategies:
  - Align the big shuffle with the output partition column (e.g., 'year')
  - Use many small partitions to minimize per-task memory
  - Parquet writer tuned for small in-memory buffers (block/page) and fewer rows per file
"""

import argparse
from pyspark.sql import functions as F
from src.utils import get_spark
from src.transformations import transform_all


def read_arxiv_json(spark, path: str, multiline: bool = False):
    """Read JSON/JSONL. Kaggle's arxiv snapshot is JSONL (so usually multiline=False)."""
    return (
        spark.read
        .option("multiLine", "true" if multiline else "false")
        .json(path)
    )


def main():
    ap = argparse.ArgumentParser(description="Ingest Kaggle arXiv metadata into Parquet (OOM-resistant).")
    ap.add_argument("--input", required=True, help="Path or glob to arxiv metadata JSON/JSONL (local).")
    ap.add_argument("--output", required=True, help="Directory to write processed Parquet.")
    ap.add_argument("--multiline", action="store_true", help="Input is pretty-printed multi-line JSON.")
    ap.add_argument("--limit", type=int, default=0, help="Optional: limit rows for quick demo.")
    ap.add_argument("--sample-frac", type=float, default=0.0, help="Optional: sample fraction (0<frac<=1).")
    ap.add_argument("--repartition", type=int, default=0, help="Target partitions per key before write (default 512).")
    ap.add_argument("--min-abstract-len", type=int, default=40, help="Filter very short abstracts.")
    ap.add_argument("--partition-by", default="year",
                    choices=["year", "primary_category", "none"],
                    help="Partition column for Parquet (or 'none').")
    ap.add_argument("--no-stats", action="store_true",
                    help="Skip post-write stats to minimize extra jobs.")
    args = ap.parse_args()

    spark = get_spark("arxiv_week8_ingestion_lowmem")

    # --- Read raw ---
    df_raw = read_arxiv_json(spark, args.input, multiline=args.multiline)

    # Optional sampling/limiting to validate pipeline quickly
    if args.sample_frac and 0.0 < args.sample_frac <= 1.0:
        df_raw = df_raw.sample(False, args.sample_frac, seed=42)
    if args.limit and args.limit > 0:
        df_raw = df_raw.limit(args.limit)

    # --- Transform & quality filters ---
    df = transform_all(df_raw, min_abstract_len=args.min_abstract_len)

    # --- Partition-aware shuffle alignment (critical to avoid a second huge shuffle) ---
    if args.partition_by in ("year", "primary_category"):
        target = args.repartition if args.repartition and args.repartition > 0 else 512
        df = df.repartition(target, F.col(args.partition_by))
    else:
        # No partitioned write: honor numeric repartition if provided
        if args.repartition and args.repartition > 0:
            df = df.repartition(args.repartition)

    # --- Write with small Parquet buffers to reduce writer heap usage ---
    writer = (
        df.write
          .mode("overwrite")
          .option("compression", "zstd")                         # can switch to 'snappy' if preferred
          .option("parquet.block.size", 8 * 1024 * 1024)         # 8 MB blocks
          .option("parquet.page.size", 512 * 1024)               # 512 KB pages
          .option("parquet.enable.dictionary", "true")
          .option("maxRecordsPerFile", 50000)                    # cap rows per file
    )
    if args.partition_by != "none":
        writer = writer.partitionBy(args.partition_by)

    writer.parquet(args.output)

    if not args.no_stats:
        # Lightweight-ish stats (aggregations only). Skip with --no-stats if you want zero extra jobs.
        n = df.count()
        print(f"[OK] Wrote {n} rows to {args.output}")

        try:
            top_cats = (
                df.groupBy("primary_category")
                  .count()
                  .orderBy(F.desc("count"))
                  .limit(10)
                  .collect()
            )
            print("[Top categories]")
            for r in top_cats:
                print(f"  {r['primary_category']}: {r['count']}")
        except Exception as e:
            print(f"[warn] top_cats aggregation skipped: {e}")

        try:
            by_year = (
                df.groupBy("year")
                  .count()
                  .orderBy("year")
                  .collect()
            )
            print("[Counts by year]")
            for r in by_year:
                print(f"  {r['year']}: {r['count']}")
        except Exception as e:
            print(f"[warn] by_year aggregation skipped: {e}")

    spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/tests/test_ingestion.py
================================================================================
"""
Unit tests for the ingestion pipeline (src/ingestion.py).

These tests validate that the ingestion process:
  - Produces a valid Spark DataFrame with the expected schema and types.
  - Contains derived columns like title_len, abstract_len, and has_doi.
  - Correctly parses authors, categories, and versions.
  - Writes readable parquet output without corruption.

Run:
    pytest tests/test_ingestion_pipeline.py -v
"""

import pytest
from pyspark.sql import SparkSession, Row
from pyspark.sql import functions as F
from src import ingestion


# ---------- Spark fixture ----------

@pytest.fixture(scope="session")
def spark():
    spark = (
        SparkSession.builder
        .appName("test_ingestion_pipeline")
        .master("local[2]")
        .getOrCreate()
    )
    yield spark
    spark.stop()


# ---------- sample raw data ----------

@pytest.fixture(scope="session")
def raw_sample_df(spark):
    """
    Creates a small synthetic DataFrame simulating raw arXiv JSON data
    (as produced by scripts/download_arxiv.py or similar).
    """
    data = [
        {
            "id": "arXiv:2101.00001",
            "title": "Deep Learning for Vision",
            "abstract": "We explore neural network architectures for computer vision.",
            "authors": "Alice, Bob",
            "categories": "cs.CV cs.LG",
            "versions": [{"v": 1}, {"v": 2}],
            "doi": "10.1000/example",
            "submitted_date": "2021-01-01T00:00:00Z",
        },
        {
            "id": "arXiv:1901.00234",
            "title": "Quantum Computing Basics",
            "abstract": "An introduction to quantum algorithms and their limitations.",
            "authors": "Carol",
            "categories": "quant-ph",
            "versions": [{"v": 1}],
            "doi": None,
            "submitted_date": "2019-02-15T00:00:00Z",
        },
    ]
    df = spark.createDataFrame(data)
    return df


# ---------- ingestion output fixture ----------

@pytest.fixture(scope="session")
def ingested_df(spark, raw_sample_df, tmp_path):
    """
    Runs the ingestion.transform() or main() function on the synthetic dataset
    to produce the processed DataFrame.
    """
    if hasattr(ingestion, "transform"):
        df_processed = ingestion.transform(raw_sample_df)
    elif hasattr(ingestion, "main"):
        df_processed = ingestion.main(raw_sample_df)
    else:
        pytest.skip("No valid ingestion entry function found in src/ingestion.py")

    # Optionally save to Parquet (to test writing)
    outpath = tmp_path / "arxiv_ingest_test.parquet"
    df_processed.write.mode("overwrite").parquet(str(outpath))
    df_reloaded = spark.read.parquet(str(outpath))
    return df_reloaded


# ---------- tests ----------

def test_schema_contains_expected_columns(ingested_df):
    expected_cols = {
        "arxiv_id", "title", "abstract", "year",
        "primary_category", "authors_list", "versions",
        "title_len", "abstract_len", "has_doi"
    }
    df_cols = set(ingested_df.columns)
    missing = expected_cols - df_cols
    assert not missing, f"Missing columns in ingestion output: {missing}"


def test_title_and_abstract_lengths_computed(ingested_df):
    lengths = ingested_df.select("title_len", "abstract_len").collect()
    for row in lengths:
        assert row.title_len > 0
        assert row.abstract_len > 0


def test_has_doi_column_valid(ingested_df):
    vals = [r.has_doi for r in ingested_df.select("has_doi").collect()]
    assert all(v in [True, False] for v in vals)


def test_authors_list_extracted(ingested_df):
    df = ingested_df.select(F.size(F.col("authors_list")).alias("n_authors"))
    assert df.filter(df.n_authors > 0).count() == df.count()


def test_primary_category_not_null(ingested_df):
    assert ingested_df.filter(F.col("primary_category").isNotNull()).count() > 0


def test_year_parsed_correctly(ingested_df):
    years = [r.year for r in ingested_df.select("year").distinct().collect()]
    assert all(isinstance(y, int) and 1900 < y < 2100 for y in years)


def test_versions_array_nonempty(ingested_df):
    counts = ingested_df.select(F.size(F.col("versions")).alias("n_versions"))
    assert counts.filter(counts.n_versions > 0).count() == counts.count()


def test_ingested_dataframe_nonempty(ingested_df):
    assert ingested_df.count() > 0


def test_unique_arxiv_ids(ingested_df):
    distinct = ingested_df.select("arxiv_id").distinct().count()
    total = ingested_df.count()
    assert distinct == total, "arxiv_id should be unique per paper"


def test_output_writable(tmp_path, ingested_df, spark):
    outpath = tmp_path / "ingestion_output.parquet"
    ingested_df.write.mode("overwrite").parquet(str(outpath))
    reloaded = spark.read.parquet(str(outpath))
    assert reloaded.count() == ingested_df.count()
    assert set(reloaded.columns) == set(ingested_df.columns)


/home/data/akhalegh/utils/ccda-course-project/tests/test_sql.py
================================================================================
"""
Unit tests for notebooks/week9-ComplexQueries.py

These tests create a miniature in-memory Spark DataFrame that imitates
the arXiv metadata and validate that each complex analytical query
runs successfully and returns expected columns.

Run:
    pytest tests/test_sql_complex.py -v
"""

import pytest
from pyspark.sql import SparkSession, Row
from notebooks import week9_ComplexQueries as w9

# ---------- Spark fixture ----------

@pytest.fixture(scope="session")
def spark():
    spark = (
        SparkSession.builder
        .appName("test_week9_complex_queries")
        .master("local[2]")
        .getOrCreate()
    )
    yield spark
    spark.stop()

# ---------- miniature dataset ----------

@pytest.fixture(scope="session")
def sample_df(spark):
    data = [
        Row(
            arxiv_id="a1",
            title="AI Paper 1",
            abstract="Deep learning improves vision tasks significantly.",
            abstract_len=55,
            primary_category="cs.LG",
            categories="cs.LG stat.ML",
            authors_list=["Alice", "Bob"],
            versions=[{"v": 1}, {"v": 2}],
            has_doi=True,
            year=2019,
            submitted_date="2019-03-15",
        ),
        Row(
            arxiv_id="a2",
            title="Physics optics",
            abstract="Quantum optics paper abstract text.",
            abstract_len=35,
            primary_category="physics.optics",
            categories="physics.optics",
            authors_list=["Carol"],
            versions=[{"v": 1}],
            has_doi=False,
            year=2018,
            submitted_date="2018-02-10",
        ),
        Row(
            arxiv_id="a3",
            title="ML and NLP",
            abstract="Language models improve translation accuracy.",
            abstract_len=47,
            primary_category="cs.CL",
            categories="cs.CL cs.LG",
            authors_list=["Alice", "Dan"],
            versions=[{"v": 1}, {"v": 2}, {"v": 3}],
            has_doi=True,
            year=2020,
            submitted_date="2020-06-02",
        ),
        Row(
            arxiv_id="a4",
            title="Cross discipline",
            abstract="Interdisciplinary study bridging physics and AI.",
            abstract_len=60,
            primary_category="cs.LG",
            categories="cs.LG physics.comp-ph",
            authors_list=["Eve", "Bob"],
            versions=[{"v": 1}],
            has_doi=False,
            year=2021,
            submitted_date="2021-09-07",
        ),
        Row(
            arxiv_id="a5",
            title="Old math",
            abstract="Classical mathematics revisited.",
            abstract_len=40,
            primary_category="math.GM",
            categories="math.GM",
            authors_list=["Frank"],
            versions=[{"v": 1}],
            has_doi=True,
            year=2010,
            submitted_date="2010-01-15",
        ),
    ]
    df = spark.createDataFrame(data)
    df.createOrReplaceTempView("papers")
    w9.ensure_aux_columns(spark)
    return df

# ---------- generic helper ----------

def check_nonempty(df, expected_cols):
    assert all(c in df.columns for c in expected_cols)
    assert df.count() >= 0  # allow empty edge cases

# ---------- individual tests ----------

def test_category_cooccurrence(spark, sample_df, tmp_path):
    w9.complex_1_category_cooccurrence(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_category_cooccurrence.csv"))
    check_nonempty(df, ["cat_a", "cat_b", "pair_count"])

def test_author_collaboration(spark, sample_df, tmp_path):
    w9.complex_2_author_collab_over_time(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_author_pairs_by_year.csv"))
    check_nonempty(df, ["year", "author_a", "author_b", "n_coauthored"])

def test_rising_declining_topics(spark, sample_df, tmp_path):
    w9.complex_3_rising_declining_topics(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_rising_declining_topics_fullrank.csv"))
    check_nonempty(df, ["primary_category", "pct_change"])

def test_readability_trends(spark, sample_df, tmp_path):
    w9.complex_4_readability_lexical_trends(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_lexical_richness_by_year.csv"))
    check_nonempty(df, ["year", "avg_lexical_richness"])

def test_doi_version_correlation(spark, sample_df, tmp_path):
    w9.complex_5_doi_versions_correlation(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_doi_versions_correlation.csv"))
    check_nonempty(df, ["corr_doi_versions"])

def test_author_lifecycle(spark, sample_df, tmp_path):
    w9.complex_6_author_productivity_lifecycle(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_author_lifecycle_top.csv"))
    check_nonempty(df, ["author", "paper_count"])

def test_author_category_migration(spark, sample_df, tmp_path):
    w9.complex_7_author_category_migration(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_author_category_migration.csv"))
    check_nonempty(df, ["author", "cat_earliest", "cat_latest"])

def test_abstract_popularity(spark, sample_df, tmp_path):
    w9.complex_8_abstract_len_vs_popularity(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_abstractlen_versions_correlation.csv"))
    check_nonempty(df, ["corr_abslen_versions"])

def test_weekday_submission(spark, sample_df, tmp_path):
    w9.complex_9_weekday_submission_patterns(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_weekday_submissions.csv"))
    check_nonempty(df, ["weekday_short", "submissions"])

def test_category_stability(spark, sample_df, tmp_path):
    w9.complex_10_category_stability_versions(spark, tmp_path)
    df = spark.read.option("header", True).csv(str(tmp_path / "complex_category_versions_avg.csv"))
    check_nonempty(df, ["primary_category", "avg_versions"])


/home/data/akhalegh/utils/ccda-course-project/tests/test_ml.py
================================================================================
def test_ml():
    assert True


/home/data/akhalegh/utils/ccda-course-project/tests/test_streaming.py
================================================================================
# tests/test_streaming.py
import io
import os
import sys
import time
from pathlib import Path

import pytest


def _write_jsonl_sample(path: Path, n: int):
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        for i in range(n):
            # minimal JSON per line
            f.write(f'{{"id":"{i}","title":"t{i}","abstract":"a{i}"}}\n')


def test_write_prefix_atomic_basic(tmp_path, monkeypatch):
    """
    Unit-test the low-level writer to ensure it writes exactly N non-empty lines,
    creates parent dirs, and performs an atomic replace.
    """
    # Import the module under test
    sys.path.insert(0, str(Path.cwd()))
    prep = __import__("scripts.prepare_sample_stream_batches".replace("/", "."), fromlist=["*"])

    src = tmp_path / "data" / "sample" / "arxiv-sample.jsonl"
    dst = tmp_path / "out" / "arxiv-sample-20250101.jsonl"
    _write_jsonl_sample(src, 10)

    written = prep.write_prefix_atomic(src, dst, n_lines=7, overwrite=False)
    assert written == 7
    assert dst.exists()

    # Check that only 7 non-empty lines are present
    with dst.open("r", encoding="utf-8") as f:
        lines = [ln for ln in f.readlines() if ln.strip()]
    assert len(lines) == 7

    # Re-run without overwrite; should skip and return -1
    written2 = prep.write_prefix_atomic(src, dst, n_lines=5, overwrite=False)
    assert written2 == -1

    # With overwrite=True it should replace contents
    written3 = prep.write_prefix_atomic(src, dst, n_lines=5, overwrite=True)
    assert written3 == 5
    with dst.open("r", encoding="utf-8") as f:
        lines2 = [ln for ln in f.readlines() if ln.strip()]
    assert len(lines2) == 5


def test_prepare_main_creates_expected_files(tmp_path, monkeypatch):
    """
    Integration-ish test of `main()`:
    - Point SOURCE and OUTDIR to temp dirs
    - Shrink SIZES to keep test snappy
    - Force --no-sleep so we don't actually wait
    - Assert filenames are weekly-stamped and end with .jsonl
    """
    sys.path.insert(0, str(Path.cwd()))
    prep = __import__("scripts.prepare_sample_stream_batches".replace("/", "."), fromlist=["*"])

    # Point module globals at temp fixture locations
    src = tmp_path / "data" / "sample" / "arxiv-sample.jsonl"
    outdir = tmp_path / "data" / "stream" / "incoming_sample"
    _write_jsonl_sample(src, 100)  # plenty for our tiny sizes

    monkeypatch.setattr(prep, "SOURCE", src, raising=True)
    monkeypatch.setattr(prep, "OUTDIR", outdir, raising=True)

    # Keep it quick: three files, increasing prefixes
    monkeypatch.setattr(prep, "SIZES", [3, 5, 7], raising=True)

    # Fake sleep to avoid delays even if code path hits it
    monkeypatch.setattr(time, "sleep", lambda s: None)

    # Run main with deterministic start date and no sleep
    argv = [
        "prepare_sample_stream_batches.py",
        "--start-date",
        "2025-10-24",
        "--interval-seconds",
        "1",
        "--no-sleep",
        "--overwrite",
    ]
    monkeypatch.setenv("PYTHONHASHSEED", "0")
    monkeypatch.setattr(sys, "argv", argv)

    # Execute
    prep.main()

    # Validate outputs
    files = sorted(outdir.glob("arxiv-sample-*.jsonl"))
    assert len(files) == 3

    # Expect weekly stepping in YYYYMMDD
    stamps = [p.stem.split("-")[-1] for p in files]
    assert stamps == ["20251024", "20251031", "20251107"]  # weekly increments from 2025-10-24

    # Confirm suffix is .jsonl (regression for the extension mismatch bug)
    assert all(p.suffix == ".jsonl" for p in files)

    # Confirm line counts match the requested prefixes
    counts = []
    for p in files:
        with p.open("r", encoding="utf-8") as f:
            counts.append(sum(1 for ln in f if ln.strip()))
    assert counts == [3, 5, 7]


def test_streaming_filename_regex_captures_8_or_12_digits(tmp_path):
    """
    Check the regex used to extract date stamps from filenames accepts either:
    - YYYYMMDD.jsonl
    - YYYYMMDDHHMM.jsonl (12-digit stamp)
    and always returns the first 8 digits for the folder.
    """
    # Import the streaming module without requiring Spark constructs
    sys.path.insert(0, str(Path.cwd()))
    stream_mod = __import__("notebooks.streaming_sample_week11".replace("/", "."), fromlist=["*"])

    # Sanity: regex exists and looks for .jsonl files
    pattern = stream_mod.FILE_DATE_REGEX
    assert pattern.pattern.endswith(r"\.jsonl$")

    paths = [
        "data/stream/incoming_sample/arxiv-sample-20251024.jsonl",
        "data/stream/incoming_sample/arxiv-sample-202510240930.jsonl",
        "/abs/path/arxiv-sample-20240101.jsonl",
        "/abs/path/arxiv-sample-202401012359.jsonl",
    ]
    expected = ["20251024", "20251024", "20240101", "20240101"]
    for p, exp in zip(paths, expected):
        m = pattern.match(p)
        assert m, f"Pattern failed to match {p}"
        assert m.group(1) == exp


def test_streaming_schema_fields_present():
    """
    Light-touch schema test (skips if pyspark isn't installed).
    Ensures expected columns exist so streaming starts without inference.
    """
    pyspark = pytest.importorskip("pyspark")  # skip gracefully if not available
    sys.path.insert(0, str(Path.cwd()))
    stream_mod = __import__("notebooks.streaming_sample_week11".replace("/", "."), fromlist=["*"])

    schema = stream_mod.JSON_SCHEMA
    field_names = {f.name for f in schema.fields}

    # A few key fields to ensure compatibility with the sample dataset/transforms
    for col in [
        "id",
        "title",
        "abstract",
        "categories",
        "doi",
        "journal-ref",
        "authors",
        "authors_parsed",
        "versions",
        "submitted_date",
        "update_date",
    ]:
        assert col in field_names


