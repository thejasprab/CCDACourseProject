PROJECT FILE TREE
================================================================================
ccda-course-project_v2/
    run_sample.sh
    run.sh
    requirements.txt
    engine/
        __init__.py
        search/
            similarity.py
            vectorize.py
            search_engine.py
        utils/
            misc.py
            spark_utils.py
            __init__.py
            io_utils.py
        complex/
            complex_queries.py
        ml/
            model_loader.py
            train.py
            __init__.py
            featurization.py
    pipelines/
        train_full.py
        train_sample.py
        ingest_full.py
        complex_sample.py
        complex_full.py
        ingest_sample.py
    streaming/
        merge_diff.py
        kaggle_downloader.py
        sample_prepare_batches.py
        full_stream.py
        sample_stream.py
    app/
        server.py
        __init__.py
        config.py
        services/
            __init__.py
            spark_session.py
            complex_service.py
            filters_service.py
            search_service.py
    spark-warehouse/


/home/data/akhalegh/utils/ccda-course-project_v2/run_sample.sh
================================================================================
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$ROOT_DIR"

SAMPLE="data/sample/arxiv-sample.jsonl"

echo "[check] ensuring sample dataset exists..."
if [[ ! -f "$SAMPLE" ]]; then
  echo "[download] fetching arXiv snapshot + writing sample via KaggleHub..."
  python -m streaming.kaggle_downloader --mode sample --sample-size 50000
fi

echo "[ingest] SAMPLE dataset → Parquet..."
python -m pipelines.ingest_sample

echo "[ml] training TF-IDF model on SAMPLE dataset..."
python -m pipelines.train_sample

echo "[complex] running complex SQL analytics on SAMPLE dataset..."
python -m pipelines.complex_sample

echo "[stream] preparing simulated weekly sample drops..."
python -m streaming.sample_prepare_batches --start-date "$(date +%Y-%m-%d)" --interval-seconds 1 --no-sleep --overwrite

echo "[stream] starting SAMPLE streaming job (Ctrl+C to stop)..."
python -m streaming.sample_stream

echo "[done] SAMPLE pipeline steps executed. Artifacts under:"
echo "  - data/processed/arxiv_sample/"
echo "  - data/models/tfidf_sample/"
echo "  - data/processed/features_sample/"
echo "  - reports/analysis_sample/"
echo "  - reports/streaming_sample/"


/home/data/akhalegh/utils/ccda-course-project_v2/run.sh
================================================================================
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$ROOT_DIR"

RAW="data/raw/arxiv-metadata-oai-snapshot.json"

echo "[check] ensuring full raw dataset exists..."
if [[ ! -f "$RAW" ]]; then
  echo "[download] fetching arXiv full snapshot via KaggleHub..."
  python -m streaming.kaggle_downloader --mode full
fi

echo "[ingest] FULL dataset → Parquet..."
python -m pipelines.ingest_full

echo "[ml] training TF-IDF model on FULL dataset..."
python -m pipelines.train_full

echo "[complex] running complex SQL analytics on FULL dataset..."
python -m pipelines.complex_full

# Optional: one-shot weekly streaming processing (batch style).
# Uncomment if you want this as part of the pipeline:
# echo "[stream] processing latest FULL streaming drop (--once)..."
# python -m streaming.full_stream --once

echo "[done] FULL pipeline complete. Artifacts under:"
echo "  - data/processed/arxiv_full/"
echo "  - data/models/tfidf_full/"
echo "  - data/processed/features_full/"
echo "  - reports/analysis_full/"
echo "  - reports/streaming_full/ (if streaming run)"


/home/data/akhalegh/utils/ccda-course-project_v2/requirements.txt
================================================================================
pyspark==3.5.1
pandas
pyarrow
matplotlib
jupyter
kagglehub
Flask

/home/data/akhalegh/utils/ccda-course-project_v2/engine/__init__.py
================================================================================
# ccda-course-project/engine/__init__.py


/home/data/akhalegh/utils/ccda-course-project_v2/engine/search/similarity.py
================================================================================
from typing import List

from pyspark.sql import DataFrame, functions as F, Window
from pyspark.ml.linalg import SparseVector
from pyspark.sql.types import DoubleType

# Cosine on L2-normalized vectors == dot product.


def _dot(a: SparseVector, b: SparseVector) -> float:
    if a is None or b is None:
        return 0.0
    ai = dict(zip(a.indices, a.values))
    s = 0.0
    for j, v in zip(b.indices, b.values):
        if j in ai:
            s += ai[j] * v
    return float(s)


_dot_udf = F.udf(_dot, DoubleType())


def topk_exact(
    test_df: DataFrame,
    train_df: DataFrame,
    k: int = 3,
    exclude_self: bool = True,
) -> DataFrame:
    """
    Brute-force exact cosine@K (safe for sample sizes). Assumes 'features' are L2-normalized.
    Expects columns:
      test_df : id_base, categories, features
      train_df: id_base, categories, features
    Returns: test_id, rank, neighbor_id, score, neighbor_categories
    """
    joined = (
        test_df.alias("q")
        .crossJoin(F.broadcast(train_df.alias("c")))
        .withColumn("score", _dot_udf(F.col("q.features"), F.col("c.features")))
    )

    if exclude_self:
        joined = joined.where(F.col("q.id_base") != F.col("c.id_base"))

    w = Window.partitionBy("q.id_base").orderBy(F.desc("score"), F.col("c.id_base"))
    top = (
        joined.withColumn("rank", F.row_number().over(w))
        .where(F.col("rank") <= k)
        .select(
            F.col("q.id_base").alias("test_id"),
            "rank",
            F.col("c.id_base").alias("neighbor_id"),
            "score",
            F.col("c.categories").alias("neighbor_categories"),
        )
    )
    return top


/home/data/akhalegh/utils/ccda-course-project_v2/engine/search/vectorize.py
================================================================================
from pyspark.sql import SparkSession, functions as F
from pyspark.ml import PipelineModel

from engine.search.similarity import topk_exact


def vectorize_query(
    spark: SparkSession, model: PipelineModel, title: str, abstract: str
):
    text = (title or "") + " " + (abstract or "")
    df = spark.createDataFrame([(text,)], ["text"])
    out = model.transform(df).select(F.col("features_norm").alias("features"))
    return out.first()["features"]


def query_topk(
    spark: SparkSession,
    model: PipelineModel,
    features_train_df,
    query_title: str,
    query_abstract: str,
    k: int = 5,
):
    from pyspark.ml.linalg import SparseVector  # noqa: F401

    qvec = vectorize_query(spark, model, query_title, query_abstract)
    qdf = spark.createDataFrame([("Q", qvec)], ["id_base", "features"])
    qdf = qdf.withColumn("categories", F.array().cast("array<string>"))
    recs = topk_exact(qdf, features_train_df, k=k, exclude_self=False)
    return recs


/home/data/akhalegh/utils/ccda-course-project_v2/engine/search/search_engine.py
================================================================================
from __future__ import annotations

from typing import List, Dict, Any

from pyspark.sql import functions as F

from engine.utils.spark_utils import get_spark
from engine.ml.model_loader import load_model_and_features
from engine.search.vectorize import query_topk


class SearchEngine:
    """
    Tiny façade around TF-IDF + cosine Top-K for either 'sample' or 'full' mode.
    """

    def __init__(self, mode: str = "sample", spark=None):
        if mode not in {"sample", "full"}:
            raise ValueError("mode must be 'sample' or 'full'")
        self.mode = mode
        self.spark = spark or get_spark(f"search_{mode}")
        self.model, self.features = load_model_and_features(self.spark, mode)

    def search(
        self, title: str = "", abstract: str = "", k: int = 10
    ) -> List[Dict[str, Any]]:
        base = self.features.select(
            "id_base",
            "paper_id",
            "title",
            "abstract",
            "categories",
            "year",
            "features",
        )

        recs = query_topk(
            self.spark,
            self.model,
            base.select("id_base", "categories", "features"),
            title,
            abstract,
            k=k,
        )

        meta = base.select(
            F.col("id_base").alias("neighbor_id_meta"),
            "paper_id",
            "title",
            "abstract",
            "categories",
            "year",
        )

        joined = (
            recs.join(
                meta, recs.neighbor_id == meta.neighbor_id_meta, "left"
            ).drop("neighbor_id_meta")
            .orderBy("rank")
        )

        out: List[Dict[str, Any]] = []
        for r in joined.collect():
            out.append(
                {
                    "rank": int(r["rank"]),
                    "score": float(r["score"]),
                    "neighbor_id": r["neighbor_id"],
                    "paper_id": r["paper_id"],
                    "title": r["title"],
                    "abstract": r["abstract"],
                    "categories": r["categories"],
                    "year": r["year"],
                }
            )
        return out


/home/data/akhalegh/utils/ccda-course-project_v2/engine/utils/misc.py
================================================================================
import logging

LOGGER_NAME = "ccda_project"

logger = logging.getLogger(LOGGER_NAME)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        "[%(asctime)s] [%(levelname)s] %(name)s - %(message)s"
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)


def log_info(msg: str) -> None:
    logger.info(msg)


def log_warn(msg: str) -> None:
    logger.warning(msg)


def log_error(msg: str) -> None:
    logger.error(msg)


/home/data/akhalegh/utils/ccda-course-project_v2/engine/utils/spark_utils.py
================================================================================
from pathlib import Path
from pyspark.sql import SparkSession


def get_spark(app_name: str = "ccda_project") -> SparkSession:
    """
    Create (or get) a SparkSession tuned for low-memory environments.
    """
    local_tmp = Path("data/tmp/spark-local")
    local_tmp.mkdir(parents=True, exist_ok=True)

    spark = (
        SparkSession.builder.appName(app_name)
        .config("spark.sql.session.timeZone", "UTC")
        # Memory
        .config("spark.driver.memory", "6g")
        .config("spark.executor.memory", "6g")
        .config("spark.driver.maxResultSize", "2g")
        # Lots of small tasks + AQE
        .config("spark.sql.shuffle.partitions", "512")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "8m")
        .config("spark.sql.files.maxPartitionBytes", "8m")
        # Skew handling
        .config("spark.sql.adaptive.skewJoin.enabled", "true")
        .config("spark.sql.adaptive.skewedPartitionThresholdInBytes", "64m")
        .config("spark.sql.adaptive.skewedPartitionMaxSplitBytes", "16m")
        # Temp dirs
        .config("spark.local.dir", str(local_tmp))
        .config("spark.shuffle.checksum.enabled", "false")
        # Default parquet compression
        .config("spark.sql.parquet.compression.codec", "zstd")
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")
        .getOrCreate()
    )
    return spark


/home/data/akhalegh/utils/ccda-course-project_v2/engine/utils/__init__.py
================================================================================
# ccda-course-project/engine/utils/__init__.py


/home/data/akhalegh/utils/ccda-course-project_v2/engine/utils/io_utils.py
================================================================================
from pathlib import Path
import json
from typing import Any


def ensure_dir(path: str | Path) -> Path:
    p = Path(path)
    p.mkdir(parents=True, exist_ok=True)
    return p


def write_json(obj: Any, path: str | Path) -> None:
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(obj, f, indent=2)


/home/data/akhalegh/utils/ccda-course-project_v2/engine/complex/complex_queries.py
================================================================================
#!/usr/bin/env python3
"""
Complex Spark SQL / DataFrame analyses for arXiv metadata.

This module is fully self-contained and does NOT import anything
from the old notebooks. It assumes you are running on the already
ingested Parquet produced by your ingestion pipeline.

Typical use
-----------
From Python:

    from engine.complex.complex_queries import run_complex_queries

    run_complex_queries(
        parquet_path="data/processed/arxiv_full",
        outdir="reports/analysis_full"
    )

From CLI:

    python -m engine.complex.complex_queries \
        --parquet data/processed/arxiv_full \
        --outdir reports/analysis_full
"""

from __future__ import annotations

import argparse
from pathlib import Path

import numpy as np
import pandas as pd

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from pyspark.sql import SparkSession, functions as F


# ----------------------------------------------------------------------
# Helpers: filesystem + plotting
# ----------------------------------------------------------------------


def _ensure_outdir(outdir: str | Path) -> Path:
    p = Path(outdir)
    p.mkdir(parents=True, exist_ok=True)
    return p


def save_df_as_csv(df_spark, path: Path):
    """
    Save a small aggregated Spark DataFrame to CSV by collecting to pandas.
    """
    pdf = df_spark.toPandas()
    pdf.to_csv(path, index=False)
    print(f"[saved] {path}")
    return pdf  # return for plotting convenience


def matplotlib_savefig(path: Path):
    plt.tight_layout()
    path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"[plot] {path}")


def _finalize_axes(ax, title: str, xlabel: str = "", ylabel: str = ""):
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)


def barh_topn(
    pdf: pd.DataFrame,
    label_col: str,
    value_col: str,
    out_png: Path,
    title: str,
    topn: int = 30,
):
    if pdf.empty:
        return
    df = pdf[[label_col, value_col]].dropna()
    df = df.sort_values(value_col, ascending=False).head(topn)
    fig, ax = plt.subplots(figsize=(10, 0.35 * max(4, len(df))))
    ax.barh(df[label_col].astype(str), df[value_col])
    ax.invert_yaxis()
    _finalize_axes(ax, title, xlabel=value_col, ylabel=label_col)
    matplotlib_savefig(out_png)


def line_xy(
    pdf: pd.DataFrame,
    x_col: str,
    y_col: str,
    out_png: Path,
    title: str,
    xlabel: str = "",
    ylabel: str = "",
):
    if pdf.empty:
        return
    df = pdf[[x_col, y_col]].dropna().sort_values(x_col)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.plot(df[x_col], df[y_col], marker="o")
    _finalize_axes(ax, title, xlabel or x_col, ylabel or y_col)
    matplotlib_savefig(out_png)


def scatter_xy(
    pdf: pd.DataFrame,
    x_col: str,
    y_col: str,
    out_png: Path,
    title: str,
    xlabel: str = "",
    ylabel: str = "",
):
    if pdf.empty:
        return
    df = pdf[[x_col, y_col]].dropna()
    fig, ax = plt.subplots(figsize=(8, 6))
    ax.scatter(df[x_col], df[y_col], alpha=0.7)
    _finalize_axes(ax, title, xlabel or x_col, ylabel or y_col)
    matplotlib_savefig(out_png)


def single_value_figure(
    value: float | int | str,
    out_png: Path,
    title: str,
    label: str = "",
):
    fig, ax = plt.subplots(figsize=(6, 3))
    ax.axis("off")
    text = f"{label}: {value}" if label else f"{value}"
    ax.text(0.5, 0.5, text, ha="center", va="center", fontsize=16)
    ax.set_title(title)
    matplotlib_savefig(out_png)


def weekday_sort(df: pd.DataFrame, day_col: str, daynum_col: str) -> pd.DataFrame:
    df[daynum_col] = pd.to_numeric(df[daynum_col], errors="coerce")
    return df.sort_values(daynum_col)


# ----------------------------------------------------------------------
# Aux view builder
# ----------------------------------------------------------------------


def ensure_aux_columns(spark: SparkSession) -> None:
    """
    Prepare helper columns on top of the `papers` view:

    Creates a new temp view `papers_enriched` with:

      - categories_list   : array<string> (from categories/primary_category)
      - n_versions        : int
      - doi_int           : 0/1 flag
    """
    cols = [c.name for c in spark.table("papers").schema]

    extra_exprs = []

    # categories_list
    if "categories_list" in cols:
        extra_exprs.append("categories_list")
    elif "categories" in cols:
        extra_exprs.append("SPLIT(categories, ' ') AS categories_list")
    else:
        # fallback to primary_category, will still work for many queries
        extra_exprs.append("ARRAY(primary_category) AS categories_list")

    # n_versions
    if "versions" in cols:
        extra_exprs.append("SIZE(versions) AS n_versions")
    else:
        extra_exprs.append("CAST(1 AS INT) AS n_versions")

    # doi_int
    if "has_doi" in cols:
        extra_exprs.append("CASE WHEN has_doi THEN 1 ELSE 0 END AS doi_int")
    elif "doi" in cols:
        extra_exprs.append(
            "CASE WHEN doi IS NOT NULL AND TRIM(doi) <> '' THEN 1 ELSE 0 END AS doi_int"
        )
    else:
        extra_exprs.append("CAST(0 AS INT) AS doi_int")

    select_extra = ", ".join(extra_exprs)
    query = f"SELECT *, {select_extra} FROM papers"

    spark.sql("DROP VIEW IF EXISTS papers_enriched")
    spark.sql(f"CREATE OR REPLACE TEMP VIEW papers_enriched AS {query}")


# ----------------------------------------------------------------------
# Complex queries (10)
# ----------------------------------------------------------------------


def complex_1_category_cooccurrence(spark: SparkSession, outdir: Path) -> None:
    """
    Interdisciplinary Category Co-occurrence:
    frequently co-occurring category pairs within the same paper.
    """
    print("\n[complex-1] Category co-occurrence pairs")

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW cat_pairs AS
        SELECT
            arxiv_id,
            c1 AS cat_a,
            c2 AS cat_b
        FROM (
          SELECT arxiv_id, categories_list
          FROM papers_enriched
        ) t
        LATERAL VIEW EXPLODE(categories_list) a AS c1
        LATERAL VIEW EXPLODE(categories_list) b AS c2
        WHERE c1 < c2
    """)

    df = spark.sql("""
        SELECT cat_a, cat_b, COUNT(*) AS pair_count
        FROM cat_pairs
        GROUP BY cat_a, cat_b
        HAVING pair_count > 1
        ORDER BY pair_count DESC, cat_a, cat_b
        LIMIT 200
    """)

    csv_path = outdir / "complex_category_cooccurrence.csv"
    pdf = save_df_as_csv(df, csv_path)

    if not pdf.empty:
        pdf["pair"] = pdf["cat_a"].astype(str) + " × " + pdf["cat_b"].astype(str)
        barh_topn(
            pdf,
            "pair",
            "pair_count",
            outdir / "complex_category_cooccurrence_top.png",
            "Top category co-occurrences",
            topn=30,
        )


def complex_2_author_collab_over_time(spark: SparkSession, outdir: Path) -> None:
    """
    Author collaboration network over time (author pairs per year).
    """
    print("\n[complex-2] Author collaboration pairs by year")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "authors_list" not in cols or "year" not in cols:
        print("[skip] authors_list or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_pairs AS
        SELECT
          year,
          a1 AS author_a,
          a2 AS author_b
        FROM (
          SELECT year, authors_list FROM papers_enriched
        )
        LATERAL VIEW EXPLODE(authors_list) L1 AS a1
        LATERAL VIEW EXPLODE(authors_list) L2 AS a2
        WHERE a1 < a2
    """)

    df = spark.sql("""
        SELECT year, author_a, author_b, COUNT(*) AS n_coauthored
        FROM author_pairs
        GROUP BY year, author_a, author_b
        HAVING n_coauthored >= 2
        ORDER BY year, n_coauthored DESC
        LIMIT 500
    """)

    csv_path = outdir / "complex_author_pairs_by_year.csv"
    pdf = save_df_as_csv(df, csv_path)

    if pdf.empty:
        return

    # per-year totals
    per_year = pdf.groupby("year", as_index=False)["n_coauthored"].sum()
    line_xy(
        per_year,
        "year",
        "n_coauthored",
        outdir / "complex_author_pairs_by_year_totals.png",
        "Total coauthored pairs (n>=2) per year",
        xlabel="year",
        ylabel="pair count (sum n_coauthored)",
    )

    # top 20 pairs (author_a × author_b (year))
    top_pairs = pdf.copy()
    top_pairs["pair"] = (
        top_pairs["author_a"].astype(str)
        + " × "
        + top_pairs["author_b"].astype(str)
        + " ("
        + top_pairs["year"].astype(str)
        + ")"
    )
    barh_topn(
        top_pairs,
        "pair",
        "n_coauthored",
        outdir / "complex_author_pairs_by_year_top20.png",
        "Top author pairs by year (n_coauthored ≥ 2)",
        topn=20,
    )


def complex_3_rising_declining_topics(spark: SparkSession, outdir: Path) -> None:
    """
    Rising and declining topics:
    compare earliest vs latest year volume per primary_category.
    """
    print("\n[complex-3] Rising and declining topics")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "primary_category" not in cols or "year" not in cols:
        print("[skip] primary_category or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW cat_year_counts AS
        SELECT primary_category, year, COUNT(*) AS c
        FROM papers_enriched
        GROUP BY primary_category, year
    """)

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW cat_year_bounds AS
        SELECT
          primary_category,
          MIN(year) AS y_min,
          MAX(year) AS y_max
        FROM cat_year_counts
        GROUP BY primary_category
    """)

    df = spark.sql("""
        WITH endpoints AS (
          SELECT
            b.primary_category,
            b.y_min,
            b.y_max,
            MIN(CASE WHEN c.year = b.y_min THEN c.c END) AS c_start,
            MIN(CASE WHEN c.year = b.y_max THEN c.c END) AS c_end
          FROM cat_year_bounds b
          JOIN cat_year_counts c
            ON c.primary_category = b.primary_category
           AND c.year IN (b.y_min, b.y_max)
          GROUP BY b.primary_category, b.y_min, b.y_max
        )
        SELECT
          primary_category,
          y_min,
          y_max,
          c_start,
          c_end,
          CASE WHEN c_start > 0 THEN ROUND((c_end - c_start) * 100.0 / c_start, 2)
               ELSE NULL
          END AS pct_change
        FROM endpoints
        WHERE c_start IS NOT NULL AND c_end IS NOT NULL
        ORDER BY pct_change DESC NULLS LAST
    """)

    full_csv = outdir / "complex_rising_declining_topics_fullrank.csv"
    pdf_full = save_df_as_csv(df, full_csv)

    if pdf_full.empty:
        return

    # Top 20 rising & declining
    rising = df.orderBy(F.col("pct_change").desc_nulls_last()).limit(20)
    declining = df.orderBy(F.col("pct_change").asc_nulls_last()).limit(20)

    pdf_rise = save_df_as_csv(rising, outdir / "complex_rising_topics_top20.csv")
    pdf_decl = save_df_as_csv(declining, outdir / "complex_declining_topics_top20.csv")

    if not pdf_rise.empty:
        barh_topn(
            pdf_rise,
            "primary_category",
            "pct_change",
            outdir / "complex_rising_topics_top20.png",
            "Top 20 rising topics by % change",
        )

    if not pdf_decl.empty:
        pdf_decl_plot = pdf_decl.sort_values("pct_change", ascending=True)
        barh_topn(
            pdf_decl_plot,
            "primary_category",
            "pct_change",
            outdir / "complex_declining_topics_top20.png",
            "Top 20 declining topics by % change",
        )


def complex_4_readability_lexical_trends(spark: SparkSession, outdir: Path) -> None:
    """
    Abstract readability proxy via lexical richness over time:
    lexical_richness = distinct_token_count / token_count
    """
    print("\n[complex-4] Readability / Lexical richness trends")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "abstract" not in cols or "year" not in cols:
        print("[skip] abstract or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW abstract_tokens AS
        SELECT
          year,
          SIZE(SPLIT(LOWER(abstract), '\\\\s+')) AS tok_count,
          SIZE(ARRAY_DISTINCT(SPLIT(LOWER(abstract), '\\\\s+'))) AS distinct_tok_count
        FROM papers_enriched
        WHERE abstract IS NOT NULL AND LENGTH(abstract) > 0
    """)

    df = spark.sql("""
        SELECT
          year,
          AVG(CAST(distinct_tok_count AS DOUBLE) / NULLIF(tok_count, 0)) AS avg_lexical_richness,
          AVG(tok_count) AS avg_token_count
        FROM abstract_tokens
        GROUP BY year
        ORDER BY year
    """)

    csv = outdir / "complex_lexical_richness_by_year.csv"
    pdf = save_df_as_csv(df, csv)

    if pdf.empty:
        return

    line_xy(
        pdf,
        "year",
        "avg_lexical_richness",
        outdir / "complex_lexical_richness_by_year.png",
        "Average lexical richness by year",
        xlabel="year",
        ylabel="avg lexical richness",
    )

    line_xy(
        pdf,
        "year",
        "avg_token_count",
        outdir / "complex_avg_token_count_by_year.png",
        "Average abstract length (tokens) by year",
        xlabel="year",
        ylabel="avg tokens",
    )


def complex_5_doi_versions_correlation(spark: SparkSession, outdir: Path) -> None:
    """
    DOI vs Version correlation:
    average number of versions for DOI vs non-DOI, plus Pearson correlation.
    """
    print("\n[complex-5] DOI vs versions correlation")

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW doi_versions AS
        SELECT doi_int, n_versions FROM papers_enriched
    """)

    group_df = spark.sql("""
        SELECT
          doi_int,
          AVG(n_versions) AS avg_versions
        FROM doi_versions
        GROUP BY doi_int
        ORDER BY doi_int DESC
    """)

    csv_group = outdir / "complex_doi_vs_versions_group.csv"
    pdf_group = save_df_as_csv(group_df, csv_group)

    corr_df = spark.sql("""
        SELECT CORR(CAST(doi_int AS DOUBLE), CAST(n_versions AS DOUBLE)) AS corr_doi_versions
        FROM doi_versions
    """)

    csv_corr = outdir / "complex_doi_versions_correlation.csv"
    pdf_corr = save_df_as_csv(corr_df, csv_corr)

    if not pdf_group.empty:
        pdf_group["doi_label"] = np.where(
            pdf_group["doi_int"].astype(int) == 1, "DOI present", "No DOI"
        )
        barh_topn(
            pdf_group,
            "doi_label",
            "avg_versions",
            outdir / "complex_doi_vs_versions_group.png",
            "Avg # versions by DOI presence",
            topn=2,
        )

    if not pdf_corr.empty:
        val = float(pdf_corr["corr_doi_versions"].iloc[0])
        single_value_figure(
            round(val, 4),
            outdir / "complex_doi_versions_correlation.png",
            "Pearson correlation",
            label="corr(doi_int, n_versions)",
        )


def complex_6_author_productivity_lifecycle(
    spark: SparkSession, outdir: Path
) -> None:
    """
    Author productivity lifespan and volume:
    first_year, last_year, active span, and paper count.
    """
    print("\n[complex-6] Author productivity lifecycle")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "authors_list" not in cols or "year" not in cols:
        print("[skip] authors_list or year missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_years AS
        SELECT
          EXPLODE(authors_list) AS author,
          year
        FROM papers_enriched
        WHERE authors_list IS NOT NULL
    """)

    df = spark.sql("""
        SELECT
          author,
          MIN(year) AS first_year,
          MAX(year) AS last_year,
          (MAX(year) - MIN(year)) AS active_span_years,
          COUNT(*) AS paper_count
        FROM author_years
        GROUP BY author
        HAVING paper_count >= 2
        ORDER BY active_span_years DESC, paper_count DESC
        LIMIT 1000
    """)

    csv = outdir / "complex_author_lifecycle_top.csv"
    pdf = save_df_as_csv(df, csv)

    if pdf.empty:
        return

    scatter_xy(
        pdf,
        "active_span_years",
        "paper_count",
        outdir / "complex_author_lifecycle_scatter.png",
        "Author lifecycle: span vs paper count",
        xlabel="active span (years)",
        ylabel="# papers",
    )


def complex_7_author_category_migration(
    spark: SparkSession, outdir: Path
) -> None:
    """
    Author category migration:
    dominant category earliest vs latest year, for authors who change.
    """
    print("\n[complex-7] Author category migration")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if (
        "authors_list" not in cols
        or "year" not in cols
        or "primary_category" not in cols
    ):
        print("[skip] need authors_list, year, primary_category")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_cat_year AS
        SELECT
          author,
          year,
          primary_category,
          COUNT(*) AS c
        FROM (
          SELECT
            year,
            primary_category,
            author
          FROM papers_enriched
          LATERAL VIEW OUTER EXPLODE(authors_list) a AS author
        ) t
        WHERE author IS NOT NULL AND TRIM(author) <> ''
        GROUP BY author, year, primary_category
    """)

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_bounds AS
        SELECT
          author,
          MIN(year) AS y_min,
          MAX(year) AS y_max
        FROM author_cat_year
        GROUP BY author
    """)

    # earliest
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_earliest AS
        SELECT author, cat_earliest FROM (
          SELECT
            acy.author,
            acy.primary_category AS cat_earliest,
            ROW_NUMBER() OVER (
              PARTITION BY acy.author
              ORDER BY acy.c DESC, acy.primary_category
            ) AS rn
          FROM author_cat_year acy
          JOIN author_bounds b
            ON b.author = acy.author AND acy.year = b.y_min
        ) t
        WHERE rn = 1
    """)

    # latest
    spark.sql("""
        CREATE OR REPLACE TEMP VIEW author_latest AS
        SELECT author, cat_latest FROM (
          SELECT
            acy.author,
            acy.primary_category AS cat_latest,
            ROW_NUMBER() OVER (
              PARTITION BY acy.author
              ORDER BY acy.c DESC, acy.primary_category
            ) AS rn
          FROM author_cat_year acy
          JOIN author_bounds b
            ON b.author = acy.author AND acy.year = b.y_max
        ) t
        WHERE rn = 1
    """)

    df = spark.sql("""
        SELECT
          e.author,
          e.cat_earliest,
          l.cat_latest
        FROM author_earliest e
        JOIN author_latest l USING (author)
        WHERE e.cat_earliest <> l.cat_latest
        ORDER BY author
        LIMIT 5000
    """)

    csv = outdir / "complex_author_category_migration.csv"
    pdf = save_df_as_csv(df, csv)

    if pdf.empty:
        return

    # top 20 transitions
    trans = (
        pdf.groupby(["cat_earliest", "cat_latest"])
        .size()
        .reset_index(name="n")
    )
    trans["transition"] = (
        trans["cat_earliest"].astype(str)
        + " → "
        + trans["cat_latest"].astype(str)
    )
    barh_topn(
        trans,
        "transition",
        "n",
        outdir / "complex_author_category_migration_top20.png",
        "Top 20 author category migrations",
        topn=20,
    )


def complex_8_abstract_len_vs_popularity(
    spark: SparkSession, outdir: Path
) -> None:
    """
    Abstract length vs popularity proxy (n_versions):
      - correlation
      - decile analysis by abstract_len
    """
    print("\n[complex-8] Abstract length vs popularity (versions)")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "abstract_len" not in cols:
        print("[skip] abstract_len missing")
        return

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW abs_pop AS
        SELECT
          CAST(abstract_len AS DOUBLE) AS abstract_len,
          CAST(n_versions AS DOUBLE) AS n_versions
        FROM papers_enriched
        WHERE abstract_len IS NOT NULL
    """)

    corr_df = spark.sql("""
        SELECT CORR(abstract_len, n_versions) AS corr_abslen_versions
        FROM abs_pop
    """)

    csv_corr = outdir / "complex_abstractlen_versions_correlation.csv"
    pdf_corr = save_df_as_csv(corr_df, csv_corr)

    spark.sql("""
        CREATE OR REPLACE TEMP VIEW abs_with_bucket AS
        SELECT
          abstract_len,
          n_versions,
          NTILE(10) OVER (ORDER BY abstract_len) AS abslen_decile
        FROM abs_pop
    """)

    bucket_df = spark.sql("""
        SELECT
          abslen_decile,
          AVG(abstract_len) AS avg_abslen,
          AVG(n_versions) AS avg_versions
        FROM abs_with_bucket
        GROUP BY abslen_decile
        ORDER BY abslen_decile
    """)

    csv_bucket = outdir / "complex_abstractlen_versions_by_decile.csv"
    pdf_bucket = save_df_as_csv(bucket_df, csv_bucket)

    if not pdf_corr.empty:
        val = float(pdf_corr["corr_abslen_versions"].iloc[0])
        single_value_figure(
            round(val, 4),
            outdir / "complex_abstractlen_versions_correlation.png",
            "Pearson correlation",
            label="corr(abstract_len, n_versions)",
        )

    if not pdf_bucket.empty:
        line_xy(
            pdf_bucket,
            "abslen_decile",
            "avg_versions",
            outdir / "complex_abstractlen_versions_by_decile.png",
            "Avg # versions by abstract length decile",
            xlabel="abstract length decile (1=short)",
            ylabel="avg # versions",
        )


def complex_9_weekday_submission_patterns(
    spark: SparkSession, outdir: Path
) -> None:
    """
    Weekday vs weekend submission patterns.
    """
    print("\n[complex-9] Weekday submission patterns")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "submitted_date" not in cols:
        print("[skip] submitted_date missing")
        return

    df = spark.sql("""
        SELECT
          DATE_FORMAT(submitted_date, 'E') AS weekday_short,
          DATE_FORMAT(submitted_date, 'u') AS weekday_num,
          COUNT(*) AS submissions
        FROM papers_enriched
        GROUP BY DATE_FORMAT(submitted_date, 'E'), DATE_FORMAT(submitted_date, 'u')
        ORDER BY CAST(weekday_num AS INT)
    """)

    csv = outdir / "complex_weekday_submissions.csv"
    pdf = save_df_as_csv(df, csv)

    if pdf.empty:
        return

    pdf_plot = weekday_sort(pdf.copy(), "weekday_short", "weekday_num")
    line_xy(
        pdf_plot,
        "weekday_short",
        "submissions",
        outdir / "complex_weekday_submissions.png",
        "Submissions by weekday",
        xlabel="weekday",
        ylabel="submissions",
    )


def complex_10_category_stability_versions(
    spark: SparkSession, outdir: Path
) -> None:
    """
    Category stability via versions:
    average # versions per primary_category.
    """
    print("\n[complex-10] Category stability via versions")

    cols = [c.name for c in spark.table("papers_enriched").schema]
    if "primary_category" not in cols:
        print("[skip] primary_category missing")
        return

    df = spark.sql("""
        SELECT
          primary_category,
          AVG(n_versions) AS avg_versions,
          COUNT(*) AS n_papers
        FROM papers_enriched
        GROUP BY primary_category
        HAVING n_papers >= 20
        ORDER BY avg_versions DESC, n_papers DESC
    """)

    csv = outdir / "complex_category_versions_avg.csv"
    pdf = save_df_as_csv(df, csv)

    if pdf.empty:
        return

    barh_topn(
        pdf,
        "primary_category",
        "avg_versions",
        outdir / "complex_category_versions_avg_top30.png",
        "Avg # versions by primary category (n_papers ≥ 20)",
        topn=30,
    )


# ----------------------------------------------------------------------
# Public API
# ----------------------------------------------------------------------


def run_complex_queries(parquet_path: str, outdir: str | Path) -> None:
    """
    Entry point used by pipelines:

    - Load parquet (already ingested arXiv data).
    - Register `papers` temp view.
    - Build `papers_enriched`.
    - Run all 10 complex analyses.
    - Save CSV + PNG into outdir.
    """
    outdir_path = _ensure_outdir(outdir)

    spark = (
        SparkSession.builder.appName("arxiv_complex_engine")
        .config("spark.sql.shuffle.partitions", "200")
        .getOrCreate()
    )

    print(f"[load] reading parquet from {parquet_path}")
    df = spark.read.parquet(parquet_path)
    df.createOrReplaceTempView("papers")

    ensure_aux_columns(spark)

    complex_1_category_cooccurrence(spark, outdir_path)
    complex_2_author_collab_over_time(spark, outdir_path)
    complex_3_rising_declining_topics(spark, outdir_path)
    complex_4_readability_lexical_trends(spark, outdir_path)
    complex_5_doi_versions_correlation(spark, outdir_path)
    complex_6_author_productivity_lifecycle(spark, outdir_path)
    complex_7_author_category_migration(spark, outdir_path)
    complex_8_abstract_len_vs_popularity(spark, outdir_path)
    complex_9_weekday_submission_patterns(spark, outdir_path)
    complex_10_category_stability_versions(spark, outdir_path)

    print(f"\n[done] Complex analytics written to {outdir_path}/")
    spark.stop()


# ----------------------------------------------------------------------
# CLI
# ----------------------------------------------------------------------


def _parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--parquet",
        required=True,
        help="Path to ingested Parquet (e.g. data/processed/arxiv_full)",
    )
    ap.add_argument(
        "--outdir",
        required=True,
        help="Output directory for analysis CSVs + PNGs (e.g. reports/analysis_full)",
    )
    return ap.parse_args()


def main() -> None:
    args = _parse_args()
    run_complex_queries(args.parquet, args.outdir)


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/engine/ml/model_loader.py
================================================================================
from pathlib import Path
from typing import Tuple

from pyspark.ml import PipelineModel
from pyspark.sql import SparkSession, DataFrame


def _paths_for_mode(mode: str) -> Tuple[Path, Path]:
    if mode not in {"full", "sample"}:
        raise ValueError("mode must be 'full' or 'sample'")
    if mode == "full":
        model_dir = Path("data/models/tfidf_full")
        feats = Path("data/processed/features_full")
    else:
        model_dir = Path("data/models/tfidf_sample")
        feats = Path("data/processed/features_sample")
    return model_dir, feats


def load_model_and_features(
    spark: SparkSession, mode: str
) -> Tuple[PipelineModel, DataFrame]:
    model_dir, feats_dir = _paths_for_mode(mode)
    model = PipelineModel.load(str(model_dir))
    feats = spark.read.parquet(str(feats_dir))
    return model, feats


/home/data/akhalegh/utils/ccda-course-project_v2/engine/ml/train.py
================================================================================
from __future__ import annotations

import argparse
import time
from pathlib import Path

from pyspark.sql import functions as F

from engine.utils.spark_utils import get_spark
from engine.utils.io_utils import write_json
from engine.ml.featurization import build_text_pipeline, compute_extra_stopwords


def ensure_id_base(df):
    if "id_base" in df.columns:
        return df
    if "arxiv_id" in df.columns:
        return df.withColumn(
            "id_base",
            F.regexp_replace(F.col("arxiv_id").cast("string"), r"v\d+$", ""),
        )
    if "id" in df.columns:
        return df.withColumn(
            "id_base", F.regexp_replace(F.col("id").cast("string"), r"v\d+$", "")
        )
    raise ValueError("Expected one of 'id_base', 'arxiv_id', or 'id' in dataset")


def normalize_schema_for_text(df):
    dtypes = dict(df.dtypes)

    if "categories" in dtypes and dtypes["categories"] == "string":
        df = df.withColumn("categories", F.split(F.col("categories"), r"\s+"))
    elif "categories" not in dtypes:
        df = df.withColumn("categories", F.array().cast("array<string>"))

    df = (
        df.withColumn("title", F.coalesce(F.col("title").cast("string"), F.lit("")))
        .withColumn("abstract", F.coalesce(F.col("abstract").cast("string"), F.lit("")))
    )

    if "arxiv_id" in df.columns:
        df = df.withColumn("paper_id", F.col("arxiv_id").cast("string"))
    elif "id" in df.columns:
        df = df.withColumn("paper_id", F.col("id").cast("string"))
    else:
        df = df.withColumn("paper_id", F.col("id_base"))

    df = df.withColumn(
        "text",
        F.concat_ws(
            " ", F.lower(F.col("title")).alias(""), F.lower(F.col("abstract")).alias("")
        ),
    )
    return df


def train_model(
    input_parquet: str,
    model_dir: str,
    features_out: str,
    vocab_size: int = 250_000,
    min_df: int = 5,
    use_bigrams: bool = False,
    extra_stopwords_topdf: int = 500,
    seed: int = 42,
    app_name: str = "tfidf_train",
):
    spark = get_spark(app_name)
    df = spark.read.parquet(input_parquet)

    df = ensure_id_base(df)
    df = normalize_schema_for_text(df)

    extra_sw = compute_extra_stopwords(
        spark, df.select("id_base", "abstract"), top_df=extra_stopwords_topdf, seed=seed
    )

    pipeline = build_text_pipeline(
        vocab_size=vocab_size,
        min_df=min_df,
        use_bigrams=use_bigrams,
        extra_stopwords=extra_sw,
    )
    model = pipeline.fit(df)

    Path(model_dir).mkdir(parents=True, exist_ok=True)
    model.write().overwrite().save(model_dir)

    meta = {
        "created_at": int(time.time()),
        "vocab_size": vocab_size,
        "min_df": min_df,
        "use_bigrams": use_bigrams,
        "extra_stopwords_topdf": extra_stopwords_topdf,
        "seed": seed,
        "spark_version": spark.version,
    }
    write_json(meta, Path(model_dir) / "model.json")

    feats = (
        model.transform(df)
        .select(
            "id_base",
            "paper_id",
            "title",
            "abstract",
            "categories",
            "year",
            F.col("features_norm").alias("features"),
        )
    )

    (
        feats.repartition(256)
        .write.mode("overwrite")
        .parquet(features_out)
    )

    print(f"[train] model saved to: {model_dir}")
    print(f"[train] features saved to: {features_out}")

    spark.stop()


def _parse_args():
    ap = argparse.ArgumentParser(description="Train TF-IDF model (no split).")
    ap.add_argument("--input-parquet", required=True)
    ap.add_argument("--model-dir", required=True)
    ap.add_argument("--features-out", required=True)
    ap.add_argument("--vocab-size", type=int, default=250000)
    ap.add_argument("--min-df", type=int, default=5)
    ap.add_argument(
        "--use-bigrams", type=lambda x: str(x).lower() == "true", default=False
    )
    ap.add_argument("--extra-stopwords-topdf", type=int, default=500)
    ap.add_argument("--seed", type=int, default=42)
    return ap.parse_args()


def main():
    args = _parse_args()
    train_model(
        input_parquet=args.input_parquet,
        model_dir=args.model_dir,
        features_out=args.features_out,
        vocab_size=args.vocab_size,
        min_df=args.min_df,
        use_bigrams=args.use_bigrams,
        extra_stopwords_topdf=args.extra_stopwords_topdf,
        seed=args.seed,
    )


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/engine/ml/__init__.py
================================================================================
# ccda-course-project/engine/ml/__init__.py


/home/data/akhalegh/utils/ccda-course-project_v2/engine/ml/featurization.py
================================================================================
from typing import List, Iterable

from pyspark.sql import DataFrame, SparkSession, functions as F
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    RegexTokenizer,
    StopWordsRemover,
    CountVectorizer,
    IDF,
    NGram,
    Normalizer,
)

DEFAULT_DOMAIN_STOPS = [
    "arxiv",
    "paper",
    "result",
    "results",
    "method",
    "methods",
    "propose",
    "proposed",
    "show",
    "shows",
    "using",
    "based",
]


def compute_extra_stopwords(
    spark: SparkSession, train_df: DataFrame, top_df: int = 200, seed: int = 42
) -> List[str]:
    """
    Return the top 'top_df' tokens by document frequency on TRAIN.
    Deterministic: counts distinct id_base per token.
    """
    if "id_base" not in train_df.columns:
        if "id" in train_df.columns:
            train_df = train_df.withColumn(
                "id_base", F.regexp_replace(F.col("id"), r"v\d+$", "")
            )
        else:
            raise ValueError(
                "compute_extra_stopwords requires 'id_base' or 'id' in train_df"
            )

    tok = RegexTokenizer(
        inputCol="abstract",
        outputCol="tmp_tokens",
        pattern=r"[^\p{L}]+",
        gaps=True,
        toLowercase=True,
    )
    toks = tok.transform(train_df.select("id_base", "abstract"))

    exploded = (
        toks.select("id_base", F.explode_outer("tmp_tokens").alias("tok"))
        .where(F.length("tok") > 1)
    )

    doc_tok = exploded.dropDuplicates(["id_base", "tok"])

    dfreq = doc_tok.groupBy("tok").agg(F.countDistinct("id_base").alias("df"))
    top = dfreq.orderBy(F.desc("df")).limit(top_df)

    return [r["tok"] for r in top.collect()]


def build_text_pipeline(
    vocab_size: int = 80000,
    min_df: int = 3,
    use_bigrams: bool = False,
    extra_stopwords: Iterable[str] = (),
) -> Pipeline:
    """
    Pipeline:
      RegexTokenizer -> StopWordsRemover(default + extra + domain)
      -> (optional NGram(2) + concat) -> CountVectorizer -> IDF -> Normalizer
    Expects an input column 'text' and outputs 'features_norm'.
    """
    tokenizer = RegexTokenizer(
        inputCol="text",
        outputCol="tokens",
        pattern=r"[^\p{L}]+",
        gaps=True,
        toLowercase=True,
    )

    remover = StopWordsRemover(
        inputCol="tokens",
        outputCol="tokens_sw_removed",
        stopWords=sorted(
            set(
                StopWordsRemover.loadDefaultStopWords("english")
                + list(DEFAULT_DOMAIN_STOPS)
                + list(extra_stopwords)
            )
        ),
    )

    stages: list = [tokenizer, remover]
    final_tokens_col = "tokens_sw_removed"

    if use_bigrams:
        bigr = NGram(n=2, inputCol=final_tokens_col, outputCol="tokens_bi")
        stages.append(bigr)
        stages.append(
            _ConcatArrays(
                inputCol1=final_tokens_col, inputCol2="tokens_bi", outputCol="tokens_all"
            )
        )
        final_tokens_col = "tokens_all"

    vectorizer = CountVectorizer(
        inputCol=final_tokens_col,
        outputCol="tf",
        vocabSize=vocab_size,
        minDF=min_df,
    )
    idf = IDF(inputCol="tf", outputCol="features_tfidf")
    norm = Normalizer(inputCol="features_tfidf", outputCol="features_norm", p=2.0)

    return Pipeline(stages=stages + [vectorizer, idf, norm])


from pyspark.ml import Transformer  # noqa: E402
from pyspark.sql.types import ArrayType, StringType  # noqa: F401, E402


class _ConcatArrays(Transformer):
    def __init__(self, inputCol1: str, inputCol2: str, outputCol: str):
        super().__init__()
        self.inputCol1 = inputCol1
        self.inputCol2 = inputCol2
        self.outputCol = outputCol

    def _transform(self, dataset: DataFrame) -> DataFrame:
        return dataset.withColumn(
            self.outputCol, F.concat(F.col(self.inputCol1), F.col(self.inputCol2))
        )


/home/data/akhalegh/utils/ccda-course-project_v2/pipelines/train_full.py
================================================================================
from engine.ml.train import train_model


def main():
    train_model(
        input_parquet="data/processed/arxiv_full",
        model_dir="data/models/tfidf_full",
        features_out="data/processed/features_full",
        vocab_size=250000,
        min_df=5,
        use_bigrams=False,
        extra_stopwords_topdf=500,
    )


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/pipelines/train_sample.py
================================================================================
from engine.ml.train import train_model


def main():
    train_model(
        input_parquet="data/processed/arxiv_sample",
        model_dir="data/models/tfidf_sample",
        features_out="data/processed/features_sample",
        vocab_size=80000,
        min_df=3,
        use_bigrams=False,
        extra_stopwords_topdf=200,
    )


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/pipelines/ingest_full.py
================================================================================
from engine.data.ingestion import run_ingestion


def main():
    run_ingestion(
        input_path="data/raw/arxiv-metadata-oai-snapshot.json",
        output_path="data/processed/arxiv_full",
        partition_by="year",
        min_abstract_len=40,
        repartition=200,
    )


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/pipelines/complex_sample.py
================================================================================
#!/usr/bin/env python3
"""
Run complex queries on the SAMPLE ingested dataset.

Default paths (can be overridden via CLI):
  parquet : data/processed/arxiv_sample
  outdir  : reports/analysis_sample
"""

from __future__ import annotations

import argparse

from engine.complex.complex_queries import run_complex_queries


DEFAULT_PARQUET = "data/processed/arxiv_sample"
DEFAULT_OUTDIR = "reports/analysis_sample"


def _parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--parquet",
        default=DEFAULT_PARQUET,
        help=f"Path to SAMPLE parquet (default: {DEFAULT_PARQUET})",
    )
    ap.add_argument(
        "--outdir",
        default=DEFAULT_OUTDIR,
        help=f"Output dir for SAMPLE complex analytics (default: {DEFAULT_OUTDIR})",
    )
    return ap.parse_args()


def main() -> None:
    args = _parse_args()
    run_complex_queries(args.parquet, args.outdir)


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/pipelines/complex_full.py
================================================================================
#!/usr/bin/env python3
"""
Run complex queries on the FULL ingested dataset.

Default paths (can be overridden via CLI):
  parquet : data/processed/arxiv_full
  outdir  : reports/analysis_full
"""

from __future__ import annotations

import argparse

from engine.complex.complex_queries import run_complex_queries


DEFAULT_PARQUET = "data/processed/arxiv_full"
DEFAULT_OUTDIR = "reports/analysis_full"


def _parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--parquet",
        default=DEFAULT_PARQUET,
        help=f"Path to FULL parquet (default: {DEFAULT_PARQUET})",
    )
    ap.add_argument(
        "--outdir",
        default=DEFAULT_OUTDIR,
        help=f"Output dir for FULL complex analytics (default: {DEFAULT_OUTDIR})",
    )
    return ap.parse_args()


def main() -> None:
    args = _parse_args()
    run_complex_queries(args.parquet, args.outdir)


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/pipelines/ingest_sample.py
================================================================================
from engine.data.ingestion import run_ingestion


def main():
    run_ingestion(
        input_path="data/sample/arxiv-sample.jsonl",
        output_path="data/processed/arxiv_sample",
        partition_by="year",
        min_abstract_len=40,
        repartition=64,
    )


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/streaming/merge_diff.py
================================================================================
from __future__ import annotations

from pathlib import Path

from pyspark.sql import functions as F

from engine.utils.spark_utils import get_spark


def diff_new_papers(old_parquet: str, new_parquet: str, out_parquet: str):
    """
    Simple helper: given old and new full Parquets, write only *new* arxiv_id rows.
    """
    spark = get_spark("merge_diff")
    old = spark.read.parquet(old_parquet).select("arxiv_id").distinct()
    new = spark.read.parquet(new_parquet)

    only_new = new.join(old, on="arxiv_id", how="left_anti")
    Path(out_parquet).parent.mkdir(parents=True, exist_ok=True)
    only_new.write.mode("overwrite").parquet(out_parquet)
    spark.stop()


/home/data/akhalegh/utils/ccda-course-project_v2/streaming/kaggle_downloader.py
================================================================================
"""
Download Cornell arXiv snapshot via KaggleHub.

Modes:
  - full   → data/raw/arxiv-metadata-oai-snapshot.json
  - sample → data/sample/arxiv-sample.jsonl (first N lines)
"""

from __future__ import annotations

import argparse
import shutil
from pathlib import Path

import kagglehub


def is_likely_jsonl(path: Path, probe_lines: int = 5) -> bool:
    try:
        with path.open("r", encoding="utf-8") as f:
            for _ in range(probe_lines):
                line = f.readline()
                if not line:
                    break
                s = line.strip()
                if not s:
                    continue
                if not s.startswith("{"):
                    return False
        return True
    except Exception:  # noqa: BLE001
        return False


def write_head_jsonl(src: Path, dst: Path, n: int) -> int:
    dst.parent.mkdir(parents=True, exist_ok=True)
    count = 0
    with src.open("r", encoding="utf-8") as fin, dst.open(
        "w", encoding="utf-8"
    ) as fout:
        for line in fin:
            if not line.strip():
                continue
            fout.write(line)
            count += 1
            if count >= n:
                break
    return count


def download_full_raw() -> Path:
    print("[KaggleHub] Downloading Cornell-University/arxiv ...")
    path = kagglehub.dataset_download("Cornell-University/arxiv")
    base = Path(path)
    candidates = list(base.rglob("arxiv-metadata-oai-snapshot.json")) + list(
        base.rglob("arxiv-metadata-oai-snapshot.jsonl")
    )
    if not candidates:
        raise FileNotFoundError(
            "Could not find arxiv-metadata-oai-snapshot.json(.jsonl) in Kaggle download."
        )
    src = candidates[0]
    print(f"[Found] {src}")
    proj_raw = Path("data/raw")
    proj_raw.mkdir(parents=True, exist_ok=True)
    target = proj_raw / "arxiv-metadata-oai-snapshot.json"
    if target.resolve() != src.resolve():
        print(f"[Copy] -> {target}")
        shutil.copyfile(src, target)
    return target


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--mode",
        choices=["full", "sample"],
        required=True,
        help="Download mode: full raw snapshot or only a JSONL sample.",
    )
    ap.add_argument(
        "--sample-size",
        type=int,
        default=30000,
        help="Lines to write into data/sample/arxiv-sample.jsonl (sample mode).",
    )
    args = ap.parse_args()

    target = download_full_raw()

    if args.mode == "sample":
        if args.sample_size <= 0:
            print("[sample] --sample-size <= 0, skipping sample generation.")
            return
        sample_dir = Path("data/sample")
        sample_dir.mkdir(parents=True, exist_ok=True)
        sample_path = sample_dir / "arxiv-sample.jsonl"
        if is_likely_jsonl(target):
            n = write_head_jsonl(target, sample_path, args.sample_size)
            print(f"[Sample] Wrote first {n} JSONL lines to {sample_path}")
        else:
            n = write_head_jsonl(target, sample_path, args.sample_size)
            print(f"[Sample] (non-JSONL heuristic) Wrote {n} lines to {sample_path}")
    else:
        print("[full] Raw file ready at data/raw/arxiv-metadata-oai-snapshot.json")


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/streaming/sample_prepare_batches.py
================================================================================
from __future__ import annotations

import argparse
import os
import time
from datetime import datetime, timedelta
from pathlib import Path

SOURCE = Path("data/sample/arxiv-sample.jsonl")
OUTDIR = Path("data/stream/incoming_sample")
SIZES = [10_000, 20_000, 30_000, 40_000, 50_000]


def parse_date(s: str) -> datetime:
    return datetime.strptime(s, "%Y-%m-%d")


def write_prefix_atomic(
    src: Path, dst: Path, n_lines: int, overwrite: bool = False
) -> int:
    """
    Write first n_lines from src (JSONL) to dst (.jsonl) via a temp file,
    then atomically rename so Spark treats the arrival correctly.
    """
    if dst.exists() and not overwrite:
        print(f"[skip] exists: {dst}")
        return -1

    dst.parent.mkdir(parents=True, exist_ok=True)
    tmp = dst.with_suffix(dst.suffix + ".tmp")

    count = 0
    with src.open("r", encoding="utf-8") as fin, tmp.open(
        "w", encoding="utf-8"
    ) as fout:
        for line in fin:
            if not line.strip():
                continue
            fout.write(line)
            count += 1
            if count >= n_lines:
                break

    os.replace(tmp, dst)
    print(f"[write] {dst}  lines={count}")
    return count


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--start-date",
        type=str,
        default=datetime.now().strftime("%Y-%m-%d"),
        help="First weekly date (YYYY-MM-DD) used in filenames (default: today)",
    )
    ap.add_argument(
        "--interval-seconds",
        type=int,
        default=60,
        help="Seconds to wait between drops (default: 60)",
    )
    ap.add_argument(
        "--no-sleep",
        action="store_true",
        help="Emit all files immediately without waiting between drops",
    )
    ap.add_argument(
        "--overwrite",
        action="store_true",
        help="Overwrite target files if they already exist",
    )
    args = ap.parse_args()

    if not SOURCE.exists():
        raise FileNotFoundError(
            f"Missing {SOURCE}. Generate it with streaming.kaggle_downloader first."
        )

    OUTDIR.mkdir(parents=True, exist_ok=True)
    start_dt = parse_date(args.start_date)

    for i, size in enumerate(SIZES):
        drop_dt = start_dt + timedelta(weeks=i)
        stamp = drop_dt.strftime("%Y%m%d")
        target = OUTDIR / f"arxiv-sample-{stamp}.jsonl"

        _ = write_prefix_atomic(SOURCE, target, size, overwrite=args.overwrite)

        if i < len(SIZES) - 1 and not args.no_sleep:
            sleep_s = max(args.interval_seconds, 1)
            print(f"[wait] sleeping {sleep_s}s before next drop...")
            time.sleep(sleep_s)

    print(f"[done] Created {len(SIZES)} weekly-dated drops under {OUTDIR}/")


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/streaming/full_stream.py
================================================================================
#!/usr/bin/env python3
"""
Full-dataset streaming job (weekly Kaggle snapshots).

Behavior
--------
- Watches a directory for new arxiv-YYYYMMDD.json(.jsonl) files (weekly snapshots).
- For each new file (micro-batch), applies the same transforms as the batch pipeline
  and writes per-drop reports (CSV + PNG) into:

    reports/streaming_full/YYYYMMDD/

- Designed to be run either continuously or once:

  Continuous mode (default):
      python streaming/full_stream.py

  Trigger-once mode (process all currently-available files then exit):
      python streaming/full_stream.py --once

Typical usage
-------------
1. Use `streaming/kaggle_downloader.py` (or equivalent) to download a new snapshot
   and place it under `data/stream/incoming/` as:

     arxiv-YYYYMMDD.json   # or .jsonl

2. Run this streaming job in the background, or periodically with `--once`.
"""

from __future__ import annotations

import argparse
import re
from pathlib import Path

import pandas as pd
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

from pyspark.sql import functions as F, types as T

from engine.utils.spark_utils import get_spark
from engine.data.transformations import transform_all


# -------------------------------------------------------------------
# Paths / constants
# -------------------------------------------------------------------

INCOMING = "data/stream/incoming"
REPORTS_ROOT = Path("reports/streaming_full")
CHECKPOINT_DEFAULT = "data/stream/checkpoints_full"

# Match either:
#   arxiv-YYYYMMDD.json
#   arxiv-YYYYMMDD.jsonl
FILE_DATE_REGEX = re.compile(r".*arxiv-(\d{8})\.jsonl?$")

# Explicit JSON schema so the stream can start immediately without inference
JSON_SCHEMA = T.StructType([
    T.StructField("id", T.StringType()),
    T.StructField("title", T.StringType()),
    T.StructField("abstract", T.StringType()),
    T.StructField("categories", T.StringType()),
    T.StructField("doi", T.StringType()),
    T.StructField("journal-ref", T.StringType()),
    T.StructField("comments", T.StringType()),
    T.StructField("submitter", T.StringType()),
    T.StructField("update_date", T.StringType()),
    T.StructField("submitted_date", T.StringType()),
    T.StructField("authors", T.StringType()),
    T.StructField("authors_parsed", T.ArrayType(T.ArrayType(T.StringType()))),
    T.StructField("versions", T.ArrayType(T.MapType(T.StringType(), T.StringType()))),
])


# -------------------------------------------------------------------
# Small I/O + plotting helpers
# -------------------------------------------------------------------

def _save_df_as_csv(df_spark, path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    df_spark.toPandas().to_csv(path, index=False)
    print(f"[saved] {path}")


def _plot_line(x, y, title, xlabel, ylabel, outpng: Path) -> None:
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.plot(x, y, marker="o")
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")


def _plot_bar(labels, values, title, xlabel, ylabel, outpng: Path, rotate_xticks: bool = False) -> None:
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.bar(labels, values)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if rotate_xticks:
        plt.xticks(rotation=60, ha="right")
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")


# -------------------------------------------------------------------
# Per-batch handler
# -------------------------------------------------------------------

def _per_drop_reports(batch_df, batch_id: int) -> None:
    """
    foreachBatch handler for Structured Streaming.

    batch_df: Spark DataFrame containing only *new* rows for this micro-batch.
    We:
      - derive a `source_date` per row from input_file_name() (YYYYMMDD),
      - group by date,
      - apply `transform_all`,
      - and emit CSV + PNG reports under:

            reports/streaming_full/YYYYMMDD/
    """
    if batch_df.rdd.isEmpty():
        print(f"[batch {batch_id}] empty batch")
        return

    # Attach source filename + date stamp
    df_with_name = batch_df.withColumn("source_file", F.input_file_name())
    df_with_date = df_with_name.withColumn(
        "source_date",
        F.regexp_extract(
            F.col("source_file"),
            r"arxiv-(\d{8})\.jsonl?$",
            1,
        ),
    )

    dates = [r["source_date"] for r in df_with_date.select("source_date").distinct().collect()]
    if not dates:
        print(f"[batch {batch_id}] no matching arxiv-YYYYMMDD files in this batch")
        return

    for d in dates:
        if not d:
            continue

        print(f"[batch {batch_id}] generating reports for date={d}")
        sub = (
            df_with_date
            .filter(F.col("source_date") == F.lit(d))
            .drop("source_file", "source_date")
        )

        # Apply the shared Week-8-style transforms
        transformed = transform_all(sub)

        outdir = REPORTS_ROOT / d
        outdir.mkdir(parents=True, exist_ok=True)

        # 1) Papers per year
        by_year = transformed.groupBy("year").count().orderBy("year")
        _save_df_as_csv(by_year, outdir / "by_year.csv")
        py = by_year.toPandas().dropna()
        if not py.empty:
            _plot_line(
                py["year"],
                py["count"],
                "Papers per Year",
                "year",
                "count",
                outdir / "papers_per_year.png",
            )

        # 2) Top categories
        topcats = (
            transformed.groupBy("primary_category")
                       .count()
                       .orderBy(F.desc("count"))
                       .limit(30)
        )
        _save_df_as_csv(topcats, outdir / "top_categories.csv")
        pc = topcats.toPandas()
        if not pc.empty:
            _plot_bar(
                pc["primary_category"],
                pc["count"],
                "Top Primary Categories",
                "primary_category",
                "count",
                outdir / "top_categories.png",
                rotate_xticks=True,
            )

        # 3) DOI rate by year
        if "has_doi" in transformed.columns:
            doi_year = (
                transformed.groupBy("year")
                           .agg(F.avg(F.col("has_doi").cast("double")).alias("doi_rate"))
                           .orderBy("year")
            )
            _save_df_as_csv(doi_year, outdir / "doi_rate_by_year.csv")
            pd_doi = doi_year.toPandas().dropna()
            if not pd_doi.empty:
                _plot_line(
                    pd_doi["year"],
                    pd_doi["doi_rate"] * 100.0,
                    "DOI Coverage by Year (%)",
                    "year",
                    "doi rate (%)",
                    outdir / "doi_rate_by_year.png",
                )

        print(f"[batch {batch_id}] reports → {outdir}/")


# -------------------------------------------------------------------
# Main entrypoint
# -------------------------------------------------------------------

def main() -> None:
    ap = argparse.ArgumentParser()
    ap.add_argument(
        "--input",
        default=INCOMING,
        help="Directory to watch for full arXiv drops (default: data/stream/incoming)",
    )
    ap.add_argument(
        "--checkpoint",
        default=CHECKPOINT_DEFAULT,
        help="Checkpoint directory for Structured Streaming state",
    )
    ap.add_argument(
        "--trigger-seconds",
        type=int,
        default=60,
        help="Micro-batch frequency in seconds (ignored if --once is set).",
    )
    ap.add_argument(
        "--max-files-per-trigger",
        type=int,
        default=1,
        help="Maximum number of new files to pick up per micro-batch.",
    )
    ap.add_argument(
        "--once",
        action="store_true",
        help=(
            "Use Trigger.Once: process all available files, write reports, "
            "then exit when caught up."
        ),
    )
    args = ap.parse_args()

    # Ensure directories exist
    REPORTS_ROOT.mkdir(parents=True, exist_ok=True)
    Path(args.input).mkdir(parents=True, exist_ok=True)
    Path(args.checkpoint).mkdir(parents=True, exist_ok=True)

    # Spark session (tuned via shared util)
    spark = get_spark("streaming_full_arxiv")
    spark.conf.set("spark.sql.streaming.schemaInference", "true")

    # Structured stream over incoming arxiv-YYYYMMDD JSON/JSONL files
    stream_df = (
        spark.readStream
             .format("json")
             .schema(JSON_SCHEMA)
             .option("multiLine", "false")
             .option("maxFilesPerTrigger", str(max(args.max_files_per_trigger, 1)))
             .option("pathGlobFilter", "arxiv-*.json*")
             .load(args.input)
    )

    writer = (
        stream_df.writeStream
                 .foreachBatch(_per_drop_reports)
                 .option("checkpointLocation", args.checkpoint)
    )

    if args.once:
        # Trigger.Once: process all currently available data and then stop.
        q = writer.trigger(once=True).start()
        print("[stream] Trigger.Once mode: will process all available files, then stop.")
    else:
        # Continuous micro-batches
        interval = max(args.trigger_seconds, 1)
        q = writer.trigger(processingTime=f"{interval} seconds").start()
        print(f"[stream] Watching {args.input} every {interval} second(s). Ctrl+C to stop.")

    try:
        q.awaitTermination()
    except KeyboardInterrupt:
        print("\n[stop] Stopping full stream...")
        q.stop()
        spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/streaming/sample_stream.py
================================================================================
#!/usr/bin/env python3
from __future__ import annotations

import argparse
import re
from pathlib import Path

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt  # noqa: E402
import pandas as pd  # noqa: E402
from pyspark.sql import functions as F, types as T  # noqa: E402

from engine.utils.spark_utils import get_spark
from engine.data.transformations import transform_all

INCOMING = "data/stream/incoming_sample"
REPORTS_ROOT = Path("reports/streaming_sample")
CHECKPOINT = "data/stream/checkpoints_sample"

FILE_DATE_REGEX = re.compile(r".*arxiv-sample-(\d{8})(\d{4})?\.jsonl$")

JSON_SCHEMA = T.StructType(
    [
        T.StructField("id", T.StringType()),
        T.StructField("title", T.StringType()),
        T.StructField("abstract", T.StringType()),
        T.StructField("categories", T.StringType()),
        T.StructField("doi", T.StringType()),
        T.StructField("journal-ref", T.StringType()),
        T.StructField("comments", T.StringType()),
        T.StructField("submitter", T.StringType()),
        T.StructField("update_date", T.StringType()),
        T.StructField("submitted_date", T.StringType()),
        T.StructField("authors", T.StringType()),
        T.StructField("authors_parsed", T.ArrayType(T.ArrayType(T.StringType()))),
        T.StructField(
            "versions", T.ArrayType(T.MapType(T.StringType(), T.StringType()))
        ),
    ]
)


def _save_df_as_csv(df_spark, path: Path):
    path.parent.mkdir(parents=True, exist_ok=True)
    df_spark.toPandas().to_csv(path, index=False)
    print(f"[saved] {path}")


def _plot_line(x, y, title, xlabel, ylabel, outpng: Path):
    fig, ax = plt.subplots(figsize=(9, 4))
    ax.plot(x, y, marker="o")
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")


def _plot_bar(labels, values, title, xlabel, ylabel, outpng: Path, rotate_xticks=False):
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.bar(labels, values)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if rotate_xticks:
        plt.xticks(rotation=60, ha="right")
    plt.tight_layout()
    outpng.parent.mkdir(parents=True, exist_ok=True)
    fig.savefig(outpng, dpi=150, bbox_inches="tight")
    plt.close(fig)
    print(f"[plot] {outpng}")


def _per_drop_reports(batch_df, batch_id: int):
    if batch_df.rdd.isEmpty():
        print(f"[batch {batch_id}] empty batch")
        return

    df_with_name = batch_df.withColumn("source_file", F.input_file_name())
    df_with_date = df_with_name.withColumn(
        "source_date",
        F.regexp_extract(
            F.col("source_file"), r"arxiv-sample-(\d{8})(\d{4})?\.jsonl$", 1
        ),
    )

    dates = [r["source_date"] for r in df_with_date.select("source_date").distinct().collect()]
    for d in dates:
        if not d:
            continue
        print(f"[batch {batch_id}] generating reports for date={d}")
        sub = df_with_date.filter(F.col("source_date") == F.lit(d)).drop(
            "source_file", "source_date"
        )

        transformed = transform_all(sub)

        outdir = REPORTS_ROOT / d
        outdir.mkdir(parents=True, exist_ok=True)

        by_year = transformed.groupBy("year").count().orderBy("year")
        _save_df_as_csv(by_year, outdir / "by_year.csv")
        py = by_year.toPandas().dropna()
        if not py.empty:
            _plot_line(
                py["year"],
                py["count"],
                "Papers per Year",
                "year",
                "count",
                outdir / "papers_per_year.png",
            )

        topcats = (
            transformed.groupBy("primary_category")
            .count()
            .orderBy(F.desc("count"))
            .limit(15)
        )
        _save_df_as_csv(topcats, outdir / "top_categories.csv")
        pc = topcats.toPandas()
        if not pc.empty:
            _plot_bar(
                pc["primary_category"],
                pc["count"],
                "Top Primary Categories",
                "primary_category",
                "count",
                outdir / "top_categories.png",
                rotate_xticks=True,
            )

        if "has_doi" in transformed.columns:
            doi_year = (
                transformed.groupBy("year")
                .agg(F.avg(F.col("has_doi").cast("double")).alias("doi_rate"))
                .orderBy("year")
            )
            _save_df_as_csv(doi_year, outdir / "doi_rate_by_year.csv")
            pd_doi = doi_year.toPandas().dropna()
            if not pd_doi.empty:
                _plot_line(
                    pd_doi["year"],
                    pd_doi["doi_rate"] * 100.0,
                    "DOI Coverage by Year (%)",
                    "year",
                    "doi rate (%)",
                    outdir / "doi_rate_by_year.png",
                )

        print(f"[batch {batch_id}] reports → {outdir}/")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", default=INCOMING)
    ap.add_argument("--checkpoint", default=CHECKPOINT)
    ap.add_argument("--trigger-seconds", type=int, default=10)
    ap.add_argument("--max-files-per-trigger", type=int, default=1)
    args = ap.parse_args()

    spark = get_spark("streaming_sample")
    spark.conf.set("spark.sql.streaming.schemaInference", "true")

    stream_df = (
        spark.readStream.format("json")
        .schema(JSON_SCHEMA)
        .option("multiLine", "false")
        .option("maxFilesPerTrigger", str(max(args.max_files_per_trigger, 1)))
        .option("pathGlobFilter", "arxiv-sample-*.jsonl")
        .load(args.input)
    )

    q = (
        stream_df.writeStream.foreachBatch(_per_drop_reports)
        .option("checkpointLocation", args.checkpoint)
        .trigger(processingTime=f"{max(args.trigger_seconds,1)} seconds")
        .start()
    )

    print(
        f"[listen] Watching {args.input} every {args.trigger_seconds} second(s). Ctrl+C to stop."
    )
    try:
        q.awaitTermination()
    except KeyboardInterrupt:
        print("\n[stop] Stopping stream...")
        q.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project_v2/app/server.py
================================================================================
# app/server.py
from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Any

import pandas as pd
from flask import (
    Flask,
    render_template,
    request,
    redirect,
    url_for,
    flash,
)

from pyspark.ml import PipelineModel
from pyspark.sql import DataFrame, functions as F

# Reuse your existing Spark helper + query utilities
from src.utils import get_spark
from src.query import query_topk


# ---------- Paths & config ----------

BASE_DIR = Path(__file__).resolve().parents[1]

DATASETS = {
    "sample": {
        "label": "Sample (≈50k papers)",
        "processed_parquet": BASE_DIR / "data" / "processed" / "arxiv_sample",
        "features_path": BASE_DIR / "data" / "processed" / "features_sample",
        "model_dir": BASE_DIR / "data" / "models" / "tfidf_sample",
        "analysis_dir": BASE_DIR / "reports" / "analysis_sample",
    },
    "full": {
        "label": "Full dataset",
        "processed_parquet": BASE_DIR / "data" / "processed" / "arxiv_full",
        "features_path": BASE_DIR / "data" / "processed" / "features_full",
        "model_dir": BASE_DIR / "data" / "models" / "tfidf_full",
        "analysis_dir": BASE_DIR / "reports" / "analysis_full",
    },
}

# Map “predefined complex query” IDs to CSV filenames + human labels
COMPLEX_QUERIES = {
    "rising_topics": {
        "label": "Rising topics (by category)",
        "filename": "complex_rising_topics_top20.csv",
        "description": "Top categories whose volume grew the most over the years.",
    },
    "declining_topics": {
        "label": "Declining topics (by category)",
        "filename": "complex_declining_topics_top20.csv",
        "description": "Categories whose volume dropped the most over the years.",
    },
    "category_versions": {
        "label": "Category stability (avg #versions)",
        "filename": "complex_category_versions_avg.csv",
        "description": "Average number of revisions per paper by primary category.",
    },
    "author_lifecycle": {
        "label": "Author lifecycle (span vs papers)",
        "filename": "complex_author_lifecycle_top.csv",
        "description": "Authors with long active spans and their paper counts.",
    },
    "author_migration": {
        "label": "Author category migration",
        "filename": "complex_author_category_migration.csv",
        "description": "Authors whose primary category changed over their career.",
    },
}


# ---------- Backend class per dataset ----------

@dataclass
class DatasetBackend:
    name: str
    label: str
    processed_parquet: Path
    features_path: Path
    model_dir: Path
    analysis_dir: Path

    # Lazily populated
    _spark: Any = None
    _model: Optional[PipelineModel] = None
    _features: Optional[DataFrame] = None
    _total_papers: Optional[int] = None

    def _get_spark(self):
        if self._spark is None:
            # Reuse your tuned Spark config
            self._spark = get_spark(app_name=f"arxiv_search_{self.name}")
        return self._spark

    @property
    def spark(self):
        return self._get_spark()

    @property
    def model(self) -> PipelineModel:
        if self._model is None:
            if not self.model_dir.exists():
                raise FileNotFoundError(f"Model dir not found: {self.model_dir}")
            self._model = PipelineModel.load(str(self.model_dir))
        return self._model

    @property
    def features(self) -> DataFrame:
        """
        Features parquet is expected to have:
          id_base, paper_id, title, abstract, categories, year, features
        (exactly what your training script wrote to data/processed/features_*).
        """
        if self._features is None:
            if not self.features_path.exists():
                raise FileNotFoundError(f"Features path not found: {self.features_path}")
            df = self.spark.read.parquet(str(self.features_path))
            # Small optimization: cache in memory
            self._features = df.cache()
            _ = self._features.count()  # materialize
        return self._features

    @property
    def total_papers(self) -> int:
        if self._total_papers is None:
            self._total_papers = self.features.count()
        return self._total_papers

    # ---- Search logic ----

    def search(
        self,
        title: str,
        abstract: str,
        k: int = 10,
        category_filter: Optional[str] = None,
        year_from: Optional[int] = None,
        year_to: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        Use existing TF-IDF model + query_topk to find similar papers.
        Applies optional category/year filters before scoring.
        """
        if not title and not abstract:
            return []

        k = max(1, min(k, 50))  # keep it sane

        feats = self.features

        # Apply filters on the candidate pool
        if category_filter:
            cat = category_filter.strip()
            if cat:
                if "categories" in feats.columns:
                    feats = feats.where(
                        F.array_contains(F.col("categories"), cat)
                        | F.col("categories").cast("string").contains(cat)
                    )

        if year_from is not None:
            feats = feats.where(F.col("year") >= int(year_from))
        if year_to is not None:
            feats = feats.where(F.col("year") <= int(year_to))

        # If filters wiped everything, bail early
        if feats.rdd.isEmpty():
            return []

        # Use your existing helper to compute top-K similarities
        recs = query_topk(
            self.spark,
            self.model,
            feats.select("id_base", "categories", "features"),
            query_title=title or "",
            query_abstract=abstract or "",
            k=k,
        )

        # Join back metadata for neighbors
        joined = (
            recs.join(
                feats.select(
                    "id_base",
                    "paper_id",
                    "title",
                    "abstract",
                    "categories",
                    "year",
                ),
                recs.neighbor_id == feats.id_base,
                "left",
            )
            .select(
                "rank",
                "score",
                "paper_id",
                "title",
                "abstract",
                "categories",
                "year",
            )
            .orderBy("rank")
        )

        rows = joined.limit(k).collect()
        results: List[Dict[str, Any]] = []
        for r in rows:
            cats = r.categories
            if isinstance(cats, list):
                cats_str = " ".join(cats)
            else:
                cats_str = str(cats) if cats is not None else ""

            abstract_short = (r.abstract or "")[:600]
            if len(r.abstract or "") > 600:
                abstract_short += " ..."

            results.append(
                {
                    "rank": int(r.rank),
                    "score": float(r.score) if r.score is not None else 0.0,
                    "paper_id": r.paper_id,
                    "title": r.title,
                    "abstract": abstract_short,
                    "categories": cats_str,
                    "year": int(r.year) if r.year is not None else None,
                }
            )
        return results

    # ---- Complex query results ----

    def load_complex_results(self, query_id: str) -> Dict[str, Any]:
        """
        Read a CSV produced by the complex query pipeline and return rows + metadata.
        """
        meta = COMPLEX_QUERIES.get(query_id)
        if meta is None:
            raise KeyError(f"Unknown complex query id: {query_id}")

        csv_path = self.analysis_dir / meta["filename"]
        if not csv_path.exists():
            raise FileNotFoundError(
                f"Complex query CSV not found for {self.label}: {csv_path}"
            )

        df = pd.read_csv(csv_path)
        rows = df.to_dict(orient="records")
        return {
            "id": query_id,
            "label": meta["label"],
            "description": meta["description"],
            "columns": list(df.columns),
            "rows": rows,
            "csv_path": str(csv_path),
        }


# Lazily instantiated dataset backends
_BACKENDS: Dict[str, DatasetBackend] = {}


def get_backend(dataset_key: str) -> DatasetBackend:
    if dataset_key not in DATASETS:
        raise KeyError(f"Unknown dataset key: {dataset_key}")
    if dataset_key not in _BACKENDS:
        cfg = DATASETS[dataset_key]
        _BACKENDS[dataset_key] = DatasetBackend(
            name=dataset_key,
            label=cfg["label"],
            processed_parquet=cfg["processed_parquet"]_]()_


/home/data/akhalegh/utils/ccda-course-project_v2/app/__init__.py
================================================================================
# app/__init__.py
from flask import Flask

def create_app():
    app = Flask(__name__)
    # If you want flash messages or secure cookies later, set a secret key:
    # app.config["SECRET_KEY"] = "change-me"
    from .server import register_routes
    register_routes(app)
    return app


/home/data/akhalegh/utils/ccda-course-project_v2/app/config.py
================================================================================
from dataclasses import dataclass


@dataclass
class Settings:
    default_mode: str = "sample"  # or "full"


settings = Settings()


/home/data/akhalegh/utils/ccda-course-project_v2/app/services/__init__.py
================================================================================
# ccda-course-project/app/services/__init__.py


/home/data/akhalegh/utils/ccda-course-project_v2/app/services/spark_session.py
================================================================================
from __future__ import annotations

from pyspark.sql import SparkSession

from engine.utils.spark_utils import get_spark

_SPARK: SparkSession | None = None


def get_spark_session() -> SparkSession:
    global _SPARK  # noqa: PLW0603
    if _SPARK is None:
        _SPARK = get_spark("ccda_app")
    return _SPARK


/home/data/akhalegh/utils/ccda-course-project_v2/app/services/complex_service.py
================================================================================
from __future__ import annotations

from pathlib import Path
from typing import List, Dict

import pandas as pd


def list_complex_reports(mode: str = "sample") -> List[str]:
    root = Path("reports/analysis_sample" if mode == "sample" else "reports/analysis_full")
    if not root.exists():
        return []
    return sorted(str(p) for p in root.glob("*.csv"))


def load_complex_report(path: str) -> Dict:
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(path)
    df = pd.read_csv(p)
    return {"path": str(p), "columns": list(df.columns), "rows": df.to_dict(orient="records")}


/home/data/akhalegh/utils/ccda-course-project_v2/app/services/filters_service.py
================================================================================
from __future__ import annotations

from typing import List

from pyspark.sql import functions as F

from app.services.spark_session import get_spark_session
from engine.ml.model_loader import load_model_and_features


def list_primary_categories(mode: str = "sample", top_k: int = 50) -> List[str]:
    spark = get_spark_session()
    _, feats = load_model_and_features(spark, mode)
    df = (
        feats.groupBy("categories")
        .count()
        .orderBy(F.desc("count"))
        .limit(top_k)
    )
    cats = set()
    for row in df.collect():
        if isinstance(row["categories"], list):
            for c in row["categories"]:
                cats.add(c)
    return sorted(cats)


/home/data/akhalegh/utils/ccda-course-project_v2/app/services/search_service.py
================================================================================
from __future__ import annotations

from typing import List, Dict, Any

from app.config import settings
from app.services.spark_session import get_spark_session
from engine.search.search_engine import SearchEngine

_ENGINE_CACHE: dict[str, SearchEngine] = {}


def _get_engine(mode: str | None = None) -> SearchEngine:
    m = mode or settings.default_mode
    if m not in _ENGINE_CACHE:
        _ENGINE_CACHE[m] = SearchEngine(mode=m, spark=get_spark_session())
    return _ENGINE_CACHE[m]


def search_papers(
    title: str = "",
    abstract: str = "",
    k: int = 10,
    mode: str | None = None,
) -> List[Dict[str, Any]]:
    engine = _get_engine(mode)
    return engine.search(title=title, abstract=abstract, k=k)


