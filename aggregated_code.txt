PROJECT FILE TREE
================================================================================
ccda-course-project/
    README.md
    Makefile
    run.sh
    requirements.txt
    notebooks/
        streaming_demo.ipynb
        eda_week8.py
        ingestion.ipynb
        ml_pipeline.ipynb
        sql_queries.ipynb
        eda.ipynb
    scripts/
        download_arxiv.py
    reports/
        full/
            category_year_matrix.csv
            doi_rate_by_year.csv
            text_length_summary.csv
            top_authors.csv
            category_pareto.csv
            top_categories.csv
            completeness.csv
            version_count_hist.csv
            distinct_selected.csv
            by_year.csv
        sample/
            category_year_matrix.csv
            doi_rate_by_year.csv
            text_length_summary.csv
            top_authors.csv
            category_pareto.csv
            top_categories.csv
            completeness.csv
            version_count_hist.csv
            distinct_selected.csv
            by_year.csv
    docs/
        methodology.md
        reproduction_guide.md
        dataset_overview.md
        limitations.md
        results.md
        slides/
            presentation.pptx
    src/
        __init__.py
        transformations.py
        utils.py
        ingestion.py
    tests/
        test_ingestion.py
        test_sql.py
        test_ml.py
        test_streaming.py


/home/data/akhalegh/utils/ccda-course-project/README.md
================================================================================
# ITCS 6190/8190 – Cloud Computing for Data Analysis
## Course Project: Data Analysis with Apache Spark (arXiv on Kaggle)

### Team Members
1. **Ali Khaleghi Rahimian** — akhalegh@charlotte.edu  
2. **Kiyoung Kim** — kkim43@charlotte.edu  
3. **Thejas Prabakaran** — tprabaka@charlotte.edu

---

## Overview
This project implements a **Spark-based data analysis pipeline** for the **Cornell arXiv metadata** dataset from Kaggle. We focus on:
- **Data ingestion** (JSON/JSONL → cleaned & partitioned Parquet)
- **Transformations** (text cleanup, field standardization, simple quality filters)
- **Exploratory Data Analysis (EDA)** (counts, trends, top categories/authors, DOI coverage, etc.)

We provide both a **sample workflow** (fast, 50k records for PRs/demos) and a **full workflow** (~1.7M records). All steps are designed to run in **GitHub Codespaces** or any local Spark environment.

---

## Dataset
- **Source**: Kaggle → *Cornell-University/arxiv*  
- **Contents**: Metadata of ~1.7M arXiv papers (id, title, abstract, categories, versions, authors, DOI, etc.)  
- **Format**: JSON Lines (one record per line)  
- **Size**: ~4.5 GB (metadata JSON), growing with updates

We use `kagglehub` to download the dataset and create a small **JSONL sample** for quick iteration.

> **Note**: We do **not** commit the full raw dataset to the repo. Only a small sample (if needed) is kept under `data/sample/` for testing/demo.

---

## Repository Structure
```
.
├─ scripts/
│  └─ download_arxiv.py        # Download full Kaggle dataset + write sample JSONL
├─ src/
│  ├─ ingestion.py             # Batch ingestion (JSON/JSONL → Parquet)
│  ├─ transformations.py       # Field cleanup & derived columns
│  └─ utils.py                 # Spark session + helper utilities
├─ notebooks/
│  └─ eda_week8.py             # Comprehensive EDA (writes CSVs + PNGs)
├─ data/
│  ├─ raw/                     # Full raw file(s) (ignored by Git)
│  ├─ sample/                  # Tiny JSONL sample for PR/demo
│  └─ processed/               # Parquet outputs (ignored by Git)
├─ reports/
│  ├─ sample/                  # EDA outputs for sample run (CSV/PNG)
│  └─ full/                    # EDA outputs for full run (CSV/PNG)
├─ requirements.txt
└─ README.md
```

**.gitignore** (recommended):
```
data/processed/
data/raw/
data/tmp/
.DS_Store
.ipynb_checkpoints/
```

---

## Environment & Requirements
Install Python deps (Codespaces or local):
```bash
pip install -r requirements.txt
```
`requirements.txt`:
```
pyspark==3.5.1
pandas
pyarrow
matplotlib
jupyter
kagglehub
```

> If needed, you can increase Spark resources by editing `src/utils.py → get_spark()` (e.g., `spark.driver.memory`, adaptive execution, shuffle partitions).

---

## 1) Download Dataset (+ Create Sample)
This downloads the full arXiv metadata and writes a 50k-line JSONL sample for quick tests.
```bash
python scripts/download_arxiv.py --sample 50000
# → raw file at data/raw/arxiv-metadata-oai-snapshot.json
# → sample JSONL at data/sample/arxiv-sample.jsonl
```

---

## 2) Ingestion (JSON/JSONL → Parquet)

### Sample (fast, for PR/demo)
```bash
python -m src.ingestion   --input data/sample/arxiv-sample.jsonl   --output data/processed/arxiv_parquet   --partition-by year
```
**Outputs**: `data/processed/arxiv_parquet/` + quick stats in console.

### Full Dataset (~1.7M records)
```bash
# IMPORTANT: Do NOT use --multiline for the Kaggle JSON (it's JSONL).
python -m src.ingestion   --input data/raw/arxiv-metadata-oai-snapshot.json   --output data/processed/arxiv_full   --partition-by year   --repartition 200
```
**Outputs**: `data/processed/arxiv_full/` (partitioned Parquet) + console stats.

> If you accidentally used `--multiline` on JSONL earlier, delete your old output with `rm -rf data/processed/arxiv_full` and re-run the command above.

---

## 3) EDA (CSV Tables + PNG Charts)
Point the EDA script at either the sample or full Parquet output.

### Sample EDA (writes to `reports/sample/`)
```bash
python notebooks/eda_week8.py --parquet data/processed/arxiv_parquet
```

### Full EDA (writes to `reports/full/`)
```bash
python notebooks/eda_week8.py --parquet data/processed/arxiv_full
```

**Artifacts written** (CSV + PNG):
- Completeness by column (`completeness.csv`)
- Distinct counts for key columns (`distinct_selected.csv`)
- Text length summary (`text_length_summary.csv`)
- Top categories (`top_categories.csv/.png`)
- Papers per year (`by_year.csv/.png`)
- Category × Year heatmap (`category_year_matrix.csv/.png`)
- Abstract length histogram (`abstract_length_hist.png`, sampled)
- DOI coverage by year (`doi_rate_by_year.csv/.png`)
- Top authors (`top_authors.csv/.png`)
- Versions per paper (`version_count_hist.csv/.png`)
- Category Pareto (`category_pareto.csv/.png`)

---

## Running in GitHub Codespaces

### Avoiding idle suspend
- Codespaces will suspend after inactivity (default 30 minutes). Increase Idle Timeout to up to 4 hours in Codespaces settings, or keep a terminal active (e.g., `watch -n 300 date`).

### tmux (optional)
Keep long jobs attached to a tmux session:
```bash
sudo apt-get update && sudo apt-get install -y tmux
tmux new -s eda
# run your command inside
python notebooks/eda_week8.py --parquet data/processed/arxiv_full
# detach with:  Ctrl+b then d
tmux attach -t eda   # to reattach
```

> tmux keeps your job alive if your terminal disconnects, but cannot prevent Codespaces from auto-suspending at the platform limit (e.g., 12h). For very long runs, consider chunking or a more persistent environment.

---

## Troubleshooting

### “Row count: 1” in EDA
- Likely ingestion read the raw file as a single multiline JSON object. The Kaggle file is **JSONL**; re-run ingestion **without** `--multiline`:
```bash
rm -rf data/processed/arxiv_full
python -m src.ingestion   --input data/raw/arxiv-metadata-oai-snapshot.json   --output data/processed/arxiv_full   --partition-by year   --repartition 200
```

### Out of Memory (OOM) in Codespaces
- Avoid `df.toPandas()` on large DataFrames.
- Use the **sample** for quick plots; for full runs, increase partitions and driver memory in `utils.get_spark()`.
- Reduce sampling in EDA histograms: `--abslen-sample-frac 0.02`.

### Permissions / Java
- Ensure Java 17+ is available in the container. In Codespaces, consider a devcontainer with Java + Python or install Temurin 17.

---

## License
- The project code is MIT (or course default).  
- Dataset metadata is CC0 (Public Domain). PDFs and individual papers may have different licenses—respect their terms.

---

## Acknowledgements
- **arXiv** (Cornell University) for maintaining the dataset and service.
- **Kaggle** for hosting a mirror and providing KaggleHub.
- **Apache Spark** community.


/home/data/akhalegh/utils/ccda-course-project/Makefile
================================================================================
run:
	bash run.sh


/home/data/akhalegh/utils/ccda-course-project/run.sh
================================================================================
#!/bin/bash
set -euo pipefail

RAW="data/raw/arxiv-metadata-oai-snapshot.json"
OUT="data/processed/arxiv_full"

# ---- Spark low-memory friendly settings for BOTH ingestion & EDA ----
export SPARK_LOCAL_DIRS="${SPARK_LOCAL_DIRS:-$(pwd)/data/tmp/spark-local}"
mkdir -p "$SPARK_LOCAL_DIRS"

# Give the local driver (and its single executor) more heap; keep tasks small
export SPARK_DRIVER_MEMORY="${SPARK_DRIVER_MEMORY:-8g}"
export SPARK_EXECUTOR_MEMORY="${SPARK_EXECUTOR_MEMORY:-8g}"

# Ensure these confs are applied even when launching via `python script.py`
export PYSPARK_SUBMIT_ARGS="\
 --conf spark.sql.session.timeZone=UTC \
 --conf spark.sql.adaptive.enabled=true \
 --conf spark.sql.adaptive.coalescePartitions.enabled=true \
 --conf spark.sql.adaptive.advisoryPartitionSizeInBytes=8m \
 --conf spark.sql.files.maxPartitionBytes=8m \
 --conf spark.sql.shuffle.partitions=800 \
 --conf spark.sql.adaptive.skewJoin.enabled=true \
 --conf spark.sql.adaptive.skewedPartitionThresholdInBytes=64m \
 --conf spark.sql.adaptive.skewedPartitionMaxSplitBytes=16m \
 --conf spark.local.dir=${SPARK_LOCAL_DIRS} \
 --driver-memory ${SPARK_DRIVER_MEMORY} \
 --conf spark.executor.memory=${SPARK_EXECUTOR_MEMORY} \
 pyspark-shell"

echo "[check] ensuring raw dataset exists..."
if [[ ! -f "$RAW" ]]; then
  echo "[download] fetching arXiv metadata via KaggleHub..."
  python scripts/download_arxiv.py --sample 0
fi

echo "[ingest] JSON/JSONL -> Parquet (full)…"
python -m src.ingestion \
  --input "$RAW" \
  --output "$OUT" \
  --partition-by year \
  --repartition 200 \
  --no-stats

echo "[eda] generating reports for FULL data…"
python notebooks/eda_week8.py \
  --parquet "$OUT" \
  --topk 30 \
  --abslen-sample-frac 0.02 \
  --outdir full

echo "[done] artifacts in reports/full/ (CSVs + PNGs)."


/home/data/akhalegh/utils/ccda-course-project/requirements.txt
================================================================================
pyspark==3.5.1
pandas
pyarrow
matplotlib
jupyter
kagglehub


/home/data/akhalegh/utils/ccda-course-project/notebooks/streaming_demo.ipynb
================================================================================
{}

/home/data/akhalegh/utils/ccda-course-project/notebooks/eda_week8.py
================================================================================
# notebooks/eda_week8.py
"""
Comprehensive EDA for the arXiv metadata after ingestion.

Usage:
  python notebooks/eda_week8.py --parquet data/processed/arxiv_full
  # or for the small sample:
  python notebooks/eda_week8.py --parquet data/processed/arxiv_parquet

This script will:
  - Print schema and table-level summaries to stdout
  - Write compact CSV summaries to reports/<run_type>/
  - Save several matplotlib charts to reports/<run_type>/*.png

Notes:
  - <run_type> is "full" if the parquet path contains "full", otherwise "sample"
    (override with --outdir if you want a custom folder).
  - Plots avoid loading huge data to the driver by using aggregated Spark DF results.
"""

from __future__ import annotations
import argparse
from pathlib import Path

import pandas as pd
import matplotlib.pyplot as plt
from pyspark.sql import SparkSession, functions as F


# ---------- helpers ----------

def pick_outdir(parquet_path: str, user_outdir: str | None) -> Path:
    if user_outdir:
        out = Path("reports") / user_outdir
    else:
        run_type = "full" if "full" in parquet_path else "sample"
        out = Path("reports") / run_type
    out.mkdir(parents=True, exist_ok=True)
    return out

def save_df_as_csv(df_spark, path: Path):
    # All saved tables are small (aggregations), safe to convert to pandas
    df_spark.toPandas().to_csv(path, index=False)
    print(f"[saved] {path}")

def matplotlib_savefig(path: Path):
    plt.tight_layout()
    plt.savefig(path, dpi=150, bbox_inches="tight")
    plt.close()
    print(f"[plot] {path}")


# ---------- core EDA ----------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--parquet", required=True, help="Path to Parquet output from src/ingestion.py")
    ap.add_argument("--topk", type=int, default=20, help="Top-K categories/authors to show")
    ap.add_argument("--abslen-sample-frac", type=float, default=0.05,
                    help="Sampling fraction for abstract length histogram (0<frac<=1)")
    ap.add_argument("--outdir", default=None,
                    help="Optional subfolder name under reports/. If not set, auto-chooses 'full' or 'sample'.")
    args = ap.parse_args()

    outdir = pick_outdir(args.parquet, args.outdir)

    spark = SparkSession.builder.appName("arxiv_week8_eda").getOrCreate()
    df = spark.read.parquet(args.parquet)

    # Basic info
    print("\n=== SCHEMA ===")
    df.printSchema()

    n_rows = df.count()
    n_cols = len(df.columns)
    print(f"\n=== SIZE ===\nrows={n_rows:,}  cols={n_cols}")

    
    print("\n=== COMPLETENESS (non-null %) ===")

    # compute row count first (single number)
    n_rows = df.count()
    n_cols = len(df.columns)
    print(f"\n=== SIZE ===\nrows={n_rows:,}  cols={n_cols}")

    # Do small per-column jobs to avoid a single wide aggregation that can OOM
    rows = []
    for c in df.columns:
        # count non-nulls in a tiny job
        nn = df.select(F.count(F.when(F.col(c).isNotNull(), 1)).alias("nn")).collect()[0]["nn"]
        pct = float(nn) / n_rows * 100.0 if n_rows else 0.0
        rows.append((c, int(nn), pct))

    comp_pd = pd.DataFrame(rows, columns=["column", "non_null", "non_null_pct"])\
                .sort_values("non_null_pct", ascending=False)

    comp_csv = outdir / "completeness.csv"
    comp_pd.to_csv(comp_csv, index=False)
    print(comp_pd.head(20))
    print(f"[saved] {comp_csv}")

    # DISTINCT COUNTS (selected)
    print("\n=== DISTINCT COUNTS (selected) ===")
    selected = ["arxiv_id", "primary_category", "year", "doi", "submitter"]
    distinct_rows = []
    for c in selected:
        if c in df.columns:
            distinct_rows.append((c, df.select(c).distinct().count()))
    distinct_pd = pd.DataFrame(distinct_rows, columns=["column", "distinct_count"]).sort_values("distinct_count", ascending=False)
    distinct_csv = outdir / "distinct_selected.csv"
    distinct_pd.to_csv(distinct_csv, index=False)
    print(distinct_pd)
    print(f"[saved] {distinct_csv}")

    # TEXT LENGTH STATS
    print("\n=== TEXT LENGTH STATS (title_len, abstract_len) ===")
    num_stats = df.select("title_len", "abstract_len").summary("count", "min", "25%", "50%", "75%", "max", "mean")
    save_df_as_csv(num_stats, outdir / "text_length_summary.csv")
    num_stats.show(truncate=False)

    # TOP CATEGORIES (bar)
    print("\n=== TOP CATEGORIES ===")
    topcats = (
        df.groupBy("primary_category")
          .count()
          .orderBy(F.desc("count"))
          .limit(args.topk)
    )
    save_df_as_csv(topcats, outdir / "top_categories.csv")
    topcats_pd = topcats.toPandas()
    if not topcats_pd.empty:
        plt.figure(figsize=(10, 5))
        topcats_pd = topcats_pd.sort_values("count", ascending=False)
        plt.bar(topcats_pd["primary_category"], topcats_pd["count"])
        plt.xticks(rotation=60, ha="right")
        plt.title(f"Top {len(topcats_pd)} Primary Categories")
        plt.xlabel("primary_category")
        plt.ylabel("count")
        matplotlib_savefig(outdir / "top_categories.png")

    # PAPERS PER YEAR (line)
    print("\n=== PAPERS PER YEAR ===")
    by_year = df.groupBy("year").count().orderBy("year")
    save_df_as_csv(by_year, outdir / "by_year.csv")
    by_year_pd = by_year.toPandas().dropna()
    if not by_year_pd.empty:
        plt.figure(figsize=(9, 4))
        plt.plot(by_year_pd["year"], by_year_pd["count"])
        plt.title("Papers per Year")
        plt.xlabel("year")
        plt.ylabel("count")
        matplotlib_savefig(outdir / "papers_per_year.png")

    # CATEGORY x YEAR HEATMAP (for top-K categories only)
    print("\n=== CATEGORY x YEAR HEATMAP (top categories) ===")
    top_cats_list = topcats_pd["primary_category"].tolist() if not topcats_pd.empty else []
    if top_cats_list:
        cat_year = (
            df.where(F.col("primary_category").isin(top_cats_list))
              .groupBy("primary_category", "year")
              .count()
        )
        cat_year_pd = cat_year.toPandas().pivot_table(index="primary_category", columns="year", values="count", fill_value=0)
        if not cat_year_pd.empty:
            plt.figure(figsize=(12, max(4, len(top_cats_list) * 0.35)))
            plt.imshow(cat_year_pd.values, aspect="auto")
            plt.yticks(range(len(cat_year_pd.index)), cat_year_pd.index)
            plt.xticks(range(len(cat_year_pd.columns)), cat_year_pd.columns, rotation=60, ha="right")
            plt.title("Counts by Primary Category (rows) and Year (cols)")
            plt.colorbar()
            matplotlib_savefig(outdir / "heatmap_category_year.png")
            cat_year_pd.to_csv(outdir / "category_year_matrix.csv")
            print(f"[saved] {outdir / 'category_year_matrix.csv'}")

    # ABSTRACT LENGTH DISTRIBUTION (hist; sampled)
    print("\n=== ABSTRACT LENGTH DISTRIBUTION ===")
    frac = args.abslen_sample_frac
    if frac > 0 and frac <= 1:
        abs_len_pd = df.select("abstract_len").sample(False, frac, seed=42).toPandas()
        if not abs_len_pd.empty:
            plt.figure(figsize=(8, 4))
            plt.hist(abs_len_pd["abstract_len"].dropna(), bins=50)
            plt.title(f"Abstract Length Distribution (sampled frac={frac})")
            plt.xlabel("abstract_len")
            plt.ylabel("frequency")
            matplotlib_savefig(outdir / "abstract_length_hist.png")

    # DOI AVAILABILITY BY YEAR (line)
    print("\n=== DOI AVAILABILITY BY YEAR ===")
    if "has_doi" in df.columns:
        has_doi_by_year = (
            df.groupBy("year")
              .agg(F.avg(F.col("has_doi").cast("double")).alias("doi_rate"))
              .orderBy("year")
        )
        save_df_as_csv(has_doi_by_year, outdir / "doi_rate_by_year.csv")
        doi_pd = has_doi_by_year.toPandas().dropna()
        if not doi_pd.empty:
            plt.figure(figsize=(9, 4))
            plt.plot(doi_pd["year"], (doi_pd["doi_rate"] * 100.0))
            plt.title("DOI Coverage by Year (%)")
            plt.xlabel("year")
            plt.ylabel("doi rate (%)")
            matplotlib_savefig(outdir / "doi_rate_by_year.png")

    # TOP AUTHORS (bar)
    if "authors_list" in df.columns:
        print("\n=== TOP AUTHORS (by paper count) ===")
        exploded = df.select(F.explode_outer("authors_list").alias("author"))
        top_authors = exploded.groupBy("author").count().orderBy(F.desc("count")).limit(args.topk)
        save_df_as_csv(top_authors, outdir / "top_authors.csv")

        top_authors_pd = top_authors.toPandas()
        if not top_authors_pd.empty:
            plt.figure(figsize=(10, 5))
            top_authors_pd = top_authors_pd.sort_values("count", ascending=False)
            plt.bar(top_authors_pd["author"], top_authors_pd["count"])
            plt.xticks(rotation=60, ha="right")
            plt.title(f"Top {len(top_authors_pd)} Authors by Paper Count")
            plt.xlabel("author")
            plt.ylabel("count")
            matplotlib_savefig(outdir / "top_authors.png")

    # VERSION COUNT PER PAPER (bar)
    print("\n=== VERSION COUNT PER PAPER ===")
    if "versions" in df.columns:
        df_versions = df.withColumn("n_versions", F.size("versions"))
        vhist = df_versions.groupBy("n_versions").count().orderBy("n_versions")
        save_df_as_csv(vhist, outdir / "version_count_hist.csv")

        vhist_pd = vhist.toPandas()
        if not vhist_pd.empty:
            plt.figure(figsize=(8, 4))
            plt.bar(vhist_pd["n_versions"], vhist_pd["count"])
            plt.title("Number of Versions per Paper")
            plt.xlabel("n_versions")
            plt.ylabel("count")
            matplotlib_savefig(outdir / "version_count_hist.png")

    # CATEGORY PARETO (cumulative %)
    print("\n=== CATEGORY PARETO (CUMULATIVE %) ===")
    cat_counts = df.groupBy("primary_category").count()
    cat_counts_pd = cat_counts.toPandas().sort_values("count", ascending=False)
    if not cat_counts_pd.empty:
        cat_counts_pd["cum_pct"] = (cat_counts_pd["count"].cumsum() / cat_counts_pd["count"].sum()) * 100.0
        cat_counts_pd.to_csv(outdir / "category_pareto.csv", index=False)
        plt.figure(figsize=(10, 4))
        plt.plot(range(1, len(cat_counts_pd) + 1), cat_counts_pd["cum_pct"])
        plt.title("Cumulative Share of Papers by Category (Pareto)")
        plt.xlabel("rank of category")
        plt.ylabel("cumulative % of papers")
        matplotlib_savefig(outdir / "category_pareto.png")

    print(f"\n[done] EDA artifacts written to {outdir}/")
    spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/notebooks/ingestion.ipynb
================================================================================
{}

/home/data/akhalegh/utils/ccda-course-project/notebooks/ml_pipeline.ipynb
================================================================================
{}

/home/data/akhalegh/utils/ccda-course-project/notebooks/sql_queries.ipynb
================================================================================
{}

/home/data/akhalegh/utils/ccda-course-project/notebooks/eda.ipynb
================================================================================
{}

/home/data/akhalegh/utils/ccda-course-project/scripts/download_arxiv.py
================================================================================
# scripts/download_arxiv.py
"""
Download the Kaggle Cornell arXiv metadata and (optionally) write a tiny JSONL sample
for Week-8 PRs / quick EDA in Codespaces.

Examples:
  python scripts/download_arxiv.py
  python scripts/download_arxiv.py --sample 50000
"""
from __future__ import annotations
import argparse
import os
import shutil
from pathlib import Path
import kagglehub

def is_likely_jsonl(path: Path, probe_lines: int = 5) -> bool:
    """
    Heuristic: JSONL typically has a complete JSON object per line.
    We'll read a few lines and check that each starts with '{' (after stripping).
    """
    try:
        with open(path, "r", encoding="utf-8") as f:
            for i in range(probe_lines):
                line = f.readline()
                if not line:
                    break
                s = line.strip()
                if not s:
                    continue
                if not s.startswith("{"):
                    return False
        return True
    except Exception:
        return False

def write_head_jsonl(src: Path, dst: Path, n: int) -> int:
    dst.parent.mkdir(parents=True, exist_ok=True)
    count = 0
    with open(src, "r", encoding="utf-8") as fin, open(dst, "w", encoding="utf-8") as fout:
        for line in fin:
            if not line.strip():
                continue
            fout.write(line)
            count += 1
            if count >= n:
                break
    return count

def main():
    ap = argparse.ArgumentParser(description="Download Kaggle arXiv dataset and optionally create a JSONL sample.")
    ap.add_argument("--sample", type=int, default=30000, help="Number of lines to write into data/sample/arxiv-sample.jsonl (0 to skip).")
    args = ap.parse_args()

    print("[KaggleHub] Downloading Cornell-University/arxiv ...")
    path = kagglehub.dataset_download("Cornell-University/arxiv")
    print("Path to dataset files:", path)

    # Locate the metadata file in the downloaded folder
    base = Path(path)
    candidates = list(base.rglob("arxiv-metadata-oai-snapshot.json")) + \
                 list(base.rglob("arxiv-metadata-oai-snapshot.jsonl"))
    if not candidates:
        raise FileNotFoundError("Could not find arxiv-metadata-oai-snapshot.json(.jsonl) in Kaggle download.")

    src = candidates[0]
    print(f"[Found] {src}")

    # Copy raw file into project (but keep it out of Git; .gitignore has data/raw/)
    proj_raw = Path("data/raw")
    proj_raw.mkdir(parents=True, exist_ok=True)
    target = proj_raw / src.name
    if target.resolve() != src.resolve():
        print(f"[Copy] -> {target}")
        shutil.copyfile(src, target)

    # Create a small JSONL sample for Week-8 PRs
    if args.sample and args.sample > 0:
        sample_dir = Path("data/sample")
        sample_dir.mkdir(parents=True, exist_ok=True)
        sample_path = sample_dir / "arxiv-sample.jsonl"

        if is_likely_jsonl(target):
            n = write_head_jsonl(target, sample_path, args.sample)
            print(f"[Sample] Wrote first {n} JSONL lines to {sample_path}")
        else:
            # If the downloaded file is pretty-printed JSON (rare), we’ll advise running with --multiline later.
            # Still write a small pseudo-sample by grabbing lines (not strict JSON); mainly for dev/testing of IO.
            n = write_head_jsonl(target, sample_path, args.sample)
            print(f"[Sample] (non-JSONL heuristic) Wrote {n} lines to {sample_path}")
            print("NOTE: If this isn't valid JSONL, use --multiline when running ingestion on the full file.")

    print("[Done] Ready for Week-8 ingestion.")

if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/reports/full/category_year_matrix.csv
================================================================================
primary_category,2008
hep-ph,1.0


/home/data/akhalegh/utils/ccda-course-project/reports/full/doi_rate_by_year.csv
================================================================================
year,doi_rate
2008,1.0


/home/data/akhalegh/utils/ccda-course-project/reports/full/text_length_summary.csv
================================================================================
summary,title_len,abstract_len
count,1,1
min,85,981
25%,85,981
50%,85,981
75%,85,981
max,85,981
mean,85.0,981.0


/home/data/akhalegh/utils/ccda-course-project/reports/full/top_authors.csv
================================================================================
author,count
 P. M. Nadolsky,1
 E. L. Berger,1
 C. -P. Yuan,1
 C. Balázs,1


/home/data/akhalegh/utils/ccda-course-project/reports/full/category_pareto.csv
================================================================================
primary_category,count,cum_pct
hep-ph,1,100.0


/home/data/akhalegh/utils/ccda-course-project/reports/full/top_categories.csv
================================================================================
primary_category,count
hep-ph,1


/home/data/akhalegh/utils/ccda-course-project/reports/full/completeness.csv
================================================================================
column,non_null,non_null_pct
arxiv_id,2854101,100.0
title,2854101,100.0
abstract,2854101,100.0
categories,2854101,100.0
update_date,2854101,100.0
title_clean,2854101,100.0
authors_parsed,2854101,100.0
authors,2854101,100.0
versions,2854101,100.0
category_list,2854101,100.0
authors_list,2854101,100.0
abstract_len,2854101,100.0
title_len,2854101,100.0
abstract_clean,2854101,100.0
title_lower,2854101,100.0
abstract_lower,2854101,100.0
primary_category,2854101,100.0
n_authors,2854101,100.0
n_categories,2854101,100.0
has_doi,2854101,100.0
year,2854101,100.0
submitter,2838932,99.46851915892255
comments,2091985,73.29751119529408
doi,1266356,44.36969819918776
journal_ref,912386,31.967544245981482


/home/data/akhalegh/utils/ccda-course-project/reports/full/version_count_hist.csv
================================================================================
n_versions,count
2,1


/home/data/akhalegh/utils/ccda-course-project/reports/full/distinct_selected.csv
================================================================================
column,distinct_count
arxiv_id,1
primary_category,1
year,1
doi,1
submitter,1


/home/data/akhalegh/utils/ccda-course-project/reports/full/by_year.csv
================================================================================
year,count
2008,1


/home/data/akhalegh/utils/ccda-course-project/reports/sample/category_year_matrix.csv
================================================================================
primary_category,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021,2022,2023,2024,2025
astro-ph,1165.0,1346.0,5446.0,311.0,152.0,71.0,21.0,86.0,286.0,46.0,66.0,9.0,179.0,13.0,0.0,3.0,2.0,1.0,2.0
cond-mat.mes-hall,264.0,357.0,688.0,33.0,43.0,17.0,26.0,6.0,54.0,8.0,2.0,3.0,2.0,2.0,1.0,0.0,0.0,0.0,0.0
cond-mat.mtrl-sci,467.0,250.0,622.0,30.0,44.0,13.0,16.0,5.0,56.0,10.0,3.0,0.0,3.0,2.0,1.0,1.0,0.0,0.0,1.0
cond-mat.other,163.0,150.0,385.0,39.0,17.0,7.0,13.0,6.0,25.0,9.0,5.0,1.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0
cond-mat.soft,167.0,111.0,350.0,13.0,25.0,11.0,12.0,2.0,18.0,10.0,3.0,2.0,2.0,0.0,0.0,3.0,0.0,1.0,0.0
cond-mat.stat-mech,239.0,258.0,546.0,34.0,58.0,9.0,14.0,7.0,43.0,16.0,4.0,0.0,0.0,2.0,0.0,1.0,0.0,0.0,1.0
cond-mat.str-el,236.0,309.0,634.0,26.0,41.0,15.0,15.0,6.0,38.0,10.0,3.0,4.0,3.0,3.0,0.0,1.0,0.0,0.0,1.0
cond-mat.supr-con,155.0,134.0,409.0,21.0,26.0,9.0,12.0,4.0,36.0,5.0,0.0,2.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0
gr-qc,202.0,1019.0,189.0,58.0,55.0,24.0,5.0,35.0,32.0,31.0,5.0,2.0,6.0,2.0,0.0,1.0,4.0,0.0,0.0
hep-ex,162.0,183.0,46.0,131.0,33.0,75.0,1.0,3.0,8.0,2.0,10.0,1.0,115.0,0.0,0.0,0.0,1.0,0.0,0.0
hep-ph,591.0,2153.0,461.0,111.0,103.0,17.0,14.0,47.0,31.0,10.0,36.0,1.0,5.0,1.0,1.0,4.0,1.0,0.0,4.0
hep-th,291.0,1690.0,665.0,102.0,62.0,16.0,8.0,109.0,37.0,10.0,13.0,3.0,5.0,3.0,1.0,2.0,0.0,2.0,0.0
math-ph,295.0,230.0,288.0,43.0,46.0,19.0,16.0,20.0,47.0,11.0,7.0,4.0,12.0,5.0,1.0,0.0,1.0,1.0,1.0
math.AG,366.0,197.0,111.0,69.0,38.0,32.0,18.0,38.0,18.0,13.0,12.0,12.0,17.0,2.0,4.0,0.0,3.0,3.0,2.0
math.CO,337.0,152.0,64.0,50.0,43.0,15.0,12.0,15.0,6.0,8.0,9.0,4.0,7.0,7.0,3.0,0.0,0.0,0.0,1.0
math.DG,317.0,177.0,103.0,39.0,43.0,23.0,12.0,19.0,20.0,10.0,6.0,5.0,7.0,4.0,5.0,0.0,1.0,1.0,0.0
math.NT,296.0,144.0,55.0,34.0,29.0,24.0,14.0,24.0,25.0,5.0,4.0,4.0,9.0,8.0,10.0,1.0,4.0,1.0,0.0
math.PR,349.0,241.0,163.0,53.0,57.0,16.0,13.0,5.0,12.0,9.0,5.0,3.0,5.0,4.0,2.0,4.0,1.0,0.0,1.0
nucl-th,112.0,644.0,101.0,18.0,23.0,3.0,3.0,14.0,18.0,2.0,19.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0
quant-ph,587.0,589.0,1048.0,75.0,94.0,41.0,30.0,19.0,91.0,28.0,15.0,5.0,3.0,2.0,3.0,9.0,5.0,2.0,0.0


/home/data/akhalegh/utils/ccda-course-project/reports/sample/doi_rate_by_year.csv
================================================================================
year,doi_rate
2007,0.1220672417963396
2008,0.6699789656923189
2009,0.907787293367855
2010,0.6661506707946336
2011,0.5072046109510087
2012,0.6248196248196248
2013,0.6778523489932886
2014,0.8387096774193549
2015,0.927570093457944
2016,0.7677642980935875
2017,0.8295819935691319
2018,0.6363636363636364
2019,0.5773584905660377
2020,0.7663551401869159
2021,0.5753424657534246
2022,0.6721311475409836
2023,0.7560975609756098
2024,0.3469387755102041
2025,0.5714285714285714


/home/data/akhalegh/utils/ccda-course-project/reports/sample/text_length_summary.csv
================================================================================
summary,title_len,abstract_len
count,49981,49981
min,4,40
25%,51,497
50%,66,717
75%,84,1028
max,227,2862
mean,69.29397170924952,790.9938976811188


/home/data/akhalegh/utils/ccda-course-project/reports/sample/top_authors.csv
================================================================================
author,count
IRCCyN  Damien Chablat,84
 B. Aubert,66
  The BABAR Collaboration,61
IRCCyN  Philippe Wenger,60
 H. Vincent Poor,59
  CDF Collaboration,41
 N. Gehrels,34
  D0 Collaboration,32
 S. Das Sarma,32
 T. Aaltonen,31
 Tshilidzi Marwala,28
 Vladimir V. Sergeichuk,27
 S. Udry,26
 M. Mayor,25
 R. Vlokh,25
 R. Michael Rich,25
 D. Queloz,24
 Delfim F. M. Torres,24
 M. I. Katsnelson,24
 G. Tagliaferri,24


/home/data/akhalegh/utils/ccda-course-project/reports/sample/category_pareto.csv
================================================================================
primary_category,count,cum_pct
astro-ph,9205,18.416998459414575
hep-ph,3591,25.601728656889616
hep-th,3019,31.642023969108262
quant-ph,2646,36.936035693563554
gr-qc,1670,40.2773053760429
cond-mat.mtrl-sci,1524,43.32646405634141
cond-mat.mes-hall,1506,46.339609051439545
cond-mat.str-el,1345,49.03063164002321
cond-mat.stat-mech,1232,51.495568315960064
math-ph,1047,53.590364338448616
nucl-th,960,55.511094215802004
math.AG,955,57.421820291710844
math.PR,943,59.30853724415278
cond-mat.other,822,60.95316220163662
cond-mat.supr-con,816,62.58578259738701
math.DG,792,64.17038474620355
hep-ex,771,65.71297092895301
math.CO,733,67.17952822072387
cond-mat.soft,730,68.64008323162803
math.NT,691,70.02260859126469
math.AP,617,71.25707768952202
hep-lat,571,72.3995118144895
cs.IT,570,73.53994517916809
physics.optics,467,74.47430023408896
physics.gen-ph,465,75.40465376843201
math.GT,442,76.28898981613013
nucl-ex,436,77.1613213020948
math.RT,425,78.01164442488145
math.DS,421,78.85396450651247
cond-mat.dis-nn,386,79.62625797803166
math.CA,346,80.31852103799444
math.ST,340,80.99877953622376
physics.atom-ph,339,81.67703727416418
math.FA,327,82.33128588863768
math.QA,294,82.91950941357716
physics.soc-ph,287,83.49372761649427
math.GR,286,84.06594505912247
math.CV,286,84.63816250175067
math.RA,276,85.19037234148976
nlin.CD,254,85.69856545487285
math.OA,250,86.1987555271003
math.AC,228,86.65492887297172
physics.flu-dyn,227,87.10910145855425
math.AT,220,87.5492687221144
stat.ME,189,87.92741241671835
nlin.SI,176,88.27954622756647
physics.ins-det,172,88.62367699725895
physics.plasm-ph,164,88.95180168464016
physics.class-ph,163,89.27792561173246
math.OC,159,89.59604649766911
math.LO,157,89.91016586302794
math.NA,153,90.21628218723114
q-bio.PE,153,90.52239851143435
math.SP,147,90.81651027390409
physics.bio-ph,140,91.09661671435146
physics.chem-ph,138,91.372721634221
math.SG,136,91.64482503351273
nlin.PS,133,91.91092615193773
physics.data-an,130,92.171024989496
math.GM,127,92.42512154618755
cs.AR,126,92.67721734259018
math.MG,125,92.9273123787039
cs.NI,121,93.16940437366199
cs.LO,120,93.40949560833116
cs.DM,119,93.64758608271143
cs.OH,113,93.87367199535824
cs.DS,112,94.09775714771614
math.KT,102,94.30183469718493
q-bio.BM,100,94.5019107260759
q-bio.QM,96,94.69398371381125
cs.AI,95,94.88405594125769
physics.comp-ph,95,95.0741281687041
stat.AP,94,95.26219963586162
cs.CC,93,95.44827034273024
physics.geo-ph,89,95.62633800844321
cs.CR,86,95.79840339328946
cs.RO,86,95.97046877813568
q-fin.ST,83,96.13653188211521
math.CT,81,96.2985934655169
q-bio.MN,79,96.45665352834078
cs.DC,75,96.606710550009
q-bio.NC,72,96.7507652908105
cs.NE,71,96.8928192713231
math.HO,69,97.03087173125788
physics.acc-ph,63,97.1569196294592
physics.hist-ph,59,97.27496448650487
nlin.AO,56,97.38700706268382
cs.HC,54,97.49504811828494
cs.CG,54,97.60308917388608
physics.ed-ph,54,97.71113022948721
q-fin.GN,51,97.8131690042216
cs.PL,50,97.9132070186671
cs.LG,49,98.01124427282367
physics.med-ph,49,98.10928152698025
math.GN,48,98.20531802084793
cs.CL,47,98.29935375442668
cs.DB,47,98.39338948800544
cs.GT,46,98.4854244612953
q-bio.GN,43,98.57145715371841
physics.ao-ph,42,98.65548908585262
cs.CE,41,98.73752025769792
physics.atm-clus,41,98.81955142954322
cs.CY,38,98.8955803205218
cs.SE,37,98.96960845121146
cs.CV,35,99.03963506132331
q-fin.PR,35,99.10966167143515
stat.CO,31,99.17168524039135
q-bio.OT,30,99.23170804905864
physics.pop-ph,29,99.28973009743703
cs.IR,29,99.3477521458154
q-bio.SC,28,99.40377343390489
q-fin.TR,27,99.45779396170545
q-fin.PM,26,99.5098137292171
q-bio.CB,25,99.55983273643984
nlin.CG,24,99.60785098337368
stat.ML,24,99.65586923030752
cs.DL,20,99.69588443608572
q-bio.TO,19,99.733898881575
physics.space-ph,18,99.76991256677537
cs.SC,14,99.79792321082012
cs.PF,14,99.82593385486484
cs.MA,14,99.85394449890958
cs.MM,13,99.87995438266542
q-fin.RM,13,99.90596426642125
cs.NA,10,99.92597186931033
cs.GR,8,99.94197795162162
cs.MS,8,99.95798403393289
cs.OS,7,99.97198935595526
q-fin.CP,6,99.98399391768872
cond-mat.quant-gas,3,99.98999619855546
cs.GL,3,99.99599847942218
cs.SD,1,99.99799923971109
cs.FL,1,100.0


/home/data/akhalegh/utils/ccda-course-project/reports/sample/top_categories.csv
================================================================================
primary_category,count
astro-ph,9205
hep-ph,3591
hep-th,3019
quant-ph,2646
gr-qc,1670
cond-mat.mtrl-sci,1524
cond-mat.mes-hall,1506
cond-mat.str-el,1345
cond-mat.stat-mech,1232
math-ph,1047
nucl-th,960
math.AG,955
math.PR,943
cond-mat.other,822
cond-mat.supr-con,816
math.DG,792
hep-ex,771
math.CO,733
cond-mat.soft,730
math.NT,691


/home/data/akhalegh/utils/ccda-course-project/reports/sample/completeness.csv
================================================================================
column,non_null,non_null_pct
arxiv_id,49981,100.0
title,49981,100.0
abstract,49981,100.0
categories,49981,100.0
update_date,49981,100.0
submitter,49981,100.0
authors_parsed,49981,100.0
authors,49981,100.0
versions,49981,100.0
category_list,49981,100.0
authors_list,49981,100.0
abstract_len,49981,100.0
title_clean,49981,100.0
abstract_clean,49981,100.0
title_lower,49981,100.0
abstract_lower,49981,100.0
primary_category,49981,100.0
n_authors,49981,100.0
n_categories,49981,100.0
has_doi,49981,100.0
title_len,49981,100.0
year,49981,100.0
comments,43923,87.87939416978452
doi,30453,60.929153078169705
journal_ref,25686,51.39152878093676


/home/data/akhalegh/utils/ccda-course-project/reports/sample/version_count_hist.csv
================================================================================
n_versions,count
1,31183
2,12932
3,4145
4,1098
5,365
6,136
7,43
8,27
9,14
10,12
11,5
12,2
13,4
14,4
16,4
18,1
21,1
25,1
26,1
34,1
68,1
99,1


/home/data/akhalegh/utils/ccda-course-project/reports/sample/distinct_selected.csv
================================================================================
column,distinct_count
arxiv_id,49981
doi,30417
submitter,28757
primary_category,133
year,19


/home/data/akhalegh/utils/ccda-course-project/reports/sample/by_year.csv
================================================================================
year,count
2007,12403
2008,13787
2009,15063
2010,1938
2011,1735
2012,693
2013,447
2014,744
2015,1284
2016,577
2017,311
2018,110
2019,530
2020,107
2021,73
2022,61
2023,41
2024,49
2025,28


/home/data/akhalegh/utils/ccda-course-project/docs/methodology.md
================================================================================
# Methodology


/home/data/akhalegh/utils/ccda-course-project/docs/reproduction_guide.md
================================================================================
# Reproduction Guide


/home/data/akhalegh/utils/ccda-course-project/docs/dataset_overview.md
================================================================================
# Dataset Overview


/home/data/akhalegh/utils/ccda-course-project/docs/limitations.md
================================================================================
# Limitations


/home/data/akhalegh/utils/ccda-course-project/docs/results.md
================================================================================
# Results


/home/data/akhalegh/utils/ccda-course-project/docs/slides/presentation.pptx
================================================================================


/home/data/akhalegh/utils/ccda-course-project/src/__init__.py
================================================================================


/home/data/akhalegh/utils/ccda-course-project/src/transformations.py
================================================================================
# src/transformations.py
from pyspark.sql import DataFrame, functions as F
from src.utils import clean_text, parse_year_from_datestr, extract_primary_category, split_categories, normalize_authors, lower

def select_and_standardize(df: DataFrame) -> DataFrame:
    cols = df.columns
    base = df.select(
        F.col("id").alias("arxiv_id"),
        F.col("title"),
        F.col("abstract"),
        F.col("categories"),
        F.col("doi"),
        F.col("journal-ref").alias("journal_ref"),
        F.col("comments"),
        F.col("submitter"),
        F.col("update_date"),
        F.col("versions"),
        *([F.col("authors")] if "authors" in cols else []),
        *([F.col("authors_parsed")] if "authors_parsed" in cols else []),
    )
    base = base.withColumn("title_clean", clean_text(F.col("title")))
    base = base.withColumn("abstract_clean", clean_text(F.col("abstract")))
    base = base.withColumn("title_lower", lower(F.col("title")))
    base = base.withColumn("abstract_lower", lower(F.col("abstract")))
    base = base.withColumn("primary_category", extract_primary_category(F.col("categories")))
    base = base.withColumn("category_list", split_categories(F.col("categories")))
    base = base.withColumn("year", parse_year_from_datestr(F.col("update_date")))

    if "authors_parsed" in cols:
        base = base.withColumn(
            "authors_list",
            F.expr("transform(authors_parsed, x -> array_join(reverse(x), ' '))")
        )
    elif "authors" in cols:
        base = base.withColumn("authors_list", normalize_authors(F.col("authors")))
    else:
        base = base.withColumn("authors_list", F.array())

    base = base.withColumn("abstract_len", F.length("abstract_clean"))
    base = base.withColumn("title_len", F.length("title_clean"))
    base = base.withColumn("n_authors", F.size("authors_list"))
    base = base.withColumn("n_categories", F.size("category_list"))
    return base

def filter_for_quality(df: DataFrame, min_abstract_len: int = 40) -> DataFrame:
    return (
        df
        .where(F.col("arxiv_id").isNotNull() & (F.col("arxiv_id") != ""))
        .where(F.col("title_clean").isNotNull() & (F.length("title_clean") > 0))
        .where(F.col("abstract_clean").isNotNull() & (F.col("abstract_len") >= min_abstract_len))
    )

def add_eda_helpers(df: DataFrame) -> DataFrame:
    return df.withColumn("has_doi", (F.col("doi").isNotNull()) & (F.col("doi") != ""))

def transform_all(df: DataFrame, min_abstract_len: int = 40) -> DataFrame:
    return add_eda_helpers(filter_for_quality(select_and_standardize(df), min_abstract_len))


/home/data/akhalegh/utils/ccda-course-project/src/utils.py
================================================================================
# src/utils.py
from pathlib import Path
from pyspark.sql import SparkSession, functions as F
from pyspark.sql.column import Column


def get_spark(app_name: str = "arxiv_week8") -> SparkSession:
    """
    Create (or get) a SparkSession tuned for low-memory environments.

    Key choices to avoid OOM:
      - Stable local temp dir inside the repo (not /tmp)
      - Many small tasks (high shuffle partitions)
      - AQE with small advisory partition sizes
      - Small input split size per task
      - Skew handling to split oversized partitions
    """
    local_tmp = Path("data/tmp/spark-local")
    local_tmp.mkdir(parents=True, exist_ok=True)

    spark = (
        SparkSession.builder.appName(app_name)
        .config("spark.sql.session.timeZone", "UTC")

        # ---- Memory / resources (tune if needed) ----
        # If your machine is tighter on RAM, drop these to 4g.
        .config("spark.driver.memory", "6g")
        .config("spark.executor.memory", "6g")
        .config("spark.driver.maxResultSize", "2g")

        # ---- Lots of small tasks; AQE keeps them efficient ----
        .config("spark.sql.shuffle.partitions", "512")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
        .config("spark.sql.adaptive.advisoryPartitionSizeInBytes", "8m")  # small target post-shuffle
        .config("spark.sql.files.maxPartitionBytes", "8m")                # small input per task

        # ---- Skew handling: split very large partitions ----
        .config("spark.sql.adaptive.skewJoin.enabled", "true")
        .config("spark.sql.adaptive.skewedPartitionThresholdInBytes", "64m")
        .config("spark.sql.adaptive.skewedPartitionMaxSplitBytes", "16m")

        # ---- Temp / stability ----
        .config("spark.local.dir", str(local_tmp))
        .config("spark.shuffle.checksum.enabled", "false")  # avoid fragile checksum files

        # ---- Parquet default compression (can be overridden per-writer) ----
        .config("spark.sql.parquet.compression.codec", "zstd")

        # ---- Misc ----
        .config("spark.sql.execution.arrow.pyspark.enabled", "true")
        .getOrCreate()
    )
    return spark


# ---------- helper column transforms ----------

def clean_text(col: Column) -> Column:
    """Trim and collapse whitespace."""
    return F.regexp_replace(F.trim(col), r"\s+", " ")


def parse_year_from_datestr(col: Column) -> Column:
    """Extract the first 4-digit year as int."""
    return F.regexp_extract(col.cast("string"), r"(\d{4})", 1).cast("int")


def extract_primary_category(categories_col: Column) -> Column:
    """First whitespace-delimited token is the primary category."""
    return F.split(F.coalesce(categories_col, F.lit("")), r"\s+")[0]


def split_categories(categories_col: Column) -> Column:
    """Split on whitespace into array, dropping empties."""
    arr = F.split(F.coalesce(categories_col, F.lit("")), r"\s+")
    return F.filter(arr, lambda x: x != "")


def normalize_authors(authors_col: Column) -> Column:
    """
    Normalize authors string into an array:
      - Replace ' and ' with commas
      - Normalize comma/whitespace
      - Trim and drop empties
    """
    replaced = F.regexp_replace(F.coalesce(authors_col, F.lit("")), r"\s+and\s+", ",")
    replaced = F.regexp_replace(replaced, r"\s*,\s*", ",")
    arr = F.split(replaced, ",")
    arr = F.transform(arr, lambda x: F.trim(x))
    return F.filter(arr, lambda x: x != "")


def lower(col: Column) -> Column:
    """Lowercase after cleanup."""
    return F.lower(clean_text(col))


/home/data/akhalegh/utils/ccda-course-project/src/ingestion.py
================================================================================
# src/ingestion.py
"""
OOM-resistant ingestion for Kaggle arXiv metadata → Parquet.

Strategies:
  - Align the big shuffle with the output partition column (e.g., 'year')
  - Use many small partitions to minimize per-task memory
  - Parquet writer tuned for small in-memory buffers (block/page) and fewer rows per file
"""

import argparse
from pyspark.sql import functions as F
from src.utils import get_spark
from src.transformations import transform_all


def read_arxiv_json(spark, path: str, multiline: bool = False):
    """Read JSON/JSONL. Kaggle's arxiv snapshot is JSONL (so usually multiline=False)."""
    return (
        spark.read
        .option("multiLine", "true" if multiline else "false")
        .json(path)
    )


def main():
    ap = argparse.ArgumentParser(description="Ingest Kaggle arXiv metadata into Parquet (OOM-resistant).")
    ap.add_argument("--input", required=True, help="Path or glob to arxiv metadata JSON/JSONL (local).")
    ap.add_argument("--output", required=True, help="Directory to write processed Parquet.")
    ap.add_argument("--multiline", action="store_true", help="Input is pretty-printed multi-line JSON.")
    ap.add_argument("--limit", type=int, default=0, help="Optional: limit rows for quick demo.")
    ap.add_argument("--sample-frac", type=float, default=0.0, help="Optional: sample fraction (0<frac<=1).")
    ap.add_argument("--repartition", type=int, default=0, help="Target partitions per key before write (default 512).")
    ap.add_argument("--min-abstract-len", type=int, default=40, help="Filter very short abstracts.")
    ap.add_argument("--partition-by", default="year",
                    choices=["year", "primary_category", "none"],
                    help="Partition column for Parquet (or 'none').")
    ap.add_argument("--no-stats", action="store_true",
                    help="Skip post-write stats to minimize extra jobs.")
    args = ap.parse_args()

    spark = get_spark("arxiv_week8_ingestion_lowmem")

    # --- Read raw ---
    df_raw = read_arxiv_json(spark, args.input, multiline=args.multiline)

    # Optional sampling/limiting to validate pipeline quickly
    if args.sample_frac and 0.0 < args.sample_frac <= 1.0:
        df_raw = df_raw.sample(False, args.sample_frac, seed=42)
    if args.limit and args.limit > 0:
        df_raw = df_raw.limit(args.limit)

    # --- Transform & quality filters ---
    df = transform_all(df_raw, min_abstract_len=args.min_abstract_len)

    # --- Partition-aware shuffle alignment (critical to avoid a second huge shuffle) ---
    if args.partition_by in ("year", "primary_category"):
        target = args.repartition if args.repartition and args.repartition > 0 else 512
        df = df.repartition(target, F.col(args.partition_by))
    else:
        # No partitioned write: honor numeric repartition if provided
        if args.repartition and args.repartition > 0:
            df = df.repartition(args.repartition)

    # --- Write with small Parquet buffers to reduce writer heap usage ---
    writer = (
        df.write
          .mode("overwrite")
          .option("compression", "zstd")                         # can switch to 'snappy' if preferred
          .option("parquet.block.size", 8 * 1024 * 1024)         # 8 MB blocks
          .option("parquet.page.size", 512 * 1024)               # 512 KB pages
          .option("parquet.enable.dictionary", "true")
          .option("maxRecordsPerFile", 50000)                    # cap rows per file
    )
    if args.partition_by != "none":
        writer = writer.partitionBy(args.partition_by)

    writer.parquet(args.output)

    if not args.no_stats:
        # Lightweight-ish stats (aggregations only). Skip with --no-stats if you want zero extra jobs.
        n = df.count()
        print(f"[OK] Wrote {n} rows to {args.output}")

        try:
            top_cats = (
                df.groupBy("primary_category")
                  .count()
                  .orderBy(F.desc("count"))
                  .limit(10)
                  .collect()
            )
            print("[Top categories]")
            for r in top_cats:
                print(f"  {r['primary_category']}: {r['count']}")
        except Exception as e:
            print(f"[warn] top_cats aggregation skipped: {e}")

        try:
            by_year = (
                df.groupBy("year")
                  .count()
                  .orderBy("year")
                  .collect()
            )
            print("[Counts by year]")
            for r in by_year:
                print(f"  {r['year']}: {r['count']}")
        except Exception as e:
            print(f"[warn] by_year aggregation skipped: {e}")

    spark.stop()


if __name__ == "__main__":
    main()


/home/data/akhalegh/utils/ccda-course-project/tests/test_ingestion.py
================================================================================
def test_ingestion():
    assert True


/home/data/akhalegh/utils/ccda-course-project/tests/test_sql.py
================================================================================
def test_sql():
    assert True


/home/data/akhalegh/utils/ccda-course-project/tests/test_ml.py
================================================================================
def test_ml():
    assert True


/home/data/akhalegh/utils/ccda-course-project/tests/test_streaming.py
================================================================================
def test_streaming():
    assert True


